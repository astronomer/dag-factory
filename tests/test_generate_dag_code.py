import unittest
import subprocess
import os
import tempfile
import sys

# Attempt to import Airflow for integration tests
try:
    from airflow.models import DagBag
    AIRFLOW_AVAILABLE = True
except ImportError:
    AIRFLOW_AVAILABLE = False

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # Should be the project root
SCRIPT_PATH = os.path.join(BASE_DIR, "dagfactory", "generate_dag_code.py")
FIXTURES_DIR = os.path.join(BASE_DIR, "tests", "fixtures", "cli_yaml_configs")

class TestGenerateDagCodeUnit(unittest.TestCase):

    def _run_script(self, yaml_file_path):
        process = subprocess.run(
            [sys.executable, SCRIPT_PATH, yaml_file_path],
            capture_output=True,
            text=True,
            check=True  # Will raise CalledProcessError if script exits with non-zero
        )
        return process.stdout

    def test_simple_dag_generation(self):
        yaml_file = os.path.join(FIXTURES_DIR, "simple_dag.yml")
        generated_code = self._run_script(yaml_file)

        # Define expected output (this needs to be precise)
        # Note: libcst might format with specific quoting/spacing. Adjust as needed.
        expected_code = """import datetime
from airflow import DAG
from airflow.operators.bash import BashOperator

# Default DAG arguments
default_args = {"owner": "airflow", "start_date": datetime.datetime(2023, 1, 1)}

# DAG Definition
with DAG(
    "my_simple_dag",
    default_args=default_args,
    schedule_interval="@daily",
    description="A simple DAG generated by CLI.",
    catchup=True,
) as dag:
    task_1 = BashOperator(task_id="task_1", bash_command='echo "Hello from task 1"')
    task_2 = BashOperator(task_id="task_2", bash_command='echo "Hello from task 2"')

    task_1 >> task_2
"""
        # Normalize whitespace for comparison (simple approach)
        self.assertEqual(
            "".join(generated_code.split()),
            "".join(expected_code.split()),
            "Generated code for simple_dag.yml does not match expected output."
        )

    def test_complex_dag_generation(self):
        yaml_file = os.path.join(FIXTURES_DIR, "complex_dag.yml")
        generated_code = self._run_script(yaml_file)
        
        expected_code = """import datetime
from airflow import DAG
from airflow.operators.bash import BashOperator

# Default DAG arguments
default_args = {
    "owner": "data_team",
    "start_date": datetime.datetime(2023, 2, 15),
    "retries": 3,
    "retry_delay": datetime.timedelta(5),
}

# DAG Definition
with DAG(
    "my_complex_dag",
    default_args=default_args,
    schedule_interval="0 0 * * *",
    description="A more complex DAG with multiple tasks and dependencies.",
    catchup=False,
    tags=["example", "complex"],
) as dag:
    start_task = BashOperator(
        task_id="start_task", bash_command='echo "Starting the complex DAG"'
    )
    middle_task_1 = BashOperator(
        task_id="middle_task_1", bash_command='echo "Middle task 1 running"'
    )
    middle_task_2 = BashOperator(
        task_id="middle_task_2",
        bash_command='echo "Middle task 2 running"',
        env={"MY_VAR": "my_value", "ANOTHER_VAR": "another_value"},
    )
    end_task = BashOperator(
        task_id="end_task", bash_command='echo "Complex DAG finished"'
    )

    start_task >> middle_task_1
    start_task >> middle_task_2
    middle_task_1 >> end_task
    middle_task_2 >> end_task
"""
        self.assertEqual(
            "".join(generated_code.split()),
            "".join(expected_code.split()),
            "Generated code for complex_dag.yml does not match expected output."
        )

    def test_dag_with_default_args_generation(self):
        yaml_file = os.path.join(FIXTURES_DIR, "dag_with_default_args.yml")
        generated_code = self._run_script(yaml_file)

        expected_code = """import datetime
from airflow import DAG
from airflow.operators.bash import BashOperator

# Default DAG arguments
default_args = {
    "owner": "default_owner",
    "start_date": datetime.datetime(2022, 12, 1),
    "email": ["test@example.com"],
    "email_on_failure": True,
}

# DAG Definition
with DAG(
    "dag_with_defaults",
    default_args=default_args,
    schedule_interval="None",
    description="DAG demonstrating default arguments application.",
    catchup=True,
) as dag:
    task_uses_defaults = BashOperator(
        task_id="task_uses_defaults",
        bash_command='echo "I use default owner and email settings."',
    )
    task_overrides_defaults = BashOperator(
        task_id="task_overrides_defaults",
        bash_command='echo "I have a specific owner."',
        owner="specific_owner",
        email_on_failure=False,
    )
    task_additional_args = BashOperator(
        task_id="task_additional_args",
        bash_command='echo "I have additional args and use defaults for others."',
        retries=5,
    )

    task_uses_defaults >> task_overrides_defaults
    task_overrides_defaults >> task_additional_args
"""
        self.assertEqual(
            "".join(generated_code.split()),
            "".join(expected_code.split()),
            "Generated code for dag_with_default_args.yml does not match expected output."
        )


class TestGenerateDagCodeIntegration(unittest.TestCase):

    @unittest.skipUnless(AIRFLOW_AVAILABLE, "Airflow is not installed, skipping integration tests.")
    def test_generated_dag_loads_in_dagbag(self):
        yaml_file = os.path.join(FIXTURES_DIR, "simple_dag.yml")
        
        # Generate DAG code using the script
        try:
            process = subprocess.run(
                [sys.executable, SCRIPT_PATH, yaml_file],
                capture_output=True,
                text=True,
                check=True
            )
            generated_code = process.stdout
        except subprocess.CalledProcessError as e:
            self.fail(f"generate_dag_code.py script failed: {e.stderr}")

        # Write generated code to a temporary file
        with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as tmp_dag_file:
            tmp_dag_file.write(generated_code)
            tmp_dag_filepath = tmp_dag_file.name
        
        dagbag = None
        try:
            dagbag = DagBag(dag_folder=tmp_dag_filepath, include_examples=False, read_dags_from_db=False)
            self.assertEqual(len(dagbag.import_errors), 0, 
                             f"DAG Bag import errors: {dagbag.import_errors}")
            
            self.assertIn("my_simple_dag", dagbag.dags)
            dag = dagbag.get_dag("my_simple_dag")
            self.assertIsNotNone(dag)
            self.assertEqual(dag.dag_id, "my_simple_dag")
            self.assertEqual(len(dag.tasks), 2)
            self.assertIsNotNone(dag.get_task("task_1"))
            self.assertIsNotNone(dag.get_task("task_2"))
            
            # Check dependencies
            task_1 = dag.get_task("task_1")
            task_2 = dag.get_task("task_2")
            self.assertEqual(task_2.upstream_task_ids, {"task_1"})

        finally:
            if os.path.exists(tmp_dag_filepath):
                os.remove(tmp_dag_filepath)
            # Clean up .pyc file if it exists
            pyc_file = tmp_dag_filepath + "c" # Common for .pyc
            if os.path.exists(pyc_file):
                os.remove(pyc_file)


if __name__ == "__main__":
    # Attempt to initialize Airflow DB if necessary for local testing,
    # but this might be problematic in CI/automated environments.
    # For this task, we assume Airflow is configured if available.
    # if AIRFLOW_AVAILABLE:
    #     try:
    #         subprocess.run(["airflow", "db", "init"], capture_output=True, text=True, check=False)
    #     except Exception as e:
    #         print(f"Could not initialize airflow db (this might be ok): {e}")
    unittest.main()
