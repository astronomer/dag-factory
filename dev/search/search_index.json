{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DAG Factory documentation","text":"<p>Everything you need to know about how to build Apache Airflow\u00ae workflows using YAML files.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>Are you new to DAG Factory? This is the place to start!</p> <ul> <li>DAG Factory at a glance<ul> <li>Quickstart with Airflow standalone</li> <li>Quickstart with Astro CLI</li> </ul> </li> <li>Using YAML instead of Python<ul> <li>Traditional Airflow Operators</li> <li>TaskFlow API</li> </ul> </li> </ul>"},{"location":"#configuration","title":"Configuration","text":"<ul> <li>Configuring your workflows<ul> <li>Environment variables</li> <li>Defaults</li> </ul> </li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Dynamic tasks</li> <li>Datasets scheduling</li> <li>Callbacks</li> <li>Custom operators</li> <li>Multiple configuration files</li> <li>HttpSensor</li> </ul>"},{"location":"#getting-help","title":"Getting help","text":"<p>Having trouble? We'd like to help!</p> <ul> <li>Report bugs, questions and feature requests in our ticket tracker.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>DAG Factory is an Open-Source project. Learn about its development process and about how you can contribute:</p> <ul> <li>Contributing to DAG Factory</li> <li>Github repository</li> </ul>"},{"location":"#license","title":"License","text":"<p>To learn more about the terms and conditions for use, reproduction and distribution, read the Apache License 2.0.</p>"},{"location":"#privacy-notice","title":"Privacy Notice","text":"<p>This project follows Astronomer's Privacy Policy.</p> <p>For further information, read this</p>"},{"location":"#security-policy","title":"Security Policy","text":"<p>Check the project's Security Policy to learn how to report security vulnerabilities in DAG Factory and how security issues reported to the DAG Factory security team are handled.</p> <p></p>"},{"location":"comparison/","title":"Using YAML instead of Python","text":"<p>By default, Apache Airflow\u00ae users write their workflows, or sequences of tasks, in Python.</p> <p>DAG Factory offers an alternative interface, allowing users to represent Airflow workflows via YAML files, often using less code.</p> <p>This section illustrates a few examples of how to represent the same workflow using plain Airflow Python DAGs in comparison to their representation using DAG Factory YAML files.</p> <ul> <li>Traditional Airflow Operators</li> <li>TaskFlow API</li> </ul>"},{"location":"comparison/taskflow_api/","title":"TaskFlow API: Using YAML instead of Python","text":"<p>For users that employ lots of Python functions in their DAGs, TaskFlow API represent a simpler way to transform functions into tasks, with a more intuitive way of passing data between them. They were  introduced in Airflow 2 as an alternative to Airflow traditional operators.</p> <p>The following section shows how to represent an Airflow DAG using TaskFlow API and how to define the same DAG using DAG Factory. Ultimately, both implementations use the same Airflow operators. The main difference is the language used to declare the workflow: one uses Python and the other uses YAML.</p>"},{"location":"comparison/taskflow_api/#goal","title":"Goal","text":"<p>Let's say we'd like to create a workflow that performs the following:</p> <ol> <li>Create a list of PyPI projects to be analysed.</li> <li>Fetch the statistics for each of these projects.</li> <li>Summarize the selected statistics as Markdown, using Python.</li> </ol> <p>We will implement all these steps using the Airflow <code>task</code> decorator, and the last task will generate a Markdown table similar to:</p> <pre><code>| package_name      |   last_day |   last_month |   last_week |\n|:------------------|-----------:|-------------:|------------:|\n| apache-airflow    |     852242 |     28194255 |     6253861 |\n| astronomer-cosmos |     442531 |     13354870 |     3127750 |\n| dag-factory       |      10078 |       354085 |       77752 |\n</code></pre> <p>The main logic is implemented as plain Python functions in pypi_stats.py:</p> pypi_stats.py<pre><code>def get_pypi_projects_list(**kwargs: dict[str, Any]) -&gt; list[str]:\n    \"\"\"\n    Return a list of PyPI project names to be analysed.\n    \"\"\"\n    projects_from_ui = kwargs.get(\"dag_run\").conf.get(\"pypi_projects\") if kwargs.get(\"dag_run\") else None\n    if projects_from_ui is None:\n        pypi_projects = DEFAULT_PYPI_PROJECTS\n    else:\n        pypi_projects = projects_from_ui\n    return pypi_projects\n\n\ndef fetch_pypi_stats_data(package_name: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Given a PyPI project name, return the PyPI stats data associated to it.\n    \"\"\"\n    url = f\"https://pypistats.org/api/packages/{package_name}/recent\"\n    package_json = httpx.get(url).json()\n    package_data = package_json[\"data\"]\n    package_data[\"package_name\"] = package_name\n    return package_data\n\n\ndef summarize(values: list[dict[str, Any]]):\n    \"\"\"\n    Given a list with PyPI stats data, create a table summarizing it, sorting by the last day total downloads.\n    \"\"\"\n    df = pd.DataFrame(values)\n    first_column = \"package_name\"\n    sorted_columns = [first_column] + [col for col in df.columns if col != first_column]\n    df = df[sorted_columns].sort_values(by=\"last_day\", ascending=False)\n    markdown_output = df.to_markdown(index=False)\n    print(markdown_output)\n    return markdown_output\n</code></pre>"},{"location":"comparison/taskflow_api/#implementation","title":"Implementation","text":"<p>As a reference, the following workflows run using Airflow 2.10.2 and DAG Factory 0.21.0.</p>"},{"location":"comparison/taskflow_api/#plain-airflow-python-dag","title":"Plain Airflow Python DAG","text":"example_pypi_stats_plain_airflow.py<pre><code>from __future__ import annotations\n\nfrom datetime import datetime\nfrom typing import Any\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom pypi_stats import fetch_pypi_stats_data, get_pypi_projects_list, summarize\n\nwith DAG(dag_id=\"example_pypi_stats_plain_airflow\", schedule=None, start_date=datetime(2022, 3, 4)) as dag:\n\n    @task\n    def get_pypi_projects_list_():\n        return get_pypi_projects_list()\n\n    @task\n    def fetch_pypi_stats_data_(project_name: str):\n        return fetch_pypi_stats_data(project_name)\n\n    @task\n    def summarize_(values: list[dict[str, Any]]):\n        return summarize(values)\n\n    pypi_stats_data = fetch_pypi_stats_data_.expand(project_name=get_pypi_projects_list_())\n    summarize_(pypi_stats_data)\n</code></pre>"},{"location":"comparison/taskflow_api/#alternative-dag-factory-yaml","title":"Alternative DAG Factory YAML","text":"example_pypi_stats_dagfactory.yml<pre><code>example_pypi_stats_dagfactory:\n  default_args:\n    start_date: 2022-03-04\n  tasks:\n    get_pypi_projects_list:\n      decorator: airflow.decorators.task\n      python_callable: pypi_stats.get_pypi_projects_list\n    fetch_pypi_stats_data:\n      decorator: airflow.decorators.task\n      python_callable: pypi_stats.fetch_pypi_stats_data\n      expand:\n        package_name: +get_pypi_projects_list\n    summarize:\n      decorator: airflow.decorators.task\n      python_callable: pypi_stats.summarize\n      values: +fetch_pypi_stats_data\n</code></pre>"},{"location":"comparison/taskflow_api/#comparison","title":"Comparison","text":""},{"location":"comparison/taskflow_api/#goal_1","title":"Goal","text":"<p>Both implementations accomplish the same goal and result in the expected Markdown table.</p>"},{"location":"comparison/taskflow_api/#airflow-graph-view","title":"Airflow Graph view","text":"<p>As shown in the screenshots below, both the DAG created using Python with standard Airflow and the DAG created using YAML and DAG Factory look identical, from a graph topology perspective, and also from the underlining operators being used.</p>"},{"location":"comparison/taskflow_api/#graph-view-plain-airflow-python-dag","title":"Graph view: Plain Airflow Python DAG","text":""},{"location":"comparison/taskflow_api/#graph-view-alternative-dag-factory-yaml","title":"Graph view: Alternative DAG Factory YAML","text":""},{"location":"comparison/taskflow_api/#airflow-dynamic-task-mapping","title":"Airflow Dynamic Task Mapping","text":"<p>In both workflows, we are dynamically generating a task for each PyPI repo.</p>"},{"location":"comparison/taskflow_api/#mapped-tasks-plain-airflow-python-dag","title":"Mapped Tasks: Plain Airflow Python DAG","text":""},{"location":"comparison/taskflow_api/#mapped-tasks-alternative-dag-factory-yaml","title":"Mapped Tasks: Alternative DAG Factory YAML","text":""},{"location":"comparison/taskflow_api/#airflow-code-view","title":"Airflow Code view","text":"<p>From an Airflow UI perspective, the content displayed in the \"Code\" view is the main difference between the two implementations. While Airflow renders the original Python DAG, as expected, in the case of the YAML DAGs, Airflow displays the Python file that references the DAG Factory YAML files:</p> example_load_yaml_dags.py<pre><code>import os\nfrom pathlib import Path\n\nfrom dagfactory import load_yaml_dags\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\nconfig_dir = str(CONFIG_ROOT_DIR / \"comparison\")\n\nload_yaml_dags(\n    globals_dict=globals(),\n    dags_folder=config_dir,\n)\n</code></pre>"},{"location":"comparison/taskflow_api/#code-view-plain-airflow-python-dag","title":"Code view: Plain Airflow Python DAG","text":""},{"location":"comparison/taskflow_api/#code-view-alternative-dag-factory-yaml","title":"Code view: Alternative DAG Factory YAML","text":"<p>To overcome this limitation, DAG Factory appends the YAML content to the DAG Documentation so users can better troubleshoot the DAG:</p> <p></p>"},{"location":"comparison/traditional_operators/","title":"Traditional Operators: Using YAML instead of Python","text":"<p>Traditionally, operators are Airflow's building blocks, and while they are robust and diverse, they can sometimes lead to boilerplate-heavy DAGs compared to the newer TaskFlow API.</p> <p>Most of the Airflow providers come with built-in traditional operators. Some examples include <code>BashOperator</code>, <code>PythonOperator</code>, <code>KubernetesPodOperator</code>, and  <code>PostgresOperator</code>.</p> <p>Below, we illustrate how to represent an Airflow DAG using traditional operators and how to define the same DAG using DAG Factory. Ultimately, both implementations use the same Airflow operators. The main difference is the language used to declare the workflow: one uses Python and the other uses YAML.</p>"},{"location":"comparison/traditional_operators/#goal","title":"Goal","text":"<p>Let's say we'd like to create a workflow that performs the following:</p> <ol> <li>Retrieve the top ten stories from Hacker News using the Hacker News API.</li> <li>Fetch the details for the two top stories using the Hacker News API.</li> <li>Summarize the selected stories as Markdown, using Python.</li> </ol> <p>We will implement the first two steps using <code>BashOperator</code> and the third step using <code>PythonOperator</code>. The last task will generate a <code>Markdown</code> snippet similar to:</p> <pre><code>| title                                                                       | url                                                                                                                    |\n|:----------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|\n| I keep turning my Google Sheets into phone-friendly webapps                 | https://arstechnica.com/gadgets/2024/12/making-tiny-no-code-webapps-out-of-spreadsheets-is-a-weirdly-fulfilling-hobby/ |\n| Coconut by Meta AI \u2013 Better LLM Reasoning with Chain of Continuous Thought? | https://aipapersacademy.com/chain-of-continuous-thought/                                                               |\n</code></pre> <p>The main logic is implemented as plain Python functions in hacker_news.py:</p> pypi_stats.py<pre><code>def summarize(**kwargs):\n    \"\"\"\n    Given the Airflow context is provided to this function, it will extract the XCom hackernews records from its\n    upstream tasks and summarise in Markdown.\n    \"\"\"\n    ti = kwargs[\"ti\"]\n    upstream_task_ids = ti.task.upstream_task_ids  # Get upstream task IDs dynamically\n    values = [json.loads(ti.xcom_pull(task_ids=task_id)) for task_id in upstream_task_ids]\n\n    df = pd.DataFrame(values)\n    selected_columns = [\"title\", \"url\"]\n    df = df[selected_columns]\n    markdown_output = df.to_markdown(index=False)\n    print(markdown_output)\n    return markdown_output\n</code></pre>"},{"location":"comparison/traditional_operators/#implementation","title":"Implementation","text":"<p>As a reference, the following workflows run using Airflow 2.10.2 and DAG Factory 0.21.0.</p>"},{"location":"comparison/traditional_operators/#plain-airflow-python-dag","title":"Plain Airflow Python DAG","text":"example_hackernews_plain_airflow.py<pre><code>from datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python import PythonOperator\nfrom hacker_news import summarize\n\nwith DAG(dag_id=\"example_hackernews_plain_airflow\", schedule=None, start_date=datetime(2022, 3, 4)) as dag:\n\n    fetch_top_ten_news = BashOperator(\n        task_id=\"fetch_top_ten_news\",\n        bash_command=\"curl -s https://hacker-news.firebaseio.com/v0/topstories.json  | jq -c -r '.[0:10]'\",\n    )\n\n    fetch_first_top_news = BashOperator(\n        task_id=\"fetch_first_top_news\",\n        bash_command=\"\"\"\n            echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[0]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\n        \"\"\",\n    )\n\n    fetch_second_top_news = BashOperator(\n        task_id=\"fetch_second_news\",\n        bash_command=\"\"\"\n            echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[1]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\n        \"\"\",\n    )\n\n    summarize = PythonOperator(task_id=\"summarize\", python_callable=summarize)\n\n    fetch_top_ten_news &gt;&gt; [fetch_first_top_news, fetch_second_top_news] &gt;&gt; summarize\n</code></pre>"},{"location":"comparison/traditional_operators/#alternative-dag-factory-yaml","title":"Alternative DAG Factory YAML","text":"example_hackernews_dagfactory.py<pre><code>example_hackernews_dagfactory:\n  default_args:\n    start_date: 2022-03-04\n  tasks:\n    fetch_top_ten_news:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"curl -s https://hacker-news.firebaseio.com/v0/topstories.json  | jq -c -r '.[0:10]'\"\n    fetch_first_top_news:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[0]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\"\n      dependencies: [fetch_top_ten_news]\n    fetch_second_top_news:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[1]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\"\n      dependencies: [fetch_top_ten_news]\n    summarize:\n      operator: airflow.operators.python.PythonOperator\n      python_callable: hacker_news.summarize\n      dependencies: [fetch_first_top_news, fetch_second_top_news]\n</code></pre>"},{"location":"comparison/traditional_operators/#comparison","title":"Comparison","text":""},{"location":"comparison/traditional_operators/#goal_1","title":"Goal","text":"<p>Both implementations accomplish the same goal and result in the expected Markdown table.</p>"},{"location":"comparison/traditional_operators/#airflow-graph-view","title":"Airflow Graph view","text":"<p>As shown in the screenshots below, both the DAG created using Python with standard Airflow and the DAG created using YAML and DAG Factory look identical, from a graph topology perspective, and also from the underlining operators being used.</p>"},{"location":"comparison/traditional_operators/#graph-view-plain-airflow-python-dag","title":"Graph view: Plain Airflow Python DAG","text":""},{"location":"comparison/traditional_operators/#graph-view-alternative-dag-factory-yaml","title":"Graph view: Alternative DAG Factory YAML","text":""},{"location":"comparison/traditional_operators/#airflow-code-view","title":"Airflow Code view","text":"<p>From an Airflow UI perspective, the content displayed in the \"Code\" view is the main difference between the two implementations. While Airflow renders the original Python DAG, as expected, in the case of the YAML DAGs, Airflow displays the Python file that references the DAG Factory YAML files:</p> example_load_yaml_dags.py<pre><code>import os\nfrom pathlib import Path\n\nfrom dagfactory import load_yaml_dags\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\nconfig_dir = str(CONFIG_ROOT_DIR / \"comparison\")\n\nload_yaml_dags(\n    globals_dict=globals(),\n    dags_folder=config_dir,\n)\n</code></pre>"},{"location":"comparison/traditional_operators/#code-view-plain-airflow-python-dag","title":"Code view: Plain Airflow Python DAG","text":""},{"location":"comparison/traditional_operators/#code-view-alternative-dag-factory-yaml","title":"Code view: Alternative DAG Factory YAML","text":"<p>To overcome this limitation, DAG Factory appends the YAML content to the DAG Documentation so users can better troubleshoot the DAG:</p> <p></p>"},{"location":"configuration/configuring_workflows/","title":"Configuring Your Workflows","text":"<p>DAG Factory allows you to define workflows in a structured, configuration-driven way using YAML files. You can define multiple workflows within a single YAML file based on your requirements.</p>"},{"location":"configuration/configuring_workflows/#key-elements-of-workflow-configuration","title":"Key Elements of Workflow Configuration","text":"<ul> <li>dag_id: Unique identifier for your DAG.</li> <li>default_args: Common arguments for all tasks.</li> <li>schedule/schedule_interval: Specifies the execution schedule.</li> <li>tasks: Defines the Airflow tasks in your workflow.</li> </ul>"},{"location":"configuration/configuring_workflows/#example-dag-configuration","title":"Example DAG Configuration","text":"example_dag_factory.yml<pre><code>basic_example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule_interval: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n</code></pre>"},{"location":"configuration/configuring_workflows/#check-out-more-configuration-params","title":"Check out more configuration params","text":"<ul> <li>Environment variables</li> <li>Defaults</li> </ul>"},{"location":"configuration/defaults/","title":"Defaults","text":"<p>DAG Factory allows you to define Airflow default_args and additional DAG-level arguments in a <code>default</code> block. This block enables you to share common settings and configurations across all DAGs in your YAML configuration, with the arguments automatically applied to each DAG defined in the file. This is one of DAG Factory's most powerful features; using defaults allows for the dynamic generation of more than a single DAG.</p>"},{"location":"configuration/defaults/#benefits-of-using-the-default-block","title":"Benefits of using the default block","text":"<ul> <li>Consistency: Ensures uniform configurations across all tasks and DAGs.</li> <li>Maintainability: Reduces duplication by centralizing common properties.</li> <li>Simplicity: Makes configurations easier to read and manage.</li> <li>Dynamic Generation: Use a single default block to easily generate more than a single DAG.</li> </ul>"},{"location":"configuration/defaults/#example-usage-of-a-default-block-for-default_args","title":"Example usage of a default block for <code>default_args</code>","text":""},{"location":"configuration/defaults/#specifying-default_args-in-the-default-block","title":"Specifying <code>default_args</code> in the <code>default</code> block","text":"<p>Using a <code>default</code> block in a YAML file allows for those key-value pairs to be applied to each DAG that is defined in    that same file. One of the most common examples is using a <code>default</code> block to specify <code>default_args</code> for each DAG    defined in that file. These arguments are automatically inherited by every DAG defined in the file. Below is an example of this.</p> Usage of default block for default_args in YAML<pre><code>default:\n  default_args:\n    start_date: '2024-01-01'\n  schedule_interval: 0 0 * * *\n  catchup: false\n  tags:\n    - \"data engineering\"\n\netl:\n  tasks:\n    extract:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo extract\"\n    transform:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo transform\"\n      dependencies:\n      - extract\n    load:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo load\"\n      dependencies:\n      - transform\n</code></pre>"},{"location":"configuration/defaults/#specifying-default_args-directly-in-a-dag-configuration","title":"Specifying <code>default_args</code> directly in a DAG configuration","text":"<p>You can override or define specific <code>default_args</code> at the individual DAG level. This allows you to customize    arguments for each DAG without affecting others. Not only can existing <code>default_args</code> be overridden directly in a DAG    configuration, but new arguments can be added.</p> <p><code>yaml    etl:      default_args:        start_date: '2024-12-31'        retries: 1  # A new default_arg was added   ...</code></p>"},{"location":"configuration/defaults/#specifying-default_args-in-a-shared-defaultsyml","title":"Specifying <code>default_args</code> in a shared <code>defaults.yml</code>","text":"<p>Starting DAG Factory 0.22.0, you can also keep the <code>default_args</code> in the <code>defaults.yml</code> file. The configuration    from <code>defaults.yml</code> will be applied to all DAG Factory generated DAGs. Be careful, these will be applied to all    generated DAGs.</p> defaults.yml<pre><code>default_args:\n  start_date: \"2025-01-01\"\n  owner: \"global_owner\"\n</code></pre> <p>Given the various ways to specify <code>default_args</code>, the following precedence order is applied when arguments are    duplicated:</p> <ol> <li>In the DAG configuration</li> <li>In the <code>default</code> block within the workflow's YAML file</li> <li>In the <code>defaults.yml</code></li> </ol>"},{"location":"configuration/defaults/#example-using-of-default-block-for-dynamic-dag-generation","title":"Example using of default block for dynamic DAG generation","text":"<p>Not only can the <code>default</code> block in a YAML file be used to define <code>default_args</code> for one or more DAGs; it can also be used to create the skeleton of \"templated\" DAGs. In the example below, the <code>default</code> block is used to define not only the <code>default_args</code> of a DAG, but also default Tasks. These Tasks provide a \"template\" for the DAGs defined in this file. Each DAG (<code>machine_learning</code>, <code>data_science</code>, <code>artificial_intelligence</code>) will be defined using the values from the <code>default</code> block, and like with <code>default_args</code>, can override these values. This is a powerful way to use DAG Factory to dynamically create DAGs using a single configuration.</p> Usage of default block in YAML<pre><code>default:\n  default_args:\n    start_date: '2024-01-01'\n  schedule_interval: 0 0 * * *\n  catchup: false\n  tags:\n    - dynamic\n  tasks:\n    extract:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo extract\"\n    transform:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo transform\"\n      dependencies:\n      - extract\n    load:\n      operator: airflow.operators.bash_operator.BashOperator\n      dependencies:\n      - transform\n\n\nmachine_learning:\n  tasks:\n    load:\n      bash_command: \"echo machine_larning\"\n\ndata_science:\n  tasks:\n    load:\n      bash_command: \"echo data_science\"\n\nartificial_intelligence:\n  tasks:\n    load:\n      bash_command: \"echo artificial_intelligence\"\n</code></pre> <p>Currently, only <code>default_args</code> can be specified using the <code>defaults.yml</code> file.</p>"},{"location":"configuration/environment_variables/","title":"Environment variables","text":"<p>Starting release <code>0.20.0</code>, DAG Factory introduces support for referencing environment variables directly within YAML configuration files. This enhancement enables dynamic configuration paths and enhances workflow portability by resolving environment variables during DAG parsing.</p> <p>With this feature, DAG Factory removes the reliance on hard-coded paths, allowing for more flexible and adaptable configurations that work seamlessly across various environments.</p>"},{"location":"configuration/environment_variables/#example-yaml-configuration-with-environment-variables","title":"Example YAML Configuration with Environment Variables","text":"Reference environment variable in YAML<pre><code>example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule_interval: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  dag_display_name: \"Pretty Example DAG\"\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.python_operator.PythonOperator\n      python_callable_name: print_hello\n      python_callable_file: $CONFIG_ROOT_DIR/print_hello.py\n      dependencies: [task_1]\n</code></pre> <p>In the above example, <code>$CONFIG_ROOT_DIR</code> is used to reference an environment variable that points to the root directory of your DAG configurations. During DAG parsing, it will be resolved to the value specified for the <code>CONFIG_ROOT_DIR</code> environment variable.</p>"},{"location":"contributing/code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"contributing/code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socioeconomic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"contributing/code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"contributing/code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"contributing/code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contributing/code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at humans@astronomer.io.</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"contributing/code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"contributing/code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"contributing/code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"contributing/code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"contributing/code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"contributing/code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ. Translations are available at this page.</p>"},{"location":"contributing/contributors/","title":"Contributors","text":"<p>There are different ways people can contribute to DAG Factory. Learn more about the project contributors roles.</p>"},{"location":"contributing/contributors/#committers","title":"Committers","text":"<ul> <li>Pankaj Koti (@pankajkoti)</li> <li>Pankaj Singh (@pankajastro)</li> <li>Tatiana Al-Chueyr (@tatiana)</li> </ul>"},{"location":"contributing/contributors/#emeritus-committers","title":"Emeritus Committers","text":"<ul> <li>Adam Boscarino (@ajbosco)</li> </ul>"},{"location":"contributing/contributors/#contributors_1","title":"Contributors","text":"<p>Many people are improving DAG Factory each day. Find more contributors in our GitHub page.</p>"},{"location":"contributing/howto/","title":"Contributing Guide","text":"<p>All contributions, bug reports, bug fixes, documentation improvements, and enhancements are welcome.</p> <p>All contributors and maintainers to this project should abide by the Contributor Code of Conduct.</p> <p>Learn more about the contributors' roles in the Roles page.</p> <p>This document describes how to contribute to DAG Factory, covering:</p> <ul> <li>Overview of how to contribute</li> <li>How to set up the local development environment</li> <li>Running tests</li> <li>Pre-commit and linting</li> <li>Authoring the documentation</li> <li>Releasing</li> </ul>"},{"location":"contributing/howto/#overview-of-how-to-contribute","title":"Overview of how to contribute","text":"<p>To contribute to the DAG Factory project:</p> <ol> <li>Please create a GitHub Issue describing a bug, enhancement, or feature request.</li> <li>Open a branch off of the <code>main</code> branch and create a Pull Request into the <code>main</code> branch from your feature branch.</li> <li>Link your issue to the pull request.</li> <li>After you complete development on your feature branch, request a review. A maintainer will merge your PR after all reviewers approve it.</li> </ol>"},{"location":"contributing/howto/#set-up-a-local-development-environment","title":"Set up a local development environment","text":""},{"location":"contributing/howto/#requirements","title":"Requirements","text":"<ul> <li>Git</li> <li>Python &lt;= 3.12 (due to dependencies, such as <code>google-re2</code> not supporting Python 3.13 yet)</li> <li>Hatch</li> </ul> <p>Clone the DAG Factory repository and change the current working directory to the repo's root directory:</p> <pre><code>git clone https://github.com/astronomer/dag-factory.git\ncd dag-factory/\n</code></pre> <p>After cloning the project, there are two options for setting up the local development environment:</p> <ul> <li>Use a Python virtual environment, or</li> <li>Use Docker</li> </ul>"},{"location":"contributing/howto/#using-a-python-virtual-environment-for-local-development","title":"Using a Python virtual environment for local development","text":"<ol> <li> <p>Install the project dependencies:</p> <pre><code>make setup\n</code></pre> </li> <li> <p>Activate the local python environment:</p> <pre><code>source venv/bin/activate\n</code></pre> </li> <li> <p>Set Apache Airflow\u00ae home to the <code>dev/</code>, so you can see DAG Factory example DAGs. Disable loading Airflow standard example DAGs:</p> </li> </ol> <pre><code>export AIRFLOW_HOME=$(pwd)/dev/\nexport AIRFLOW__CORE__LOAD_EXAMPLES=false\n</code></pre> <p>Then, run Airflow in standalone mode; the command below will create a new user (if it does not exist) and run the necessary Airflow component (webserver, scheduler and triggered):</p> <p>Note: By default, Airflow will use sqlite as a database; you can override this by setting the variable <code>AIRFLOW__DATABASE__SQL_ALCHEMY_CONN</code> to the SQL connection string.</p> <pre><code>airflow standalone\n</code></pre> <p>After Airflow is running, you can access the Airflow UI at <code>http://localhost:8080</code>.</p> <p>Note: whenever you want to start the development server, you need to activate the <code>virtualenv</code> and set the <code>environment variables</code></p>"},{"location":"contributing/howto/#use-docker-for-local-development","title":"Use Docker for local development","text":"<p>It is also possible to build the development environment using Docker:</p> <pre><code>make docker-run\n</code></pre> <p>After the sandbox is running, you can access the Airflow UI at <code>http://localhost:8080</code>.</p> <p>This approach builds a DAG Factory wheel, so if there are code changes, you must stop and restart the containers:</p> <pre><code>make docker-stop\n</code></pre>"},{"location":"contributing/howto/#testing-application-with-hatch","title":"Testing application with hatch","text":"<p>The tests are developed using PyTest and run using hatch.</p> <p>The pyproject. toml file currently defines a matrix of supported versions of Python and Airflow against which a user can run the tests.</p>"},{"location":"contributing/howto/#run-unit-tests","title":"Run unit tests","text":"<p>To run unit tests using Python 3.10 and Airflow 2.5, use the following:</p> <pre><code>hatch run tests.py3.10-2.5:test-cov\n</code></pre> <p>It is also possible to run the tests using all the matrix combinations, by using:</p> <pre><code>hatch run tests:test-cov\n</code></pre>"},{"location":"contributing/howto/#run-integration-tests","title":"Run integration tests","text":"<p>Note: these tests create local Python virtual environments in a hatch-managed directory. They also use the user-defined <code>AIRFLOW_HOME</code>, overriding any pre-existing <code>airflow.cfg</code> and <code>airflow.db</code> files.</p> <p>First, set the following environment variables:</p> <pre><code>export AIRFLOW_HOME=$(pwd)/dev/\nexport CONFIG_ROOT_DIR=`pwd`\"/dev/dags\"\nexport PYTHONPATH=dev/dags:$PYTHONPATH\n</code></pre> <p>To run the integration tests using Python 3.9 and Airflow 2.9, use</p> <pre><code>hatch run tests.py3.9-2.9:test-integration-setup\nhatch run tests.py3.9-2.9:test-integration\n</code></pre>"},{"location":"contributing/howto/#pre-commit-and-linting","title":"Pre-Commit and linting","text":"<p>We use pre-commit to run several checks on the code before committing. To install pre-commit hooks, run:</p> <pre><code>pre-commit install\n</code></pre> <p>To run the checks manually, run the following:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>Pre-commit runs several static checks, including Black and Ruff. It is also possible to run them using <code>hatch</code>:</p> <pre><code>hatch run tests.py3.9-2.9:static-check\n</code></pre>"},{"location":"contributing/howto/#write-docs","title":"Write docs","text":"<p>We use Markdown to author DAG Factory documentation.</p> <p>Similar to running tests, we also use hatch to manage the documentation.</p> <p>To build and serve the documentation locally:</p> <pre><code>hatch run docs:dev\n</code></pre> <p>To release the documentation with the current project version and set it to the latest:</p> <pre><code>hatch run docs:gh-release\n</code></pre>"},{"location":"contributing/howto/#releasing","title":"Releasing","text":"<p>We currently use hatch for building and distributing <code>dag-factory</code>.</p> <p>We use GitHub actions to create and deploy new releases. To create a new release, update the latest release version.</p> <p>It is possible to update the version either by using hatch:</p> <p>Note: You can update the version in several different ways. To learn more, check out the hatch docs.</p> <pre><code>hatch version minor\n</code></pre> <p>Or by manually updating the value of <code>__version__</code> in <code>dagfactory/__init__.py</code>.</p> <p>Make sure the CHANGELOG file is up-to-date.</p> <p>Create a release using the GitHub UI. GitHub will update the package directly to PyPI.</p> <p>If you're a project maintainer in PyPI, it is also possible to create a release manually, by authenticating to PyPI and running the commands:</p> <pre><code>hatch build\nhatch publish\n</code></pre>"},{"location":"contributing/roles/","title":"Contributor roles","text":"<p>Contributors are welcome and are greatly appreciated! Every little bit helps, and we give credit to them.</p> <p>This document aims to explain the current roles in the DAG Factory project. For more information, check the contributing docs.</p>"},{"location":"contributing/roles/#contributors","title":"Contributors","text":"<p>A contributor is anyone who wants to contribute code, documentation, tests, ideas, or anything to the DAG Factory project.</p> <p>DAG Factory contributors are listed in the Github insights page.</p> <p>Contributors are responsible for:</p> <ul> <li>Fixing bugs</li> <li>Refactoring code</li> <li>Improving processes and tooling</li> <li>Adding features</li> <li>Improving the documentation</li> </ul>"},{"location":"contributing/roles/#committers","title":"Committers","text":"<p>Committers are community members with write access to the DAG Factory GitHub repository. They can modify the code and the documentation and accept others' contributions to the repo.</p> <p>Check contributors for the official list of DAG Factory committers.</p> <p>Committers have the same responsibilities as standard contributors and also perform the following actions:</p> <ul> <li>Reviewing &amp; merging pull-requests</li> <li>Scanning and responding to GitHub issues, helping triaging them</li> </ul> <p>If you know you are not going to be able to contribute for a long time (for instance, due to a change of job or circumstances), you should inform other maintainers, and we will mark you as \"emeritus\". Emeritus committers will no longer have write access to the repo. As merit earned never expires, once an emeritus committer becomes active again, they can simply email another maintainer from Astronomer and ask to be reinstated.</p>"},{"location":"contributing/roles/#pre-requisites-to-becoming-a-committer","title":"Pre-requisites to becoming a committer","text":"<p>General prerequisites that we look for in all candidates:</p> <ol> <li>Consistent contribution over last few months</li> <li>Visibility on discussions on the Slack channel or GitHub issues/discussions</li> <li>Contributes to community health and project's sustainability for the long-term</li> <li>Understands the project's contributors' guidelines. Astronomer is responsible and accountable for releasing new versions of DAG Factory in PyPI, following the milestones. Astronomer has the right to grant and revoke write access permissions to the project's official repository for any reason it sees fit.</li> </ol>"},{"location":"features/callbacks/","title":"Callbacks","text":"<p>DAG Factory supports the use of callbacks. These callbacks can be set at the DAG, TaskGroup, or Task level. The way that callbacks that can be configured for DAGs, TaskGroups, and Tasks differ slightly, and details around this can be  found in the Apache Airflow documentation.</p> <p>Within DAG Factory itself, there are three approaches to defining callbacks. The goal is to make this process  intuitive and provide parity with the traditional DAG authoring experience. These approaches to configure callbacks are outlined below, each with an example of implementation. While proceeding examples are all defined for individual  Tasks, callbacks can also be defined using <code>default_args</code>, or at the DAG and TaskGroup level.</p> <ul> <li>Passing a string that points to a callable</li> <li>Specifying a user-defined <code>.py</code> and the function within that file to be executed</li> <li>Configuring callbacks from providers</li> </ul>"},{"location":"features/callbacks/#passing-a-string-that-points-to-a-callable","title":"Passing a string that points to a callable","text":"<p>The most traditional way of configuring callbacks is by defining a custom function within the Airflow project and  assigning that callback to the desired Task. Using the syntax below, this can be implemented using DAG Factory. In this  case, the <code>output_standard_message</code> function is a user-defined function stored in the <code>include/custom_callbacks.py</code>  file. This function requires no parameters, and the YAML would take the form below.</p> <p>For this example to be implemented in DAG Factory, the <code>include/custom_callbacks.py</code> file must be on the Python  <code>sys.path</code>. If this is not the case, the full path to a <code>.py</code> function can be specified, as shown below.</p> <pre><code>...\n\n  task_1:\n    operator: airflow.operators.bash_operator.BashOperator\n    bash_command: \"echo task_1\"\n    on_failure_callback: include.custom_callbacks.output_standard_message\n...\n</code></pre> <p>Sometimes, a function may have parameters that need to be defined within the Task itself. Here, the  <code>output_custom_message</code> callback takes two key-word arguments; <code>param1</code>, and <code>param2</code>. These values are defined in the  YAML itself, offering DAG Factory authors an additional degree of flexibility and verbosity.</p> <pre><code>...\n\n  task_2:\n    operator: airflow.operators.bash_operator.BashOperator\n    bash_command: \"echo task_2\"\n    on_success_callback:\n      callback: include.custom_callbacks.output_custom_message\n      param1: \"Task status\"\n      param2: \"Successful!\"\n...\n</code></pre>"},{"location":"features/callbacks/#specifying-a-user-defined-py-file-and-function","title":"Specifying a user-defined <code>.py</code> file and function","text":"<p>In addition to passing a string that points to a callback, the full path to the file and name of the callback can be  specified for a DAG, TaskGroup, or Task. This provides a viable option for defining a callback when the director the  <code>.py</code> file is stored in is not on the Python path.</p> <pre><code>...\n\n  task_3:\n    operator: airflow.operators.bash_operator.BashOperator\n    bash_command: \"echo task_3\"\n    on_retry_callback_name: output_standard_message\n    on_retry_callback_file: /usr/local/airflow/include/custom_callbacks.py\n...\n</code></pre> <p>Note that this method for defining callbacks in DAG Factory does not allow for parameters to be passed to the callable within the YAML itself.</p>"},{"location":"features/callbacks/#provider-callbacks","title":"Provider callbacks","text":"<p>In addition to custom-built callbacks, there are a number of provider-built callbacks that can be used when defining a  DAG. With DAG Factory, these callbacks can be configured similar to how they would be when authoring a traditional DAG.  First, the type of callback is specified (<code>on_success_callback</code>, <code>on_failure_callback</code>, etc.). The <code>callback</code> key-value pair specifies the provider-built function to be executed. Then, the specific key-word arguments the callback takes can be specified, as shown below.</p> <p>Note that the provider package being used must be available on the Python <code>sys.path</code> path, meaning it may need to be <code>pip installed</code>.</p> <pre><code>...\n  task_4:\n    operator: airflow.operators.bash_operator.BashOperator\n    bash_command: \"echo task_4\"\n    on_failure_callback:\n      callback: airflow.providers.slack.notifications.slack.send_slack_notification\n      slack_conn_id: slack_conn_id\n      text: |\n        :red_circle: Task Failed.\n        This task has failed and needs to be addressed.\n        Please remediate this issue ASAP.\n      channel: \"#channel\"\n...\n</code></pre>"},{"location":"features/custom_operators/","title":"Custom Operators","text":"<p>DAG-Factory supports custom operators. To leverage, set the path to the custom operator within the <code>operator</code> key in the configuration file. You can add any additional parameters that the custom operator requires.</p> <pre><code>...\n  tasks:\n    begin:\n      operator: airflow.operators.empty.EmptyOperator\n    make_bread_1:\n      operator: customized.operators.breakfast_operators.MakeBreadOperator\n      bread_type: 'Sourdough'\n</code></pre> <p></p>"},{"location":"features/datasets/","title":"Datasets","text":"<p>DAG Factory supports Airflow\u2019s Datasets.</p>"},{"location":"features/datasets/#datasets-outlets-and-inlets","title":"Datasets Outlets and Inlets","text":"<p>To leverage datasets, you need to specify the <code>Dataset</code> in the <code>outlets</code> and <code>inlets</code> keys in the configuration file.  The <code>outlets</code> and <code>inlets</code> keys should contain a list of strings representing dataset locations. In the <code>schedule</code> key of the consumer DAG, you can set the <code>Dataset</code> that the DAG should be scheduled against. The key should contain a list of dataset locations. The consumer DAG will run when all the specified datasets become avai</p>"},{"location":"features/datasets/#example-outlet-and-inlet","title":"Example: Outlet and Inlet","text":"example_dag_datasets_outlet_inlet.yml<pre><code>producer_dag:\n  default_args:\n    owner: \"example_owner\"\n    retries: 1\n    start_date: '2024-01-01'\n  description: \"Example DAG producer simple datasets\"\n  schedule_interval: \"0 5 * * *\"\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n      inlets: [ 's3://bucket_example/raw/dataset1_source.json' ]\n      outlets: [ 's3://bucket_example/raw/dataset1.json' ]\n    task_2:\n      bash_command: \"echo 2\"\n      dependencies: [ task_1 ]\n      inlets: [ 's3://bucket_example/raw/dataset2_source.json' ]\n      outlets: [ 's3://bucket_example/raw/dataset2.json' ]\nconsumer_dag:\n  default_args:\n    owner: \"example_owner\"\n    retries: 1\n    start_date: '2024-01-01'\n  description: \"Example DAG consumer simple datasets\"\n  schedule: [ 's3://bucket_example/raw/dataset1.json', 's3://bucket_example/raw/dataset2.json' ]\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 'consumer datasets'\"\n</code></pre>"},{"location":"features/datasets/#conditional-dataset-scheduling","title":"Conditional Dataset Scheduling","text":""},{"location":"features/datasets/#minimum-requirements","title":"Minimum Requirements:","text":"<ul> <li>dag-factory 0.22.0+</li> <li>Apache Airflow\u00ae 2.9+</li> </ul>"},{"location":"features/datasets/#logical-operators-for-datasets","title":"Logical operators for datasets","text":"<p>Airflow supports two logical operators for combining dataset conditions:</p> <ul> <li>AND (<code>&amp;</code>): Specifies that the DAG should be triggered only after all of the specified datasets have been updated.</li> <li>OR (<code>|</code>): Specifies that the DAG should be triggered when any of the specified datasets is updated.</li> </ul> <p>These operators enable you to configure your Airflow workflows to use more complex dataset update conditions, making them more dynamic and flexible.</p>"},{"location":"features/datasets/#examples-of-conditional-dataset-scheduling","title":"Examples of Conditional Dataset Scheduling","text":"<p>Below are examples demonstrating how to configure a consumer DAG using conditional dataset scheduling.</p>"},{"location":"features/datasets/#example-1-string-condition","title":"Example 1: String Condition","text":"example_dataset_condition_string.yml<pre><code>consumer_dag:\n  default_args:\n    owner: \"example_owner\"\n    retries: 1\n    start_date: '2024-01-01'\n  description: \"Example DAG consumer simple datasets\"\n  schedule:\n    datasets: \"((s3://bucket-cjmm/raw/dataset_custom_1 &amp; s3://bucket-cjmm/raw/dataset_custom_2) | s3://bucket-cjmm/raw/dataset_custom_3)\"\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 'consumer datasets'\"\n</code></pre>"},{"location":"features/datasets/#example-2-yaml-syntax","title":"Example 2: YAML Syntax","text":"example_dataset_yaml_syntax.yml<pre><code>consumer_dag:\n  default_args:\n    owner: \"example_owner\"\n    retries: 1\n    start_date: '2024-01-01'\n  description: \"Example DAG consumer simple datasets\"\n  schedule:\n    datasets:\n      !or\n        - !and\n          - \"s3://bucket-cjmm/raw/dataset_custom_1\"\n          - \"s3://bucket-cjmm/raw/dataset_custom_2\"\n        - \"s3://bucket-cjmm/raw/dataset_custom_3\"\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 'consumer datasets'\"\n</code></pre>"},{"location":"features/datasets/#visualization","title":"Visualization","text":"<p>The following diagrams illustrate the dataset conditions described in the example configurations:</p> <ol> <li><code>s3://bucket-cjmm/raw/dataset_custom_1</code> and <code>s3://bucket-cjmm/raw/dataset_custom_2</code> must both be updated for the first condition to be satisfied.</li> <li>Alternatively, <code>s3://bucket-cjmm/raw/dataset_custom_3</code> alone can satisfy the condition.</li> </ol> <p> </p>"},{"location":"features/dynamic_tasks/","title":"Dynamic tasks","text":"<p>DAG Factory supports Airflow\u2019s Dynamic Task Mapping, enabling workflows to dynamically create tasks at runtime.  This approach allows the number of tasks to be determined during execution, usually based on the outcome of a preceding task, rather than being predefined during DAG authoring.</p>"},{"location":"features/dynamic_tasks/#example-defining-dynamic-tasks","title":"Example: Defining Dynamic Tasks","text":"<p>Below is an example configuration for implementing dynamic tasks using DAG Factory:</p> example_dynamic_task_mapping.yml<pre><code>test_expand:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"test expand\"\n  schedule_interval: \"0 3 * * *\"\n  default_view: \"graph\"\n  tasks:\n    process:\n      operator: airflow.operators.python_operator.PythonOperator\n      python_callable_name: consume_value\n      python_callable_file: $CONFIG_ROOT_DIR/expand_tasks.py\n      partial:\n        op_kwargs:\n          fixed_param: \"test\"\n      expand:\n        op_args:\n            request.output\n      dependencies: [request]\n    # This task is intentionally placed after the \"process\" task to demonstrate that DAG Factory does not require tasks\n    # to be topologically ordered in the YAML file according to their dependencies.\n    request:\n      operator: airflow.operators.python.PythonOperator\n      python_callable_name: make_list\n      python_callable_file: $CONFIG_ROOT_DIR/expand_tasks.py\n</code></pre>"},{"location":"features/dynamic_tasks/#explanation-of-the-configuration","title":"Explanation of the Configuration","text":"<ol> <li> <p><code>request</code> Task:</p> <ul> <li>Generates a list of items using the <code>make_list</code> function from the expand_tasks.py module.</li> <li>This task serves as the input provider for the dynamically mapped tasks.</li> </ul> </li> <li> <p><code>process</code> Task:</p> <ul> <li>Dynamically generates one task for each item in the list produced by the <code>request</code> task.</li> <li>The expand argument is used to create these tasks at runtime, with <code>request.output</code> supplying the input list.</li> <li>Additionally, the <code>partial</code> argument is used to specify fixed parameters (<code>op_kwargs</code>) that are applied to all dynamically generated tasks.</li> </ul> </li> </ol>"},{"location":"features/dynamic_tasks/#how-it-works","title":"How It Works","text":"<ul> <li> <p>Dynamic Task Creation:     The <code>expand</code> keyword allows the process task to spawn multiple tasks at runtime, each processing a single item from the list output of the <code>request</code> task.</p> </li> <li> <p>Fixed Parameters:     The partial keyword ensures that common parameters, such as <code>fixed_param</code>, are passed to every dynamically created task instance.</p> </li> </ul>"},{"location":"features/dynamic_tasks/#benefits-of-dynamic-task-mapping-with-dag-factory","title":"Benefits of Dynamic Task Mapping with DAG Factory","text":"<ul> <li>Flexibility: Handle varying input sizes and conditions dynamically without modifying the DAG definition.</li> <li>Scalability: Efficiently process large datasets by leveraging Airflow\u2019s parallel execution capabilities.</li> <li>Simplicity: Define dynamic workflows declaratively using YAML, minimizing boilerplate code.</li> </ul>"},{"location":"features/dynamic_tasks/#airflow-mapped-tasks-view","title":"Airflow mapped tasks view","text":"<p>Below, you can see a list of mapped tasks generated dynamically as part of the <code>process</code> task.</p> <p></p>"},{"location":"features/dynamic_tasks/#advanced-dynamic-task-mapping-with-dag-factory","title":"Advanced Dynamic Task Mapping with DAG Factory","text":"<p>Below, we explain the different methods for defining dynamic task mapping, illustrated by the provided example configuration.</p> Dynamic Task Mapping advanced usage<pre><code>example_taskflow:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"Example of TaskFlow powered DAG that includes dynamic task mapping\"\n  schedule_interval: \"0 3 * * *\"\n  default_view: \"graph\"\n  tasks:\n    some_number:\n      decorator: airflow.decorators.task\n      python_callable: sample.some_number\n    numbers_list:\n      decorator: airflow.decorators.task\n      python_callable_name: build_numbers_list\n      python_callable_file: $CONFIG_ROOT_DIR/sample.py\n    another_numbers_list:\n      decorator: airflow.decorators.task\n      python_callable: sample.build_numbers_list\n    double_number_from_arg:\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      number: 2\n    double_number_from_task:\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      number: +some_number  # the prefix + leads to resolving this value as the task `some_number`, previously defined\n    double_number_with_dynamic_task_mapping_static:\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      expand:\n          number:\n            - 1\n            - 3\n            - 5\n    double_number_with_dynamic_task_mapping_taskflow:\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      expand:\n          number: +numbers_list  # the prefix + tells DagFactory to resolve this value as the task `numbers_list`, previously defined\n    multiply_with_multiple_parameters:\n      decorator: airflow.decorators.task\n      python_callable: sample.multiply\n      expand:\n          a: +numbers_list  # the prefix + tells DagFactory to resolve this value as the task `numbers_list`, previously defined\n          b: +another_numbers_list # the prefix + tells DagFactory to resolve this value as the task `another_numbers_list`, previously defined\n    double_number_with_dynamic_task_and_partial:\n      decorator: airflow.decorators.task\n      python_callable: sample.double_with_label\n      expand:\n          number: +numbers_list  # the prefix + tells DagFactory to resolve this value as the task `numbers_list`, previously defined\n      partial:\n          label: True\n</code></pre> <p>The example above illustrates advanced usage of Dynamic Task Mapping using Dag Factory (the callable functions used in the example are kept in sample.py):</p> <ol> <li> <p>Static Input Mapping</p> <p>The task <code>double_number_with_dynamic_task_mapping_static</code> shows how dynamic tasks can be created using static lists as input. Three tasks are created, each processing one number.</p> </li> <li> <p>Task-Generated Input Mapping</p> <p>The task <code>double_number_with_dynamic_task_mapping_taskflow</code> shows how tasks can use outputs from other tasks as input for dynamic task mapping. The prefix <code>+</code> tells DAG Factory to resolve this value as the task <code>numbers_list</code>, previously defined.</p> </li> <li> <p>Mapping with Multiple Inputs</p> <p>The task <code>multiply_with_multiple_parameters</code> shows how dynamic task mapping can combine outputs from multiple tasks as input parameters.</p> </li> </ol>"},{"location":"features/dynamic_tasks/#named-mapping-in-dynamic-tasks-with-dag-factory","title":"Named Mapping in Dynamic Tasks with DAG Factory","text":"<p>Starting with Airflow 2.9, the <code>map_index_template</code> feature allows for custom mapping name for dynamic tasks based on a user-defined key. DAG Factory fully supports this feature, enabling users to name tasks dynamically in a meaningful way during runtime. This can be useful for tracing and debugging tasks.</p> <p>Below is an example of how to configure and use custom names for mapped tasks</p> example_map_index_template.yml<pre><code># Requires Airflow 2.9 or higher\nexample_map_index_template:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"Example of TaskFlow powered DAG that includes dynamic task mapping\"\n  schedule_interval: \"0 3 * * *\"\n  default_view: \"graph\"\n  tasks:\n    dynamic_task_with_named_mapping:\n      decorator: airflow.decorators.task\n      python_callable: sample.extract_last_name\n      map_index_template: \"{{ custom_mapping_key }}\"\n      expand:\n        full_name:\n          - Lucy Black\n          - Vera Santos\n          - Marks Spencer\n</code></pre>"},{"location":"features/dynamic_tasks/#how-it-works_1","title":"How it works","text":"<ol> <li>map_index_template:    Customizes the naming of dynamically mapped tasks using a Jinja2 expression. In this example, it uses <code>custom_mapping_key</code> from the task context to define task names.</li> <li>expand:    Dynamically generates tasks for each entry in the <code>full_name</code> list<ul> <li>Lucy Black</li> <li>Vera Santos</li> <li>Marks Spencer</li> </ul> </li> <li>Dynamic Task Naming:    The <code>custom_mapping_key</code> is set to the first name of each person, e.g., Lucy, Vera, and Marks using the callable function <code>extract_last_name</code>. This callable function is kept in sample.py</li> </ol>"},{"location":"features/dynamic_tasks/#airflow-named-mapped-tasks-view","title":"Airflow named mapped tasks view","text":"<p>The image below shows that the <code>map_index</code> gets the first name of the person in the mapped tasks with the above configuration.</p> <p></p>"},{"location":"features/dynamic_tasks/#scope-and-limitations","title":"Scope and limitations","text":"<p>The Airflow documentation on dynamic task mapping provides various examples of this feature. While the previous sections have discussed the forms supported by DAG Factory, it\u2019s important to note the scenarios that have not been tested or are known to be unsupported.</p> <p>The following cases are tested and expected to work (you can refer to previous sections on how to use them with DAG Factory):</p> <ul> <li>Simple mapping</li> <li>Task-generated mapping</li> <li>Repeated mapping</li> <li>Adding parameters that do not expand (partial)</li> <li>Mapping over multiple parameters</li> <li>Named mapping (map_index_template)</li> </ul> <p>The following cases are untested but are expected to work:</p> <ul> <li>Mapping with non-TaskFlow operators</li> <li>Mapping over the result of classic operators</li> <li>Filtering items from a mapped task</li> </ul> <p>The following cases are untested and may not work:</p> <ul> <li>Assigning multiple parameters to a non-TaskFlow operator</li> <li>Mapping over a task group</li> <li>Transforming expanding data</li> <li>Combining upstream data (aka \u201czipping\u201d)</li> </ul>"},{"location":"features/http_task/","title":"HttpSensor","text":"<p>DAG-Factory supports the HttpSensor from the <code>airflow.providers.http.sensors.http</code> package.</p> <p>The example below demonstrates the response_check logic in a Python file:</p> <pre><code>task_2:\n  operator: airflow.providers.http.sensors.http.HttpSensor\n  http_conn_id: 'test-http'\n  method: 'GET'\n  response_check_name: check_sensor\n  response_check_file: /path/to/example1/http_conn.py\n  dependencies: [task_1]\n</code></pre> <p>The <code>response_check</code> logic can also be provided as a lambda:</p> <pre><code>task_2:\n  operator: airflow.providers.http.sensors.http.HttpSensor\n  http_conn_id: 'test-http'\n  method: 'GET'\n  response_check_lambda: 'lambda response: \"ok\" in response.text'\n  dependencies: [task_1]\n</code></pre>"},{"location":"features/multiple_configuration_files/","title":"Multiple Configuration Files","text":"<p>Using DAG-Factory if you want to split your DAG configuration into multiple files, you can do so by leveraging a suffix in the configuration file name.</p> <pre><code>    from dagfactory import load_yaml_dags  # load relevant YAML files as airflow DAGs\n\n    load_yaml_dags(globals_dict=globals(), suffix=['dag.yaml'])\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/","title":"DAG Factory: Quick Start Guide With Airflow","text":"<p>DAG Factory is a Python library Apache Airflow\u00ae that simplifies DAG creation using declarative YAML configuration files instead of Python.</p>"},{"location":"getting-started/quick-start-airflow-standalone/#prerequisites","title":"Prerequisites","text":"<p>The minimum requirements for dag-factory are:</p> <ul> <li>Python 3.8.0+</li> <li>Apache Airflow\u00ae 2.3+</li> </ul>"},{"location":"getting-started/quick-start-airflow-standalone/#step-1-create-a-python-virtual-environment","title":"Step 1: Create a Python Virtual Environment","text":"<p>Create and activate a virtual environment:</p> <pre><code>python3 -m venv dagfactory_env\nsource dagfactory_env/bin/activate\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-2-install-apache-airflow","title":"Step 2: Install Apache Airflow","text":"<p>Install Apache Airflow\u00ae:</p> <ol> <li> <p>Create a directory for your project and navigate to it:</p> <pre><code>mkdir dag-factory-quick-start &amp;&amp; cd dag-factory-quick-start\n</code></pre> </li> <li> <p>Set the <code>AIRFLOW_HOME</code> environment variable:</p> <pre><code>export AIRFLOW_HOME=$(pwd)\nexport AIRFLOW__CORE__LOAD_EXAMPLES=False\n</code></pre> </li> <li> <p>Install Apache Airflow:</p> <pre><code>pip install apache-airflow\n</code></pre> </li> </ol>"},{"location":"getting-started/quick-start-airflow-standalone/#step-3-install-dag-factory","title":"Step 3: Install DAG Factory","text":"<p>Install the DAG Factory library in your virtual environment:</p> <pre><code>pip install dag-factory\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-4-set-up-the-dags-folder","title":"Step 4: Set Up the DAGS Folder","text":"<p>Create a DAGs folder inside the $AIRFLOW_HOME directory, which is where your DAGs will be stored:</p> <pre><code>mkdir dags\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-5-define-a-dag-in-yaml","title":"Step 5: Define a DAG in YAML","text":"<p>DAG Factory uses YAML files to define DAG configurations. Create a file named <code>example_dag_factory.yml</code> in the <code>$AIRFLOW_HOME/dags</code> folder with the following content:</p> example_dag_factory.yml<pre><code>default:\n  default_args:\n    catchup: false,\n    start_date: 2024-11-11\n\nbasic_example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule_interval: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-6-generate-the-dag-from-yaml","title":"Step 6: Generate the DAG from YAML","text":"<p>Create a Python script named <code>example_dag_factory.py</code> in the <code>$AIRFLOW_HOME/dags</code> folder. This script will generate the DAG from the YAML configuration</p> example_dag_factory.py<pre><code>import os\nfrom pathlib import Path\n\n# The following import is here so Airflow parses this file\n# from airflow import DAG\nimport dagfactory\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\n\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\n\nconfig_file = str(CONFIG_ROOT_DIR / \"example_dag_factory.yml\")\n\nexample_dag_factory = dagfactory.DagFactory(config_file)\n\n# Creating task dependencies\nexample_dag_factory.clean_dags(globals())\nexample_dag_factory.generate_dags(globals())\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-7-start-airflow","title":"Step 7: Start Airflow","text":"<p>To start the Airflow environment with your DAG Factory setup, run the following command:</p> <pre><code>airflow standalone\n</code></pre> <p>This will take a few minutes to set up. Once completed, you can access the Airflow UI and the generated DAG at <code>http://localhost:8080</code> \ud83d\ude80.</p>"},{"location":"getting-started/quick-start-airflow-standalone/#view-your-generated-dag","title":"View Your Generated DAG","text":"<p>Once Airflow is up and running, you can login with the username <code>admin</code> and the password in <code>$AIRFLOW_HOME/standalone_admin_password.txt</code>. You should be able to see your generated DAG in the Airflow UI.</p>"},{"location":"getting-started/quick-start-airflow-standalone/#generated-dag","title":"Generated DAG","text":""},{"location":"getting-started/quick-start-airflow-standalone/#graph-view","title":"Graph View","text":"<p>Checkout examples for generating more advanced DAGs.</p>"},{"location":"getting-started/quick-start-astro-cli/","title":"DAG Factory: Quick Start Guide With Astro CLI","text":"<p>DAG Factory is a Python library Apache Airflow\u00ae that simplifies DAG creation using declarative YAML configuration files instead of Python.</p>"},{"location":"getting-started/quick-start-astro-cli/#prerequisites","title":"Prerequisites","text":"<p>The minimum requirements for dag-factory are:</p> <ul> <li>Python 3.8.0+</li> <li>Astro CLI</li> </ul>"},{"location":"getting-started/quick-start-astro-cli/#step-1-initialize-airflow-project","title":"Step 1: Initialize Airflow Project","text":"<p>Create a new directory and initialize your Astro CLI project:</p> <pre><code>mkdir dag-factory-quick-start &amp;&amp; cd dag-factory-quick-start\n\nastro dev init\n</code></pre> <p>This will set up the necessary Airflow files and directories.</p>"},{"location":"getting-started/quick-start-astro-cli/#step-2-install-dag-factory","title":"Step 2: Install DAG Factory","text":"<p>Install DAG Factory in your Airflow environment:</p> <ol> <li>Add dag-factory as a dependency to the <code>requirements.txt</code> file created during the project initialization.</li> </ol>"},{"location":"getting-started/quick-start-astro-cli/#step-3-define-a-dag-in-yaml","title":"Step 3: Define a DAG in YAML","text":"<p>DAG Factory uses YAML files to define DAG configurations. Create a file named <code>example_dag_factory.yml</code> in the <code>$AIRFLOW_HOME/dags</code> folder with the following content:</p> example_dag_factory.yml<pre><code>default:\n  default_args:\n    catchup: false,\n    start_date: 2024-11-11\n\nbasic_example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule_interval: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n</code></pre>"},{"location":"getting-started/quick-start-astro-cli/#step-4-generate-the-dag-from-yaml","title":"Step 4: Generate the DAG from YAML","text":"<p>Create a Python script named <code>example_dag_factory.py</code> in the <code>$AIRFLOW_HOME/dags</code> folder. This script will generate the DAG from the YAML configuration</p> example_dag_factory.py<pre><code>import os\nfrom pathlib import Path\n\n# The following import is here so Airflow parses this file\n# from airflow import DAG\nimport dagfactory\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\n\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\n\nconfig_file = str(CONFIG_ROOT_DIR / \"example_dag_factory.yml\")\n\nexample_dag_factory = dagfactory.DagFactory(config_file)\n\n# Creating task dependencies\nexample_dag_factory.clean_dags(globals())\nexample_dag_factory.generate_dags(globals())\n</code></pre>"},{"location":"getting-started/quick-start-astro-cli/#step-5-start-airflow-project","title":"Step 5: Start Airflow Project","text":"<p>Once you've set up your YAML configuration and Python script, start the Airflow environment with the following command:</p> <pre><code>astro dev start\n</code></pre> <p>This will take a few minutes to set up. Once completed, you can access the Airflow UI and the generated DAG at <code>http://localhost:8080</code> \ud83d\ude80.</p>"},{"location":"getting-started/quick-start-astro-cli/#view-your-generated-dag","title":"View Your Generated DAG","text":"<p>Once Airflow is up and running, you can login with the username <code>admin</code> and the password <code>admin</code>. You should be able to see your generated DAG in the Airflow UI.</p>"},{"location":"getting-started/quick-start-astro-cli/#generated-dag","title":"Generated DAG","text":""},{"location":"getting-started/quick-start-astro-cli/#graph-view","title":"Graph View","text":"<p>Checkout examples for generating more advanced DAGs.</p>"}]}