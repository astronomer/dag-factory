{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DAG Factory documentation","text":"<p>Everything you need to know about how to build Apache Airflow\u00ae workflows using YAML files.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>Are you new to DAG Factory? This is the place to start!</p> <ul> <li>DAG Factory at a glance<ul> <li>Quickstart with Airflow standalone</li> <li>Quickstart with Astro CLI</li> </ul> </li> <li>Install guide</li> <li>Using YAML instead of Python<ul> <li>Traditional Airflow Operators</li> <li>TaskFlow API</li> </ul> </li> </ul>"},{"location":"#configuration","title":"Configuration","text":"<ul> <li>Configuring your workflows<ul> <li>Environment variables</li> <li>Defaults</li> </ul> </li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Dynamic tasks</li> </ul>"},{"location":"#getting-help","title":"Getting help","text":"<p>Having trouble? We'd like to help!</p> <ul> <li>Report bugs, questions and feature requests in our ticket tracker.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>DAG Factory is an Open-Source project. Learn about its development process and about how you can contribute:</p> <ul> <li>Contributing to DAG Factory</li> <li>Github repository</li> </ul>"},{"location":"#license","title":"License","text":"<p>To learn more about the terms and conditions for use, reproduction and distribution, read the Apache License 2.0.</p>"},{"location":"#privacy-notice","title":"Privacy Notice","text":"<p>This project follows Astronomer's Privacy Policy.</p> <p>For further information, read this</p>"},{"location":"#security-policy","title":"Security Policy","text":"<p>Check the project's Security Policy to learn how to report security vulnerabilities in DAG Factory and how security issues reported to the DAG Factory security team are handled.</p>"},{"location":"comparison/","title":"Using YAML instead of Python","text":"<p>By default, Apache Airflow\u00ae users write their workflows, or sequences of tasks, in Python.</p> <p>DAG Factory offers an alternative interface, allowing users to represent Airflow workflows via YAML files, often using less code.</p> <p>This section illustrates a few examples of how to represent the same workflow using plain Airflow Python DAGs in comparison to their representation using DAG Factory YAML files.</p> <ul> <li>Traditional Airflow Operators</li> <li>TaskFlow API</li> </ul>"},{"location":"comparison/taskflow_api/","title":"TaskFlow API: Using YAML instead of Python","text":"<p>For users that employ lots of Python functions in their DAGs, TaskFlow API represent a simpler way to transform functions into tasks, with a more intuitive way of passing data between them. They were  introduced in Airflow 2 as an alternative to Airflow traditional operators.</p> <p>The following section shows how to represent an Airflow DAG using TaskFlow API and how to define the same DAG using DAG Factory. Ultimately, both implementations use the same Airflow operators. The main difference is the language used to declare the workflow: one uses Python and the other uses YAML.</p>"},{"location":"comparison/taskflow_api/#goal","title":"Goal","text":"<p>Let's say we'd like to create a workflow that performs the following:</p> <ol> <li>Create a list of PyPI projects to be analysed.</li> <li>Fetch the statistics for each of these projects.</li> <li>Summarize the selected statistics as Markdown, using Python.</li> </ol> <p>We will implement all these steps using the Airflow <code>task</code> decorator, and the last task will generate a Markdown table similar to:</p> <pre><code>| package_name      |   last_day |   last_month |   last_week |\n|:------------------|-----------:|-------------:|------------:|\n| apache-airflow    |     852242 |     28194255 |     6253861 |\n| astronomer-cosmos |     442531 |     13354870 |     3127750 |\n| dag-factory       |      10078 |       354085 |       77752 |\n</code></pre> <p>The main logic is implemented as plain Python functions in pypi_stats.py:</p> pypi_stats.py<pre><code>def get_pypi_projects_list(**kwargs: dict[str, Any]) -&gt; list[str]:\n    \"\"\"\n    Return a list of PyPI project names to be analysed.\n    \"\"\"\n    projects_from_ui = kwargs.get(\"dag_run\").conf.get(\"pypi_projects\") if kwargs.get(\"dag_run\") else None\n    if projects_from_ui is None:\n        pypi_projects = DEFAULT_PYPI_PROJECTS\n    else:\n        pypi_projects = projects_from_ui\n    return pypi_projects\n\n\ndef fetch_pypi_stats_data(package_name: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Given a PyPI project name, return the PyPI stats data associated to it.\n    \"\"\"\n    url = f\"https://pypistats.org/api/packages/{package_name}/recent\"\n    package_json = httpx.get(url).json()\n    package_data = package_json[\"data\"]\n    package_data[\"package_name\"] = package_name\n    return package_data\n\n\ndef summarize(values: list[dict[str, Any]]):\n    \"\"\"\n    Given a list with PyPI stats data, create a table summarizing it, sorting by the last day total downloads.\n    \"\"\"\n    df = pd.DataFrame(values)\n    first_column = \"package_name\"\n    sorted_columns = [first_column] + [col for col in df.columns if col != first_column]\n    df = df[sorted_columns].sort_values(by=\"last_day\", ascending=False)\n    markdown_output = df.to_markdown(index=False)\n    print(markdown_output)\n    return markdown_output\n</code></pre>"},{"location":"comparison/taskflow_api/#implementation","title":"Implementation","text":"<p>As a reference, the following workflows run using Airflow 2.10.2 and DAG Factory 0.21.0.</p>"},{"location":"comparison/taskflow_api/#plain-airflow-python-dag","title":"Plain Airflow Python DAG","text":"example_pypi_stats_plain_airflow.py<pre><code>from __future__ import annotations\n\nfrom datetime import datetime\nfrom typing import Any\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom pypi_stats import fetch_pypi_stats_data, get_pypi_projects_list, summarize\n\nwith DAG(dag_id=\"example_pypi_stats_plain_airflow\", schedule=None, start_date=datetime(2022, 3, 4)) as dag:\n\n    @task\n    def get_pypi_projects_list_():\n        return get_pypi_projects_list()\n\n    @task\n    def fetch_pypi_stats_data_(project_name: str):\n        return fetch_pypi_stats_data(project_name)\n\n    @task\n    def summarize_(values: list[dict[str, Any]]):\n        return summarize(values)\n\n    pypi_stats_data = fetch_pypi_stats_data_.expand(project_name=get_pypi_projects_list_())\n    summarize_(pypi_stats_data)\n</code></pre>"},{"location":"comparison/taskflow_api/#alternative-dag-factory-yaml","title":"Alternative DAG Factory YAML","text":"example_pypi_stats_dagfactory.yml<pre><code>example_pypi_stats_dagfactory:\n  default_args:\n    start_date: 2022-03-04\n  tasks:\n    get_pypi_projects_list:\n      decorator: airflow.decorators.task\n      python_callable: pypi_stats.get_pypi_projects_list\n    fetch_pypi_stats_data:\n      decorator: airflow.decorators.task\n      python_callable: pypi_stats.fetch_pypi_stats_data\n      expand:\n        package_name: +get_pypi_projects_list\n    summarize:\n      decorator: airflow.decorators.task\n      python_callable: pypi_stats.summarize\n      values: +fetch_pypi_stats_data\n</code></pre>"},{"location":"comparison/taskflow_api/#comparison","title":"Comparison","text":""},{"location":"comparison/taskflow_api/#goal_1","title":"Goal","text":"<p>Both implementations accomplish the same goal and result in the expected Markdown table.</p>"},{"location":"comparison/taskflow_api/#airflow-graph-view","title":"Airflow Graph view","text":"<p>As shown in the screenshots below, both the DAG created using Python with standard Airflow and the DAG created using YAML and DAG Factory look identical, from a graph topology perspective, and also from the underlining operators being used.</p>"},{"location":"comparison/taskflow_api/#graph-view-plain-airflow-python-dag","title":"Graph view: Plain Airflow Python DAG","text":""},{"location":"comparison/taskflow_api/#graph-view-alternative-dag-factory-yaml","title":"Graph view: Alternative DAG Factory YAML","text":""},{"location":"comparison/taskflow_api/#airflow-dynamic-task-mapping","title":"Airflow Dynamic Task Mapping","text":"<p>In both workflows, we are dynamically generating a task for each PyPI repo.</p>"},{"location":"comparison/taskflow_api/#mapped-tasks-plain-airflow-python-dag","title":"Mapped Tasks: Plain Airflow Python DAG","text":""},{"location":"comparison/taskflow_api/#mapped-tasks-alternative-dag-factory-yaml","title":"Mapped Tasks: Alternative DAG Factory YAML","text":""},{"location":"comparison/taskflow_api/#airflow-code-view","title":"Airflow Code view","text":"<p>From an Airflow UI perspective, the content displayed in the \"Code\" view is the main difference between the two implementations. While Airflow renders the original Python DAG, as expected, in the case of the YAML DAGs, Airflow displays the Python file that references the DAG Factory YAML files:</p> example_load_yaml_dags.py<pre><code>import os\nfrom pathlib import Path\n\nfrom dagfactory import load_yaml_dags\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\nconfig_dir = str(CONFIG_ROOT_DIR / \"comparison\")\n\nload_yaml_dags(\n    globals_dict=globals(),\n    dags_folder=config_dir,\n)\n</code></pre>"},{"location":"comparison/taskflow_api/#code-view-plain-airflow-python-dag","title":"Code view: Plain Airflow Python DAG","text":""},{"location":"comparison/taskflow_api/#code-view-alternative-dag-factory-yaml","title":"Code view: Alternative DAG Factory YAML","text":"<p>To overcome this limitation, DAG Factory appends the YAML content to the DAG Documentation so users can better troubleshoot the DAG:</p> <p></p>"},{"location":"comparison/traditional_operators/","title":"Traditional Operators: Using YAML instead of Python","text":"<p>Traditionally, operators are Airflow's building blocks, and while they are robust and diverse, they can sometimes lead to boilerplate-heavy DAGs compared to the newer TaskFlow API.</p> <p>Most of the Airflow providers come with built-in traditional operators. Some examples include <code>BashOperator</code>, <code>PythonOperator</code>, <code>KubernetesPodOperator</code>, and  <code>PostgresOperator</code>.</p> <p>Below, we illustrate how to represent an Airflow DAG using traditional operators and how to define the same DAG using DAG Factory. Ultimately, both implementations use the same Airflow operators. The main difference is the language used to declare the workflow: one uses Python and the other uses YAML.</p>"},{"location":"comparison/traditional_operators/#goal","title":"Goal","text":"<p>Let's say we'd like to create a workflow that performs the following:</p> <ol> <li>Retrieve the top ten stories from Hacker News using the Hacker News API.</li> <li>Fetch the details for the two top stories using the Hacker News API.</li> <li>Summarize the selected stories as Markdown, using Python.</li> </ol> <p>We will implement the first two steps using <code>BashOperator</code> and the third step using <code>PythonOperator</code>. The last task will generate a <code>Markdown</code> snippet similar to:</p> <pre><code>| title                                                                       | url                                                                                                                    |\n|:----------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|\n| I keep turning my Google Sheets into phone-friendly webapps                 | https://arstechnica.com/gadgets/2024/12/making-tiny-no-code-webapps-out-of-spreadsheets-is-a-weirdly-fulfilling-hobby/ |\n| Coconut by Meta AI \u2013 Better LLM Reasoning with Chain of Continuous Thought? | https://aipapersacademy.com/chain-of-continuous-thought/                                                               |\n</code></pre> <p>The main logic is implemented as plain Python functions in hacker_news.py:</p> pypi_stats.py<pre><code>def summarize(**kwargs):\n    \"\"\"\n    Given the Airflow context is provided to this function, it will extract the XCom hackernews records from its\n    upstream tasks and summarise in Markdown.\n    \"\"\"\n    ti = kwargs[\"ti\"]\n    upstream_task_ids = ti.task.upstream_task_ids  # Get upstream task IDs dynamically\n    values = [json.loads(ti.xcom_pull(task_ids=task_id)) for task_id in upstream_task_ids]\n\n    df = pd.DataFrame(values)\n    selected_columns = [\"title\", \"url\"]\n    df = df[selected_columns]\n    markdown_output = df.to_markdown(index=False)\n    print(markdown_output)\n    return markdown_output\n</code></pre>"},{"location":"comparison/traditional_operators/#implementation","title":"Implementation","text":"<p>As a reference, the following workflows run using Airflow 2.10.2 and DAG Factory 0.21.0.</p>"},{"location":"comparison/traditional_operators/#plain-airflow-python-dag","title":"Plain Airflow Python DAG","text":"example_hackernews_plain_airflow.py<pre><code>from datetime import datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python import PythonOperator\nfrom hacker_news import summarize\n\nwith DAG(dag_id=\"example_hackernews_plain_airflow\", schedule=None, start_date=datetime(2022, 3, 4)) as dag:\n\n    fetch_top_ten_news = BashOperator(\n        task_id=\"fetch_top_ten_news\",\n        bash_command=\"curl -s https://hacker-news.firebaseio.com/v0/topstories.json  | jq -c -r '.[0:10]'\",\n    )\n\n    fetch_first_top_news = BashOperator(\n        task_id=\"fetch_first_top_news\",\n        bash_command=\"\"\"\n            echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[0]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\n        \"\"\",\n    )\n\n    fetch_second_top_news = BashOperator(\n        task_id=\"fetch_second_news\",\n        bash_command=\"\"\"\n            echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[1]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\n        \"\"\",\n    )\n\n    summarize = PythonOperator(task_id=\"summarize\", python_callable=summarize)\n\n    fetch_top_ten_news &gt;&gt; [fetch_first_top_news, fetch_second_top_news] &gt;&gt; summarize\n</code></pre>"},{"location":"comparison/traditional_operators/#alternative-dag-factory-yaml","title":"Alternative DAG Factory YAML","text":"example_hackernews_dagfactory.py<pre><code>example_hackernews_dagfactory:\n  default_args:\n    start_date: 2022-03-04\n  tasks:\n    fetch_top_ten_news:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"curl -s https://hacker-news.firebaseio.com/v0/topstories.json  | jq -c -r '.[0:10]'\"\n    fetch_first_top_news:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[0]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\"\n      dependencies: [fetch_top_ten_news]\n    fetch_second_top_news:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[1]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\"\n      dependencies: [fetch_top_ten_news]\n    summarize:\n      operator: airflow.operators.python.PythonOperator\n      python_callable: hacker_news.summarize\n      dependencies: [fetch_first_top_news, fetch_second_top_news]\n</code></pre>"},{"location":"comparison/traditional_operators/#comparison","title":"Comparison","text":""},{"location":"comparison/traditional_operators/#goal_1","title":"Goal","text":"<p>Both implementations accomplish the same goal and result in the expected Markdown table.</p>"},{"location":"comparison/traditional_operators/#airflow-graph-view","title":"Airflow Graph view","text":"<p>As shown in the screenshots below, both the DAG created using Python with standard Airflow and the DAG created using YAML and DAG Factory look identical, from a graph topology perspective, and also from the underlining operators being used.</p>"},{"location":"comparison/traditional_operators/#graph-view-plain-airflow-python-dag","title":"Graph view: Plain Airflow Python DAG","text":""},{"location":"comparison/traditional_operators/#graph-view-alternative-dag-factory-yaml","title":"Graph view: Alternative DAG Factory YAML","text":""},{"location":"comparison/traditional_operators/#airflow-code-view","title":"Airflow Code view","text":"<p>From an Airflow UI perspective, the content displayed in the \"Code\" view is the main difference between the two implementations. While Airflow renders the original Python DAG, as expected, in the case of the YAML DAGs, Airflow displays the Python file that references the DAG Factory YAML files:</p> example_load_yaml_dags.py<pre><code>import os\nfrom pathlib import Path\n\nfrom dagfactory import load_yaml_dags\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\nconfig_dir = str(CONFIG_ROOT_DIR / \"comparison\")\n\nload_yaml_dags(\n    globals_dict=globals(),\n    dags_folder=config_dir,\n)\n</code></pre>"},{"location":"comparison/traditional_operators/#code-view-plain-airflow-python-dag","title":"Code view: Plain Airflow Python DAG","text":""},{"location":"comparison/traditional_operators/#code-view-alternative-dag-factory-yaml","title":"Code view: Alternative DAG Factory YAML","text":"<p>To overcome this limitation, DAG Factory appends the YAML content to the DAG Documentation so users can better troubleshoot the DAG:</p> <p></p>"},{"location":"configuration/configuring_workflows/","title":"Configuring Your Workflows","text":"<p>DAG Factory allows you to define workflows in a structured, configuration-driven way using YAML files. You can define multiple workflows within a single YAML file based on your requirements.</p>"},{"location":"configuration/configuring_workflows/#key-elements-of-workflow-configuration","title":"Key Elements of Workflow Configuration","text":"<ul> <li>dag_id: Unique identifier for your DAG.</li> <li>default_args: Common arguments for all tasks.</li> <li>schedule/schedule_interval: Specifies the execution schedule.</li> <li>tasks: Defines the Airflow tasks in your workflow.</li> </ul>"},{"location":"configuration/configuring_workflows/#example-dag-configuration","title":"Example DAG Configuration","text":"example_dag_factory.yml<pre><code>basic_example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule_interval: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n</code></pre>"},{"location":"configuration/configuring_workflows/#check-out-more-configuration-params","title":"Check out more configuration params","text":"<ul> <li>Environment variables</li> <li>Defaults</li> </ul>"},{"location":"configuration/defaults/","title":"Defaults","text":"<p>DAG Factory allows you to define Airflow default_args and additional DAG-level arguments in a <code>default</code> block. This block enables you to share common settings across all DAGs in your YAML configuration, with the arguments automatically applied to each DAG defined in the file.</p>"},{"location":"configuration/defaults/#benefits-of-using-the-default-block","title":"Benefits of using the default block","text":"<ul> <li>Consistency: Ensures uniform configurations across all tasks and DAGs.</li> <li>Maintainability: Reduces duplication by centralizing common properties.</li> <li>Simplicity: Makes configurations easier to read and manage.</li> </ul>"},{"location":"configuration/defaults/#example-usage-of-default-block","title":"Example usage of default block","text":"Usage of default block in YAML<pre><code>default:\n  default_args:\n    owner: default_owner\n    retries: 1\n    retry_delay_sec: 300\n    start_date: 2024-01-01\n  default_view: tree\n  max_active_runs: 1\n  schedule_interval: 0 1 * * *\nexample_task_group:\n  description: \"this dag uses task groups\"\n  task_groups:\n    task_group_1:\n      tooltip: \"this is a task group\"\n      dependencies: [task_1]\n    task_group_2:\n      tooltip: \"this is a task group\"\n      parent_group_name: task_group_1\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      task_group_name: task_group_1\n    task_4:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 4\"\n      task_group_name: task_group_2\n</code></pre> <p>The arguments specified in the <code>default</code> block, such as <code>default_args</code>, <code>default_view</code>, <code>max_active_runs</code>, <code>schedule_interval</code>, and any others defined, will be applied to all the DAGs in the YAML configuration.</p>"},{"location":"configuration/defaults/#multiple-ways-for-specifying-airflow-default_args","title":"Multiple ways for specifying Airflow default_args","text":"<p>DAG Factory offers flexibility in defining Airflow\u2019s <code>default_args</code>. These can be specified in several ways, depending on your requirements.</p> <ol> <li> <p>Specifying <code>default_args</code> in the <code>default</code> block</p> <p>As seen in the previous example, you can define shared <code>default_args</code> for all DAGs in the configuration YAML under the <code>default</code> block. These arguments are automatically inherited by every DAG defined in the file.</p> </li> <li> <p>Specifying <code>default_args</code> directly in a DAG configuration</p> <p>You can override or define specific default_args at the individual DAG level. This allows you to customize arguments for each DAG without affecting others.</p> <p>Example:</p> DAG level default_args<pre><code>default:\n  default_args:\n    catchup: false,\n    start_date: 2024-11-11\n\nbasic_example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule_interval: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n</code></pre> </li> <li> <p>Specifying <code>default_args</code> in a shared <code>defaults.yml</code></p> <p>Starting DAG Factory 0.22.0, you can also keep the <code>default_args</code> in the <code>defaults.yml</code> file. The configuration from <code>defaults.yml</code> will be applied to all DAG Factory generated DAGs.</p> defaults.yml<pre><code>default_args:\n  start_date: \"2025-01-01\"\n  owner: \"global_owner\"\n</code></pre> </li> </ol> <p>Given the various ways to specify <code>default_args</code>, the following precedence order is applied when arguments are duplicated:</p> <ol> <li>In the DAG configuration</li> <li>In the <code>default</code> block within the workflow's YAML file</li> <li>In the <code>defaults.yml</code></li> </ol>"},{"location":"configuration/environment_variables/","title":"Environment variables","text":"<p>Starting release <code>0.20.0</code>, DAG Factory introduces support for referencing environment variables directly within YAML configuration files. This enhancement enables dynamic configuration paths and enhances workflow portability by resolving environment variables during DAG parsing.</p> <p>With this feature, DAG Factory removes the reliance on hard-coded paths, allowing for more flexible and adaptable configurations that work seamlessly across various environments.</p>"},{"location":"configuration/environment_variables/#example-yaml-configuration-with-environment-variables","title":"Example YAML Configuration with Environment Variables","text":"Reference environment variable in YAML<pre><code>example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule_interval: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  dag_display_name: \"Pretty Example DAG\"\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.python_operator.PythonOperator\n      python_callable_name: print_hello\n      python_callable_file: $CONFIG_ROOT_DIR/print_hello.py\n      dependencies: [task_1]\n</code></pre> <p>In the above example, <code>$CONFIG_ROOT_DIR</code> is used to reference an environment variable that points to the root directory of your DAG configurations. During DAG parsing, it will be resolved to the value specified for the <code>CONFIG_ROOT_DIR</code> environment variable.</p>"},{"location":"contributing/code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"contributing/code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socioeconomic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"contributing/code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"contributing/code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"contributing/code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contributing/code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at humans@astronomer.io.</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"contributing/code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"contributing/code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"contributing/code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"contributing/code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"contributing/code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"contributing/code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ. Translations are available at this page.</p>"},{"location":"contributing/contributors/","title":"Contributors","text":"<p>There are different ways people can contribute to DAG Factory. Learn more about the project contributors roles.</p>"},{"location":"contributing/contributors/#committers","title":"Committers","text":"<ul> <li>Pankaj Koti (@pankajkoti)</li> <li>Pankaj Singh (@pankajastro)</li> <li>Tatiana Al-Chueyr (@tatiana)</li> </ul>"},{"location":"contributing/contributors/#emeritus-committers","title":"Emeritus Committers","text":"<ul> <li>Adam Boscarino (@ajbosco)</li> </ul>"},{"location":"contributing/contributors/#contributors_1","title":"Contributors","text":"<p>Many people are improving DAG Factory each day. Find more contributors in our GitHub page.</p>"},{"location":"contributing/howto/","title":"Contributing Guide","text":"<p>All contributions, bug reports, bug fixes, documentation improvements, and enhancements are welcome.</p> <p>All contributors and maintainers to this project should abide by the Contributor Code of Conduct.</p> <p>Learn more about the contributors' roles in the Roles page.</p> <p>This document describes how to contribute to DAG Factory, covering:</p> <ul> <li>Overview of how to contribute</li> <li>How to set up the local development environment</li> <li>Running tests</li> <li>Pre-commit and linting</li> <li>Authoring the documentation</li> <li>Releasing</li> </ul>"},{"location":"contributing/howto/#overview-of-how-to-contribute","title":"Overview of how to contribute","text":"<p>To contribute to the DAG Factory project:</p> <ol> <li>Please create a GitHub Issue describing a bug, enhancement, or feature request.</li> <li>Open a branch off of the <code>main</code> branch and create a Pull Request into the <code>main</code> branch from your feature branch.</li> <li>Link your issue to the pull request.</li> <li>After you complete development on your feature branch, request a review. A maintainer will merge your PR after all reviewers approve it.</li> </ol>"},{"location":"contributing/howto/#set-up-a-local-development-environment","title":"Set up a local development environment","text":""},{"location":"contributing/howto/#requirements","title":"Requirements","text":"<ul> <li>Git</li> <li>Python &lt;= 3.12 (due to dependencies, such as <code>google-re2</code> not supporting Python 3.13 yet)</li> <li>Hatch</li> </ul> <p>Clone the DAG Factory repository and change the current working directory to the repo's root directory:</p> <pre><code>git clone https://github.com/astronomer/dag-factory.git\ncd dag-factory/\n</code></pre> <p>After cloning the project, there are two options for setting up the local development environment:</p> <ul> <li>Use a Python virtual environment, or</li> <li>Use Docker</li> </ul>"},{"location":"contributing/howto/#using-a-python-virtual-environment-for-local-development","title":"Using a Python virtual environment for local development","text":"<ol> <li> <p>Install the project dependencies:</p> <pre><code>make setup\n</code></pre> </li> <li> <p>Activate the local python environment:</p> <pre><code>source venv/bin/activate\n</code></pre> </li> <li> <p>Set Apache Airflow\u00ae home to the <code>dev/</code>, so you can see DAG Factory example DAGs. Disable loading Airflow standard example DAGs:</p> </li> </ol> <pre><code>export AIRFLOW_HOME=$(pwd)/dev/\nexport AIRFLOW__CORE__LOAD_EXAMPLES=false\n</code></pre> <p>Then, run Airflow in standalone mode; the command below will create a new user (if it does not exist) and run the necessary Airflow component (webserver, scheduler and triggered):</p> <p>Note: By default, Airflow will use sqlite as a database; you can override this by setting the variable <code>AIRFLOW__DATABASE__SQL_ALCHEMY_CONN</code> to the SQL connection string.</p> <pre><code>airflow standalone\n</code></pre> <p>After Airflow is running, you can access the Airflow UI at <code>http://localhost:8080</code>.</p> <p>Note: whenever you want to start the development server, you need to activate the <code>virtualenv</code> and set the <code>environment variables</code></p>"},{"location":"contributing/howto/#use-docker-for-local-development","title":"Use Docker for local development","text":"<p>It is also possible to build the development environment using Docker:</p> <pre><code>make docker-run\n</code></pre> <p>After the sandbox is running, you can access the Airflow UI at <code>http://localhost:8080</code>.</p> <p>This approach builds a DAG Factory wheel, so if there are code changes, you must stop and restart the containers:</p> <pre><code>make docker-stop\n</code></pre>"},{"location":"contributing/howto/#testing-application-with-hatch","title":"Testing application with hatch","text":"<p>The tests are developed using PyTest and run using hatch.</p> <p>The pyproject. toml file currently defines a matrix of supported versions of Python and Airflow against which a user can run the tests.</p>"},{"location":"contributing/howto/#run-unit-tests","title":"Run unit tests","text":"<p>To run unit tests using Python 3.10 and Airflow 2.5, use the following:</p> <pre><code>hatch run tests.py3.10-2.5:test-cov\n</code></pre> <p>It is also possible to run the tests using all the matrix combinations, by using:</p> <pre><code>hatch run tests:test-cov\n</code></pre>"},{"location":"contributing/howto/#run-integration-tests","title":"Run integration tests","text":"<p>Note: these tests create local Python virtual environments in a hatch-managed directory. They also use the user-defined <code>AIRFLOW_HOME</code>, overriding any pre-existing <code>airflow.cfg</code> and <code>airflow.db</code> files.</p> <p>First, set the following environment variables:</p> <pre><code>export AIRFLOW_HOME=$(pwd)/dev/\nexport CONFIG_ROOT_DIR=`pwd`\"/dev/dags\"\nexport PYTHONPATH=dev/dags:$PYTHONPATH\n</code></pre> <p>To run the integration tests using Python 3.9 and Airflow 2.9, use</p> <pre><code>hatch run tests.py3.9-2.9:test-integration-setup\nhatch run tests.py3.9-2.9:test-integration\n</code></pre>"},{"location":"contributing/howto/#pre-commit-and-linting","title":"Pre-Commit and linting","text":"<p>We use pre-commit to run several checks on the code before committing. To install pre-commit hooks, run:</p> <pre><code>pre-commit install\n</code></pre> <p>To run the checks manually, run the following:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>Pre-commit runs several static checks, including Black and Ruff. It is also possible to run them using <code>hatch</code>:</p> <pre><code>hatch run tests.py3.9-2.9:static-check\n</code></pre>"},{"location":"contributing/howto/#write-docs","title":"Write docs","text":"<p>We use Markdown to author DAG Factory documentation.</p> <p>Similar to running tests, we also use hatch to manage the documentation.</p> <p>To build and serve the documentation locally:</p> <pre><code>hatch run docs:dev\n</code></pre> <p>To release the documentation with the current project version and set it to the latest:</p> <pre><code>hatch run docs:gh-release\n</code></pre>"},{"location":"contributing/howto/#releasing","title":"Releasing","text":"<p>We currently use hatch for building and distributing <code>dag-factory</code>.</p> <p>We use GitHub actions to create and deploy new releases. To create a new release, update the latest release version.</p> <p>It is possible to update the version either by using hatch:</p> <p>Note: You can update the version in several different ways. To learn more, check out the hatch docs.</p> <pre><code>hatch version minor\n</code></pre> <p>Or by manually updating the value of <code>__version__</code> in <code>dagfactory/__init__.py</code>.</p> <p>Make sure the CHANGELOG file is up-to-date.</p> <p>Create a release using the GitHub UI. GitHub will update the package directly to PyPI.</p> <p>If you're a project maintainer in PyPI, it is also possible to create a release manually, by authenticating to PyPI and running the commands:</p> <pre><code>hatch build\nhatch publish\n</code></pre>"},{"location":"contributing/roles/","title":"Contributor roles","text":"<p>Contributors are welcome and are greatly appreciated! Every little bit helps, and we give credit to them.</p> <p>This document aims to explain the current roles in the DAG Factory project. For more information, check the contributing docs.</p>"},{"location":"contributing/roles/#contributors","title":"Contributors","text":"<p>A contributor is anyone who wants to contribute code, documentation, tests, ideas, or anything to the DAG Factory project.</p> <p>DAG Factory contributors are listed in the Github insights page.</p> <p>Contributors are responsible for:</p> <ul> <li>Fixing bugs</li> <li>Refactoring code</li> <li>Improving processes and tooling</li> <li>Adding features</li> <li>Improving the documentation</li> </ul>"},{"location":"contributing/roles/#committers","title":"Committers","text":"<p>Committers are community members with write access to the DAG Factory GitHub repository. They can modify the code and the documentation and accept others' contributions to the repo.</p> <p>Check contributors for the official list of DAG Factory committers.</p> <p>Committers have the same responsibilities as standard contributors and also perform the following actions:</p> <ul> <li>Reviewing &amp; merging pull-requests</li> <li>Scanning and responding to GitHub issues, helping triaging them</li> </ul> <p>If you know you are not going to be able to contribute for a long time (for instance, due to a change of job or circumstances), you should inform other maintainers, and we will mark you as \"emeritus\". Emeritus committers will no longer have write access to the repo. As merit earned never expires, once an emeritus committer becomes active again, they can simply email another maintainer from Astronomer and ask to be reinstated.</p>"},{"location":"contributing/roles/#pre-requisites-to-becoming-a-committer","title":"Pre-requisites to becoming a committer","text":"<p>General prerequisites that we look for in all candidates:</p> <ol> <li>Consistent contribution over last few months</li> <li>Visibility on discussions on the Slack channel or GitHub issues/discussions</li> <li>Contributes to community health and project's sustainability for the long-term</li> <li>Understands the project's contributors' guidelines. Astronomer is responsible and accountable for releasing new versions of DAG Factory in PyPI, following the milestones. Astronomer has the right to grant and revoke write access permissions to the project's official repository for any reason it sees fit.</li> </ol>"},{"location":"features/dynamic_tasks/","title":"Dynamic tasks","text":"<p>DAG Factory supports Airflow\u2019s Dynamic Task Mapping, enabling workflows to dynamically create tasks at runtime.  This approach allows the number of tasks to be determined during execution, usually based on the outcome of a preceding task, rather than being predefined during DAG authoring.</p>"},{"location":"features/dynamic_tasks/#example-defining-dynamic-tasks","title":"Example: Defining Dynamic Tasks","text":"<p>Below is an example configuration for implementing dynamic tasks using DAG Factory:</p> example_dynamic_task_mapping.yml<pre><code>test_expand:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"test expand\"\n  schedule_interval: \"0 3 * * *\"\n  default_view: \"graph\"\n  tasks:\n    process:\n      operator: airflow.operators.python_operator.PythonOperator\n      python_callable_name: consume_value\n      python_callable_file: $CONFIG_ROOT_DIR/expand_tasks.py\n      partial:\n        op_kwargs:\n          fixed_param: \"test\"\n      expand:\n        op_args:\n            request.output\n      dependencies: [request]\n    # This task is intentionally placed after the \"process\" task to demonstrate that DAG Factory does not require tasks\n    # to be topologically ordered in the YAML file according to their dependencies.\n    request:\n      operator: airflow.operators.python.PythonOperator\n      python_callable_name: make_list\n      python_callable_file: $CONFIG_ROOT_DIR/expand_tasks.py\n</code></pre>"},{"location":"features/dynamic_tasks/#explanation-of-the-configuration","title":"Explanation of the Configuration","text":"<ol> <li> <p><code>request</code> Task:</p> <ul> <li>Generates a list of items using the <code>make_list</code> function from the expand_tasks.py module.</li> <li>This task serves as the input provider for the dynamically mapped tasks.</li> </ul> </li> <li> <p><code>process</code> Task:</p> <ul> <li>Dynamically generates one task for each item in the list produced by the <code>request</code> task.</li> <li>The expand argument is used to create these tasks at runtime, with <code>request.output</code> supplying the input list.</li> <li>Additionally, the <code>partial</code> argument is used to specify fixed parameters (<code>op_kwargs</code>) that are applied to all dynamically generated tasks.</li> </ul> </li> </ol>"},{"location":"features/dynamic_tasks/#how-it-works","title":"How It Works","text":"<ul> <li> <p>Dynamic Task Creation:     The <code>expand</code> keyword allows the process task to spawn multiple tasks at runtime, each processing a single item from the list output of the <code>request</code> task.</p> </li> <li> <p>Fixed Parameters:     The partial keyword ensures that common parameters, such as <code>fixed_param</code>, are passed to every dynamically created task instance.</p> </li> </ul>"},{"location":"features/dynamic_tasks/#benefits-of-dynamic-task-mapping-with-dag-factory","title":"Benefits of Dynamic Task Mapping with DAG Factory","text":"<ul> <li>Flexibility: Handle varying input sizes and conditions dynamically without modifying the DAG definition.</li> <li>Scalability: Efficiently process large datasets by leveraging Airflow\u2019s parallel execution capabilities.</li> <li>Simplicity: Define dynamic workflows declaratively using YAML, minimizing boilerplate code.</li> </ul>"},{"location":"features/dynamic_tasks/#airflow-mapped-tasks-view","title":"Airflow mapped tasks view","text":"<p>Below, you can see a list of mapped tasks generated dynamically as part of the <code>process</code> task.</p> <p></p>"},{"location":"features/dynamic_tasks/#advanced-dynamic-task-mapping-with-dag-factory","title":"Advanced Dynamic Task Mapping with DAG Factory","text":"<p>Below, we explain the different methods for defining dynamic task mapping, illustrated by the provided example configuration.</p> Dynamic Task Mapping advanced usage<pre><code>example_taskflow:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"Example of TaskFlow powered DAG that includes dynamic task mapping\"\n  schedule_interval: \"0 3 * * *\"\n  default_view: \"graph\"\n  tasks:\n    some_number:\n      decorator: airflow.decorators.task\n      python_callable: sample.some_number\n    numbers_list:\n      decorator: airflow.decorators.task\n      python_callable_name: build_numbers_list\n      python_callable_file: $CONFIG_ROOT_DIR/sample.py\n    another_numbers_list:\n      decorator: airflow.decorators.task\n      python_callable: sample.build_numbers_list\n    double_number_from_arg:\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      number: 2\n    double_number_from_task:\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      number: +some_number  # the prefix + leads to resolving this value as the task `some_number`, previously defined\n    double_number_with_dynamic_task_mapping_static:\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      expand:\n          number:\n            - 1\n            - 3\n            - 5\n    double_number_with_dynamic_task_mapping_taskflow:\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      expand:\n          number: +numbers_list  # the prefix + tells DagFactory to resolve this value as the task `numbers_list`, previously defined\n    multiply_with_multiple_parameters:\n      decorator: airflow.decorators.task\n      python_callable: sample.multiply\n      expand:\n          a: +numbers_list  # the prefix + tells DagFactory to resolve this value as the task `numbers_list`, previously defined\n          b: +another_numbers_list # the prefix + tells DagFactory to resolve this value as the task `another_numbers_list`, previously defined\n    double_number_with_dynamic_task_and_partial:\n      decorator: airflow.decorators.task\n      python_callable: sample.double_with_label\n      expand:\n          number: +numbers_list  # the prefix + tells DagFactory to resolve this value as the task `numbers_list`, previously defined\n      partial:\n          label: True\n</code></pre> <p>The example above illustrates advanced usage of Dynamic Task Mapping using Dag Factory (the callable functions used in the example are kept in sample.py):</p> <ol> <li> <p>Static Input Mapping</p> <p>The task <code>double_number_with_dynamic_task_mapping_static</code> shows how dynamic tasks can be created using static lists as input. Three tasks are created, each processing one number.</p> </li> <li> <p>Task-Generated Input Mapping</p> <p>The task <code>double_number_with_dynamic_task_mapping_taskflow</code> shows how tasks can use outputs from other tasks as input for dynamic task mapping. The prefix <code>+</code> tells DAG Factory to resolve this value as the task <code>numbers_list</code>, previously defined.</p> </li> <li> <p>Mapping with Multiple Inputs</p> <p>The task <code>multiply_with_multiple_parameters</code> shows how dynamic task mapping can combine outputs from multiple tasks as input parameters.</p> </li> </ol>"},{"location":"features/dynamic_tasks/#named-mapping-in-dynamic-tasks-with-dag-factory","title":"Named Mapping in Dynamic Tasks with DAG Factory","text":"<p>Starting with Airflow 2.9, the <code>map_index_template</code> feature allows for custom mapping name for dynamic tasks based on a user-defined key. DAG Factory fully supports this feature, enabling users to name tasks dynamically in a meaningful way during runtime. This can be useful for tracing and debugging tasks.</p> <p>Below is an example of how to configure and use custom names for mapped tasks</p> example_map_index_template.yml<pre><code># Requires Airflow 2.9 or higher\nexample_map_index_template:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"Example of TaskFlow powered DAG that includes dynamic task mapping\"\n  schedule_interval: \"0 3 * * *\"\n  default_view: \"graph\"\n  tasks:\n    dynamic_task_with_named_mapping:\n      decorator: airflow.decorators.task\n      python_callable: sample.extract_last_name\n      map_index_template: \"{{ custom_mapping_key }}\"\n      expand:\n        full_name:\n          - Lucy Black\n          - Vera Santos\n          - Marks Spencer\n</code></pre>"},{"location":"features/dynamic_tasks/#how-it-works_1","title":"How it works","text":"<ol> <li>map_index_template:    Customizes the naming of dynamically mapped tasks using a Jinja2 expression. In this example, it uses <code>custom_mapping_key</code> from the task context to define task names.</li> <li>expand:    Dynamically generates tasks for each entry in the <code>full_name</code> list<ul> <li>Lucy Black</li> <li>Vera Santos</li> <li>Marks Spencer</li> </ul> </li> <li>Dynamic Task Naming:    The <code>custom_mapping_key</code> is set to the first name of each person, e.g., Lucy, Vera, and Marks using the callable function <code>extract_last_name</code>. This callable function is kept in sample.py</li> </ol>"},{"location":"features/dynamic_tasks/#airflow-named-mapped-tasks-view","title":"Airflow named mapped tasks view","text":"<p>The image below shows that the <code>map_index</code> gets the first name of the person in the mapped tasks with the above configuration.</p> <p></p>"},{"location":"features/dynamic_tasks/#scope-and-limitations","title":"Scope and limitations","text":"<p>The Airflow documentation on dynamic task mapping provides various examples of this feature. While the previous sections have discussed the forms supported by DAG Factory, it\u2019s important to note the scenarios that have not been tested or are known to be unsupported.</p> <p>The following cases are tested and expected to work (you can refer to previous sections on how to use them with DAG Factory):</p> <ul> <li>Simple mapping</li> <li>Task-generated mapping</li> <li>Repeated mapping</li> <li>Adding parameters that do not expand (partial)</li> <li>Mapping over multiple parameters</li> <li>Named mapping (map_index_template)</li> </ul> <p>The following cases are untested but are expected to work:</p> <ul> <li>Mapping with non-TaskFlow operators</li> <li>Mapping over the result of classic operators</li> <li>Filtering items from a mapped task</li> </ul> <p>The following cases are untested and may not work:</p> <ul> <li>Assigning multiple parameters to a non-TaskFlow operator</li> <li>Mapping over a task group</li> <li>Transforming expanding data</li> <li>Combining upstream data (aka \u201czipping\u201d)</li> </ul>"},{"location":"getting-started/quick-start-airflow-standalone/","title":"DAG Factory: Quick Start Guide With Airflow","text":"<p>DAG Factory is a Python library Apache Airflow\u00ae that simplifies DAG creation using declarative YAML configuration files instead of Python.</p>"},{"location":"getting-started/quick-start-airflow-standalone/#prerequisites","title":"Prerequisites","text":"<p>The minimum requirements for dag-factory are:</p> <ul> <li>Python 3.8.0+</li> <li>Apache Airflow\u00ae 2.0+</li> </ul>"},{"location":"getting-started/quick-start-airflow-standalone/#step-1-create-a-python-virtual-environment","title":"Step 1: Create a Python Virtual Environment","text":"<p>Create and activate a virtual environment:</p> <pre><code>python3 -m venv dagfactory_env\nsource dagfactory_env/bin/activate\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-2-install-apache-airflow","title":"Step 2: Install Apache Airflow","text":"<p>Install Apache Airflow\u00ae:</p> <ol> <li> <p>Create a directory for your project and navigate to it:</p> <pre><code>mkdir dag-factory-quick-start &amp;&amp; cd dag-factory-quick-start\n</code></pre> </li> <li> <p>Set the <code>AIRFLOW_HOME</code> environment variable:</p> <pre><code>export AIRFLOW_HOME=$(pwd)\nexport AIRFLOW__CORE__LOAD_EXAMPLES=False\n</code></pre> </li> <li> <p>Install Apache Airflow:</p> <pre><code>pip install apache-airflow\n</code></pre> </li> </ol>"},{"location":"getting-started/quick-start-airflow-standalone/#step-3-install-dag-factory","title":"Step 3: Install DAG Factory","text":"<p>Install the DAG Factory library in your virtual environment:</p> <pre><code>pip install dag-factory\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-4-set-up-the-dags-folder","title":"Step 4: Set Up the DAGS Folder","text":"<p>Create a DAGs folder inside the $AIRFLOW_HOME directory, which is where your DAGs will be stored:</p> <pre><code>mkdir dags\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-5-define-a-dag-in-yaml","title":"Step 5: Define a DAG in YAML","text":"<p>DAG Factory uses YAML files to define DAG configurations. Create a file named <code>example_dag_factory.yml</code> in the <code>$AIRFLOW_HOME/dags</code> folder with the following content:</p> example_dag_factory.yml<pre><code>default:\n  default_args:\n    catchup: false,\n    start_date: 2024-11-11\n\nbasic_example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule_interval: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-6-generate-the-dag-from-yaml","title":"Step 6: Generate the DAG from YAML","text":"<p>Create a Python script named <code>example_dag_factory.py</code> in the <code>$AIRFLOW_HOME/dags</code> folder. This script will generate the DAG from the YAML configuration</p> example_dag_factory.py<pre><code>import os\nfrom pathlib import Path\n\n# The following import is here so Airflow parses this file\n# from airflow import DAG\nimport dagfactory\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\n\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\n\nconfig_file = str(CONFIG_ROOT_DIR / \"example_dag_factory.yml\")\n\nexample_dag_factory = dagfactory.DagFactory(config_file)\n\n# Creating task dependencies\nexample_dag_factory.clean_dags(globals())\nexample_dag_factory.generate_dags(globals())\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-7-start-airflow","title":"Step 7: Start Airflow","text":"<p>To start the Airflow environment with your DAG Factory setup, run the following command:</p> <pre><code>airflow standalone\n</code></pre> <p>This will take a few minutes to set up. Once completed, you can access the Airflow UI and the generated DAG at <code>http://localhost:8080</code> \ud83d\ude80.</p>"},{"location":"getting-started/quick-start-airflow-standalone/#view-your-generated-dag","title":"View Your Generated DAG","text":"<p>Once Airflow is up and running, you can login with the username <code>admin</code> and the password in <code>$AIRFLOW_HOME/standalone_admin_password.txt</code>. You should be able to see your generated DAG in the Airflow UI.</p>"},{"location":"getting-started/quick-start-airflow-standalone/#generated-dag","title":"Generated DAG","text":""},{"location":"getting-started/quick-start-airflow-standalone/#graph-view","title":"Graph View","text":"<p>Checkout examples for generating more advanced DAGs.</p>"},{"location":"getting-started/quick-start-astro-cli/","title":"DAG Factory: Quick Start Guide With Astro CLI","text":"<p>DAG Factory is a Python library Apache Airflow\u00ae that simplifies DAG creation using declarative YAML configuration files instead of Python.</p>"},{"location":"getting-started/quick-start-astro-cli/#prerequisites","title":"Prerequisites","text":"<p>The minimum requirements for dag-factory are:</p> <ul> <li>Python 3.8.0+</li> <li>Astro CLI</li> </ul>"},{"location":"getting-started/quick-start-astro-cli/#step-1-initialize-airflow-project","title":"Step 1: Initialize Airflow Project","text":"<p>Create a new directory and initialize your Astro CLI project:</p> <pre><code>mkdir dag-factory-quick-start &amp;&amp; cd dag-factory-quick-start\n\nastro dev init\n</code></pre> <p>This will set up the necessary Airflow files and directories.</p>"},{"location":"getting-started/quick-start-astro-cli/#step-2-install-dag-factory","title":"Step 2: Install DAG Factory","text":"<p>Install DAG Factory in your Airflow environment:</p> <ol> <li>Add dag-factory as a dependency to the <code>requirements.txt</code> file created during the project initialization.</li> </ol>"},{"location":"getting-started/quick-start-astro-cli/#step-3-define-a-dag-in-yaml","title":"Step 3: Define a DAG in YAML","text":"<p>DAG Factory uses YAML files to define DAG configurations. Create a file named <code>example_dag_factory.yml</code> in the <code>$AIRFLOW_HOME/dags</code> folder with the following content:</p> example_dag_factory.yml<pre><code>default:\n  default_args:\n    catchup: false,\n    start_date: 2024-11-11\n\nbasic_example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule_interval: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  tasks:\n    task_1:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.bash_operator.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n</code></pre>"},{"location":"getting-started/quick-start-astro-cli/#step-4-generate-the-dag-from-yaml","title":"Step 4: Generate the DAG from YAML","text":"<p>Create a Python script named <code>example_dag_factory.py</code> in the <code>$AIRFLOW_HOME/dags</code> folder. This script will generate the DAG from the YAML configuration</p> example_dag_factory.py<pre><code>import os\nfrom pathlib import Path\n\n# The following import is here so Airflow parses this file\n# from airflow import DAG\nimport dagfactory\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\n\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\n\nconfig_file = str(CONFIG_ROOT_DIR / \"example_dag_factory.yml\")\n\nexample_dag_factory = dagfactory.DagFactory(config_file)\n\n# Creating task dependencies\nexample_dag_factory.clean_dags(globals())\nexample_dag_factory.generate_dags(globals())\n</code></pre>"},{"location":"getting-started/quick-start-astro-cli/#step-5-start-airflow-project","title":"Step 5: Start Airflow Project","text":"<p>Once you've set up your YAML configuration and Python script, start the Airflow environment with the following command:</p> <pre><code>astro dev start\n</code></pre> <p>This will take a few minutes to set up. Once completed, you can access the Airflow UI and the generated DAG at <code>http://localhost:8080</code> \ud83d\ude80.</p>"},{"location":"getting-started/quick-start-astro-cli/#view-your-generated-dag","title":"View Your Generated DAG","text":"<p>Once Airflow is up and running, you can login with the username <code>admin</code> and the password <code>admin</code>. You should be able to see your generated DAG in the Airflow UI.</p>"},{"location":"getting-started/quick-start-astro-cli/#generated-dag","title":"Generated DAG","text":""},{"location":"getting-started/quick-start-astro-cli/#graph-view","title":"Graph View","text":"<p>Checkout examples for generating more advanced DAGs.</p>"}]}