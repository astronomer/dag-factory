{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DAG Factory documentation","text":"<p>Everything you need to know about how to build Apache Airflow\u00ae workflows using YAML files.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>Are you new to DAG Factory? This is the place to start!</p> <ul> <li>DAG Factory at a glance<ul> <li>Quickstart with Airflow standalone</li> <li>Quickstart with Astro CLI</li> </ul> </li> <li>Using YAML instead of Python<ul> <li>Traditional Airflow Operators</li> <li>TaskFlow API</li> </ul> </li> </ul>"},{"location":"#command-line","title":"Command line","text":"<ul> <li>Using DAG Factory CLI features</li> </ul>"},{"location":"#configuration","title":"Configuration","text":"<ul> <li>Configuring your workflows<ul> <li>Environment variables</li> <li>Defaults</li> </ul> </li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Dynamic tasks</li> <li>Datasets scheduling</li> <li>Callbacks</li> <li>Custom operators</li> <li>Multiple configuration files</li> <li>HttpSensor</li> </ul>"},{"location":"#migrate-to-dag-factory-v100","title":"Migrate to DAG-Factory V1.0.0","text":"<ul> <li>Migration Guide</li> </ul>"},{"location":"#getting-help","title":"Getting help","text":"<p>Having trouble? We'd like to help!</p> <ul> <li>Report bugs, questions and feature requests in our ticket tracker.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>DAG Factory is an Open-Source project. Learn about its development process and about how you can contribute:</p> <ul> <li>Contributing to DAG Factory</li> <li>Github repository</li> </ul>"},{"location":"#license","title":"License","text":"<p>To learn more about the terms and conditions for use, reproduction and distribution, read the Apache License 2.0.</p>"},{"location":"#privacy-notice","title":"Privacy Notice","text":"<p>This project follows Astronomer's Privacy Policy.</p> <p>For further information, read this</p>"},{"location":"#security-policy","title":"Security Policy","text":"<p>Check the project's Security Policy to learn how to report security vulnerabilities in DAG Factory and how security issues reported to the DAG Factory security team are handled.</p> <p></p>"},{"location":"migration_guide/","title":"Migrate to DAG-Factory v1.0.0","text":""},{"location":"migration_guide/#airflow-providers-dependency","title":"Airflow Providers Dependency","text":"<p>If you're using operators or sensors from apache-airflow-providers-http or apache-airflow-providers-cncf-kubernetes, you may encounter import errors, causing your DAG to fail. With the new release, DAG Factory no longer enforces the installation of Airflow providers. We recommend that you manually install the required Airflow providers in your environment.</p> <p>For more details, check out this PR.</p>"},{"location":"migration_guide/#removed-clean_dags-method-from-dagfactory","title":"Removed clean_dags() Method from DagFactory","text":"<p>The <code>clean_dags()</code> method is no longer part of the <code>DagFactory</code> class and has been removed. If your DAG was using this method, you can safely remove it from your DAG file. The DAG refresh behavior is now controlled by the Airflow config setting <code>AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL</code>.</p> <p>For more details, check out this PR.</p>"},{"location":"migration_guide/#removed-inconsistent-dag-factory-parameters","title":"Removed Inconsistent DAG Factory Parameters","text":"<p>DAG Factory no longer accepts the parameters <code>dagrun_timeout_sec</code>, <code>retry_delay_sec</code>, <code>sla_secs</code>, <code>execution_delta_secs</code> and <code>execution_timeout_secs</code> in the YAML/YML configuration. If your DAG was using these parameters, we recommend switching to their Airflow equivalents: <code>dagrun_timeout</code>, <code>retry_delay</code>, <code>sla</code>, <code>execution_delta</code>, and <code>execution_timeout</code>.</p> <p>Example:</p> <pre><code>dagrun_timeout:\n  __type__: datetime.timedelta\n  seconds: 300\n</code></pre> <pre><code>retry_delay:\n  __type__: datetime.timedelta\n  seconds: 10\n</code></pre> <p>For more examples, check out the documentation at Custom Python Objects in Configuration.</p> <p>For further details, refer to this  PR.</p>"},{"location":"migration_guide/#list-based-airflow-dag-tasks-and-task-groups","title":"List-Based Airflow DAG Tasks and Task Groups","text":"<p>While Airflow DAG tasks and task groups defined as dictionaries are still supported, we recommend transitioning to the list-based approach for better readability and consistency.</p>"},{"location":"migration_guide/#example","title":"Example","text":"<pre><code>basic_example_dag:\n  schedule: \"0 3 * * *\"\n  start_date:\n    __type__: datetime.datetime\n    year: 2025\n    month: 1\n    day: 1\n  catchup: false\n  task_groups:\n    - group_name: \"example_task_group\"\n      tooltip: \"this is an example task group\"\n      dependencies: [task_1]\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 1\"\n    - task_id: \"task_2\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    - task_id: \"task_3\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 3\"\n      dependencies: [task_1]\n      task_group_name: \"example_task_group\"\n</code></pre> <p>For more examples, check out the DAG's.</p> <p>For further details, refer to this  PR.</p>"},{"location":"migration_guide/#remove-legacy-type-casting-for-kubernetespodoperator","title":"Remove Legacy Type Casting for KubernetesPodOperator","text":"<p>With the latest update, the DAG-Factory <code>KubernetesPodOperator</code> now accepts a YAML dictionary only if it is compatible with the Airflow <code>KubernetesPodOperator</code>. You must use the <code>__type__</code> syntax to specify Kubernetes objects.</p>"},{"location":"migration_guide/#example_1","title":"Example","text":"<pre><code>kubernetes_pod_dag:\n  start_date: 2025-01-01\n  schedule: \"@daily\"\n  description: \"A DAG that runs a simple KubernetesPodOperator task\"\n  catchup: false\n  tasks:\n    - task_id: hello-world-pod\n      operator: airflow.providers.cncf.kubernetes.operators.pod.KubernetesPodOperator\n      config_file: \"path/to/kube/config\"\n      image: \"python:3.12-slim\"\n      cmds: [\"python\", \"-c\"]\n      arguments: [\"print('Hello from KubernetesPodOperator!')\"]\n      name: \"example-pod-task\"\n      namespace: \"default\"\n      get_logs: true\n      container_resources:\n        __type__: kubernetes.client.models.V1ResourceRequirements\n        limits:\n          cpu: \"1\"\n          memory: \"1024Mi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"512Mi\"\n</code></pre> <p>For more details, check out this PR.</p>"},{"location":"comparison/","title":"Using YAML instead of Python","text":"<p>By default, Apache Airflow\u00ae users write their workflows, or sequences of tasks, in Python.</p> <p>DAG Factory offers an alternative interface, allowing users to represent Airflow workflows via YAML files, often using less code.</p> <p>This section illustrates a few examples of how to represent the same workflow using plain Airflow Python DAGs in comparison to their representation using DAG Factory YAML files.</p> <ul> <li>Traditional Airflow Operators</li> <li>TaskFlow API</li> </ul>"},{"location":"comparison/taskflow_api/","title":"TaskFlow API: Using YAML instead of Python","text":"<p>For users that employ lots of Python functions in their DAGs, TaskFlow API represent a simpler way to transform functions into tasks, with a more intuitive way of passing data between them. They were  introduced in Airflow 2 as an alternative to Airflow traditional operators.</p> <p>The following section shows how to represent an Airflow DAG using TaskFlow API and how to define the same DAG using DAG Factory. Ultimately, both implementations use the same Airflow operators. The main difference is the language used to declare the workflow: one uses Python and the other uses YAML.</p>"},{"location":"comparison/taskflow_api/#goal","title":"Goal","text":"<p>Let's say we'd like to create a workflow that performs the following:</p> <ol> <li>Create a list of PyPI projects to be analysed.</li> <li>Fetch the statistics for each of these projects.</li> <li>Summarize the selected statistics as Markdown, using Python.</li> </ol> <p>We will implement all these steps using the Airflow <code>task</code> decorator, and the last task will generate a Markdown table similar to:</p> <pre><code>| package_name      |   last_day |   last_month |   last_week |\n|:------------------|-----------:|-------------:|------------:|\n| apache-airflow    |     852242 |     28194255 |     6253861 |\n| astronomer-cosmos |     442531 |     13354870 |     3127750 |\n| dag-factory       |      10078 |       354085 |       77752 |\n</code></pre> <p>The main logic is implemented as plain Python functions in pypi_stats.py:</p> pypi_stats.py<pre><code>def get_pypi_projects_list(**kwargs: dict[str, Any]) -&gt; list[str]:\n    \"\"\"\n    Return a list of PyPI project names to be analysed.\n    \"\"\"\n    projects_from_ui = kwargs.get(\"dag_run\").conf.get(\"pypi_projects\") if kwargs.get(\"dag_run\") else None\n    if projects_from_ui is None:\n        pypi_projects = DEFAULT_PYPI_PROJECTS\n    else:\n        pypi_projects = projects_from_ui\n    return pypi_projects\n\n\ndef fetch_pypi_stats_data(package_name: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Given a PyPI project name, return the PyPI stats data associated to it.\n    \"\"\"\n    url = f\"https://pypistats.org/api/packages/{package_name}/recent\"\n    response = httpx.get(url)\n\n    try:\n        response.raise_for_status()\n        package_json = response.json()\n        package_data = package_json[\"data\"]\n        package_data[\"package_name\"] = package_name\n        return package_data\n\n    except httpx.HTTPStatusError as e:\n        logger.error(f\"HTTP Error while fetching {package_name}: {e.response.status_code}\")\n        logger.error(f\"Response content: {e.response.text}\")\n    except ValueError as e:\n        logger.error(f\"JSON Decode Error for {package_name}: {e}\")\n        logger.error(f\"Response content: {response.text}\")\n    except KeyError as e:\n        logger.error(f\"Unexpected response format for {package_name}: {e}\")\n        logger.error(f\"Response content: {response.text}\")\n\n    return {\n        \"package_name\": package_name,\n        \"last_day\": 0,\n        \"last_week\": 0,\n        \"last_month\": 0,\n    }\n\n\ndef summarize(data: list[dict[str, Any]]) -&gt; str:\n    \"\"\"\n    Given a list with PyPI stats data, create a table summarizing it, sorting by the last day total downloads.\n    \"\"\"\n    df = pd.DataFrame(data)\n    first_column = \"package_name\"\n    sorted_columns = [first_column] + [col for col in df.columns if col != first_column]\n    df = df[sorted_columns].sort_values(by=\"last_day\", ascending=False)\n    markdown_output = df.to_markdown(index=False)\n    logger.info(markdown_output)\n    return markdown_output\n</code></pre>"},{"location":"comparison/taskflow_api/#implementation","title":"Implementation","text":"<p>As a reference, the following workflows run using Airflow 2.10.2 and DAG Factory 0.21.0.</p>"},{"location":"comparison/taskflow_api/#plain-airflow-python-dag","title":"Plain Airflow Python DAG","text":"example_pypi_stats_plain_airflow.py<pre><code>from __future__ import annotations\n\nfrom datetime import datetime\nfrom typing import Any\n\ntry:\n    from airflow.sdk.definitions.dag import DAG\n    from airflow.sdk.definitions.decorators import task\nexcept ImportError:\n    from airflow.decorators import task\n    from airflow.models.dag import DAG\nfrom pypi_stats import fetch_pypi_stats_data, get_pypi_projects_list, summarize\n\nwith DAG(dag_id=\"example_pypi_stats_plain_airflow\", schedule=None, start_date=datetime(2022, 3, 4)) as dag:\n\n    @task\n    def get_pypi_projects_list_():\n        return get_pypi_projects_list()\n\n    @task\n    def fetch_pypi_stats_data_(project_name: str):\n        return fetch_pypi_stats_data(project_name)\n\n    @task\n    def summarize_(values: list[dict[str, Any]]):\n        return summarize(values)\n\n    pypi_stats_data = fetch_pypi_stats_data_.expand(project_name=get_pypi_projects_list_())\n    summarize_(pypi_stats_data)\n</code></pre>"},{"location":"comparison/taskflow_api/#alternative-dag-factory-yaml","title":"Alternative DAG Factory YAML","text":"example_pypi_stats_dagfactory.yml<pre><code>example_pypi_stats_dagfactory:\n  default_args:\n    start_date: 2022-03-04\n  tasks:\n    - task_id: \"get_pypi_projects_list\"\n      decorator: airflow.decorators.task\n      python_callable: pypi_stats.get_pypi_projects_list\n    - task_id: \"fetch_pypi_stats_data\"\n      decorator: airflow.decorators.task\n      python_callable: pypi_stats.fetch_pypi_stats_data\n      expand:\n        package_name: +get_pypi_projects_list\n    - task_id: \"summarize\"\n      decorator: airflow.decorators.task\n      python_callable: pypi_stats.summarize\n      values: +fetch_pypi_stats_data\n</code></pre>"},{"location":"comparison/taskflow_api/#comparison","title":"Comparison","text":""},{"location":"comparison/taskflow_api/#goal_1","title":"Goal","text":"<p>Both implementations accomplish the same goal and result in the expected Markdown table.</p>"},{"location":"comparison/taskflow_api/#airflow-graph-view","title":"Airflow Graph view","text":"<p>As shown in the screenshots below, both the DAG created using Python with standard Airflow and the DAG created using YAML and DAG Factory look identical, from a graph topology perspective, and also from the underlining operators being used.</p>"},{"location":"comparison/taskflow_api/#graph-view-plain-airflow-python-dag","title":"Graph view: Plain Airflow Python DAG","text":""},{"location":"comparison/taskflow_api/#graph-view-alternative-dag-factory-yaml","title":"Graph view: Alternative DAG Factory YAML","text":""},{"location":"comparison/taskflow_api/#airflow-dynamic-task-mapping","title":"Airflow Dynamic Task Mapping","text":"<p>In both workflows, we are dynamically generating a task for each PyPI repo.</p>"},{"location":"comparison/taskflow_api/#mapped-tasks-plain-airflow-python-dag","title":"Mapped Tasks: Plain Airflow Python DAG","text":""},{"location":"comparison/taskflow_api/#mapped-tasks-alternative-dag-factory-yaml","title":"Mapped Tasks: Alternative DAG Factory YAML","text":""},{"location":"comparison/taskflow_api/#airflow-code-view","title":"Airflow Code view","text":"<p>From an Airflow UI perspective, the content displayed in the \"Code\" view is the main difference between the two implementations. While Airflow renders the original Python DAG, as expected, in the case of the YAML DAGs, Airflow displays the Python file that references the DAG Factory YAML files:</p> example_load_yaml_dags.py<pre><code>import os\nfrom pathlib import Path\n\nfrom dagfactory import load_yaml_dags\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\nconfig_dir = str(CONFIG_ROOT_DIR / \"comparison\")\n\nload_yaml_dags(\n    globals_dict=globals(),\n    dags_folder=config_dir,\n)\n</code></pre>"},{"location":"comparison/taskflow_api/#code-view-plain-airflow-python-dag","title":"Code view: Plain Airflow Python DAG","text":""},{"location":"comparison/taskflow_api/#code-view-alternative-dag-factory-yaml","title":"Code view: Alternative DAG Factory YAML","text":"<p>To overcome this limitation, DAG Factory appends the YAML content to the DAG Documentation so users can better troubleshoot the DAG:</p> <p></p>"},{"location":"comparison/traditional_operators/","title":"Traditional Operators: Using YAML instead of Python","text":"<p>Traditionally, operators are Airflow's building blocks, and while they are robust and diverse, they can sometimes lead to boilerplate-heavy DAGs compared to the newer TaskFlow API.</p> <p>Most of the Airflow providers come with built-in traditional operators. Some examples include <code>BashOperator</code>, <code>PythonOperator</code>, <code>KubernetesPodOperator</code>, and  <code>PostgresOperator</code>.</p> <p>Below, we illustrate how to represent an Airflow DAG using traditional operators and how to define the same DAG using DAG Factory. Ultimately, both implementations use the same Airflow operators. The main difference is the language used to declare the workflow: one uses Python and the other uses YAML.</p>"},{"location":"comparison/traditional_operators/#goal","title":"Goal","text":"<p>Let's say we'd like to create a workflow that performs the following:</p> <ol> <li>Retrieve the top ten stories from Hacker News using the Hacker News API.</li> <li>Fetch the details for the two top stories using the Hacker News API.</li> <li>Summarize the selected stories as Markdown, using Python.</li> </ol> <p>We will implement the first two steps using <code>BashOperator</code> and the third step using <code>PythonOperator</code>. The last task will generate a <code>Markdown</code> snippet similar to:</p> <pre><code>| title                                                                       | url                                                                                                                    |\n|:----------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|\n| I keep turning my Google Sheets into phone-friendly webapps                 | https://arstechnica.com/gadgets/2024/12/making-tiny-no-code-webapps-out-of-spreadsheets-is-a-weirdly-fulfilling-hobby/ |\n| Coconut by Meta AI \u2013 Better LLM Reasoning with Chain of Continuous Thought? | https://aipapersacademy.com/chain-of-continuous-thought/                                                               |\n</code></pre> <p>The main logic is implemented as plain Python functions in hacker_news.py:</p> pypi_stats.py<pre><code>def summarize(**kwargs):\n    \"\"\"\n    Given the Airflow context is provided to this function, it will extract the XCom hackernews records from its\n    upstream tasks and summarise in Markdown.\n    \"\"\"\n    ti = kwargs[\"ti\"]\n    upstream_task_ids = ti.task.upstream_task_ids  # Get upstream task IDs dynamically\n    values = [json.loads(ti.xcom_pull(task_ids=task_id)) for task_id in upstream_task_ids]\n\n    df = pd.DataFrame(values)\n    selected_columns = [\"title\", \"url\"]\n    df = df[selected_columns]\n    markdown_output = df.to_markdown(index=False)\n    print(markdown_output)\n    return markdown_output\n</code></pre>"},{"location":"comparison/traditional_operators/#implementation","title":"Implementation","text":"<p>As a reference, the following workflows run using Airflow 2.10.2 and DAG Factory 0.21.0.</p>"},{"location":"comparison/traditional_operators/#plain-airflow-python-dag","title":"Plain Airflow Python DAG","text":"example_hackernews_plain_airflow.py<pre><code>from datetime import datetime\n\ntry:\n    from airflow.providers.standard.operators.bash import BashOperator\n    from airflow.providers.standard.operators.python import PythonOperator\n    from airflow.sdk.definitions.dag import DAG\nexcept ImportError:\n    from airflow.models.dag import DAG\n    from airflow.operators.bash import BashOperator\n    from airflow.operators.python import PythonOperator\n\nfrom hacker_news import summarize\n\nwith DAG(dag_id=\"example_hackernews_plain_airflow\", schedule=None, start_date=datetime(2022, 3, 4)) as dag:\n\n    fetch_top_ten_news = BashOperator(\n        task_id=\"fetch_top_ten_news\",\n        bash_command=\"curl -s https://hacker-news.firebaseio.com/v0/topstories.json  | jq -c -r '.[0:10]'\",\n    )\n\n    fetch_first_top_news = BashOperator(\n        task_id=\"fetch_first_top_news\",\n        bash_command=\"\"\"\n            echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[0]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\n        \"\"\",\n    )\n\n    fetch_second_top_news = BashOperator(\n        task_id=\"fetch_second_news\",\n        bash_command=\"\"\"\n            echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[1]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\n        \"\"\",\n    )\n\n    summarize = PythonOperator(task_id=\"summarize\", python_callable=summarize)\n\n    fetch_top_ten_news &gt;&gt; [fetch_first_top_news, fetch_second_top_news] &gt;&gt; summarize\n</code></pre>"},{"location":"comparison/traditional_operators/#alternative-dag-factory-yaml","title":"Alternative DAG Factory YAML","text":"example_hackernews_dagfactory.py<pre><code>example_hackernews_dagfactory:\n  default_args:\n    start_date: 2022-03-04\n  tasks:\n    - task_id: \"fetch_top_ten_news\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"curl -s https://hacker-news.firebaseio.com/v0/topstories.json  | jq -c -r '.[0:10]'\"\n    - task_id: \"fetch_first_top_news\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[0]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\"\n      dependencies: [fetch_top_ten_news]\n    - task_id: \"fetch_second_top_news\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo {{ task_instance.xcom_pull(task_ids='fetch_top_ten_news') }} | jq -c -r '.[1]' |  xargs -I {} curl -s 'https://hacker-news.firebaseio.com/v0/item/{}.json'\"\n      dependencies: [fetch_top_ten_news]\n    - task_id: \"summarize\"\n      operator: airflow.operators.python.PythonOperator\n      python_callable: hacker_news.summarize\n      dependencies: [fetch_first_top_news, fetch_second_top_news]\n</code></pre>"},{"location":"comparison/traditional_operators/#comparison","title":"Comparison","text":""},{"location":"comparison/traditional_operators/#goal_1","title":"Goal","text":"<p>Both implementations accomplish the same goal and result in the expected Markdown table.</p>"},{"location":"comparison/traditional_operators/#airflow-graph-view","title":"Airflow Graph view","text":"<p>As shown in the screenshots below, both the DAG created using Python with standard Airflow and the DAG created using YAML and DAG Factory look identical, from a graph topology perspective, and also from the underlining operators being used.</p>"},{"location":"comparison/traditional_operators/#graph-view-plain-airflow-python-dag","title":"Graph view: Plain Airflow Python DAG","text":""},{"location":"comparison/traditional_operators/#graph-view-alternative-dag-factory-yaml","title":"Graph view: Alternative DAG Factory YAML","text":""},{"location":"comparison/traditional_operators/#airflow-code-view","title":"Airflow Code view","text":"<p>From an Airflow UI perspective, the content displayed in the \"Code\" view is the main difference between the two implementations. While Airflow renders the original Python DAG, as expected, in the case of the YAML DAGs, Airflow displays the Python file that references the DAG Factory YAML files:</p> example_load_yaml_dags.py<pre><code>import os\nfrom pathlib import Path\n\nfrom dagfactory import load_yaml_dags\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\nconfig_dir = str(CONFIG_ROOT_DIR / \"comparison\")\n\nload_yaml_dags(\n    globals_dict=globals(),\n    dags_folder=config_dir,\n)\n</code></pre>"},{"location":"comparison/traditional_operators/#code-view-plain-airflow-python-dag","title":"Code view: Plain Airflow Python DAG","text":""},{"location":"comparison/traditional_operators/#code-view-alternative-dag-factory-yaml","title":"Code view: Alternative DAG Factory YAML","text":"<p>To overcome this limitation, DAG Factory appends the YAML content to the DAG Documentation so users can better troubleshoot the DAG:</p> <p></p>"},{"location":"configuration/configuring_workflows/","title":"Configuring Your Workflows","text":"<p>DAG Factory allows you to define workflows in a structured, configuration-driven way using YAML files. You can define multiple workflows within a single YAML file based on your requirements.</p>"},{"location":"configuration/configuring_workflows/#key-elements-of-workflow-configuration","title":"Key Elements of Workflow Configuration","text":"<ul> <li>dag_id: Unique identifier for your DAG.</li> <li>default_args: Common arguments for all tasks.</li> <li>schedule: Specifies the execution schedule.</li> <li>tasks: Defines the Airflow tasks in your workflow.</li> <li>task_groups: Defines Airflow task groups to organize and group related tasks.</li> </ul>"},{"location":"configuration/configuring_workflows/#example-dag-configuration","title":"Example DAG Configuration","text":""},{"location":"configuration/configuring_workflows/#task-and-task-group-configuration-formats","title":"Task and Task Group Configuration Formats","text":"<p>DAG Factory supports two formats for defining <code>tasks</code> and <code>task_groups</code>:</p>"},{"location":"configuration/configuring_workflows/#list-format-recommended","title":"List Format (Recommended)","text":"<p>The list format is the recommended and more readable approach. In this format, tasks are defined as a list where each task includes a <code>task_id</code> field, and task groups are also defined as a list where each group includes a <code>group_name</code> field:</p> <p>Version Support</p> <p>List format support was introduced in version 1.0.0.</p> example_dag_factory.yml<pre><code>basic_example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  catchup: false\n  task_groups:\n    - group_name: \"example_task_group\"\n      tooltip: \"this is an example task group\"\n      dependencies: [task_1]\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 1\"\n    - task_id: \"task_2\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    - task_id: \"task_3\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 3\"\n      dependencies: [task_1]\n      task_group_name: \"example_task_group\"\n</code></pre>"},{"location":"configuration/configuring_workflows/#dictionary-format-legacy","title":"Dictionary Format (Legacy)","text":"<p>The dictionary format is also supported for backward compatibility. In this format, tasks are defined as a dictionary where the key is the task ID, and task groups are also defined as a dictionary where the key is the group name:</p> example_dag_factory_tasks_taskgroups_as_dict_format.yml<pre><code>basic_example_dag_dict_format:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag using dictionary format for tasks and task_groups\"\n  schedule_interval: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  catchup: false\n  task_groups:\n    example_task_group:\n      tooltip: \"this is an example task group\"\n      dependencies: [task_1]\n  tasks:\n    task_1:\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 1\"\n    task_2:\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 3\"\n      dependencies: [task_1]\n      task_group_name: example_task_group\n</code></pre> <p>Format Recommendation</p> <p>While both formats are supported, we recommend using the list format as it is more readable and easier to maintain.</p>"},{"location":"configuration/configuring_workflows/#check-out-more-configuration-params","title":"Check out more configuration params","text":"<ul> <li>Environment variables</li> <li>Defaults</li> </ul>"},{"location":"configuration/custom_py_object/","title":"Custom Python Object","text":"<p>DAG Factory supports the definition of complex Python objects directly in your YAML configuration files using a generalized object syntax. This feature, available from DAG Factory 0.23.0, allows you to represent objects such as <code>datetime</code>, <code>timedelta</code>, Airflow timetables, assets, and even Kubernetes objects in a declarative, readable way.</p>"},{"location":"configuration/custom_py_object/#why-use-generalised-objects","title":"Why Use Generalised Objects?","text":"<ul> <li>Expressiveness: Define complex parameters and objects in YAML, not just simple values.</li> <li>Reproducibility: Configuration is portable and version-controlled.</li> <li>Clarity: YAML mirrors the Python object structure, making it easy to understand and maintain.</li> </ul>"},{"location":"configuration/custom_py_object/#general-format","title":"General Format","text":"<p>A generalized object in YAML is defined using the <code>__type__</code> key, which specifies the full Python import path of the object/class. Arguments and attributes are then provided as additional keys.</p> <p>General YAML Format:</p> <pre><code>object_name:\n  __type__: &lt;python.import.path.ClassName&gt;\n  &lt;attribute1&gt;: &lt;value1&gt;\n  &lt;attribute2&gt;: &lt;value2&gt;\n  ...\n</code></pre> <ul> <li>Use <code>__args__</code> for positional arguments (as a list)</li> <li>Use other keys for keyword arguments or attributes</li> </ul>"},{"location":"configuration/custom_py_object/#supported-types-python-class-or-object","title":"Supported Types: Python Class or Object","text":"<p>The generalized object feature is extremely flexible\u2014it supports Python type or class that is importable in your Airflow environment. The table below shows just a few common examples, but you can use any class by specifying its full import path in <code>__type__</code>.</p> <p>Note</p> <p>If you use a custom or third-party class, make sure it is installed and importable in your Airflow environment.</p> Python Object/Class YAML <code>__type__</code> Value Example Use Case <code>datetime.datetime</code> <code>datetime.datetime</code> DAG start date <code>datetime.timedelta</code> <code>datetime.timedelta</code> Task timeout <code>airflow.timetables.trigger.CronTriggerTimetable</code> <code>airflow.timetables.trigger.CronTriggerTimetable</code> Custom scheduling <code>airflow.sdk.Asset</code> <code>airflow.sdk.Asset</code> Asset definition <code>airflow.io.path.ObjectStoragePath</code> <code>airflow.io.path.ObjectStoragePath</code> Object storage path <code>kubernetes.client.models.V1Pod</code> <code>kubernetes.client.models.V1Pod</code> Kubernetes pod override"},{"location":"configuration/custom_py_object/#examples","title":"Examples","text":""},{"location":"configuration/custom_py_object/#1-datetime-object","title":"1. Datetime Object","text":"<p>Python:</p> <pre><code>from datetime import datetime\nstart_date = datetime(2025, 1, 1)\n</code></pre> <p>YAML:</p> <pre><code>start_date:\n  __type__: datetime.datetime\n  year: 2025\n  month: 1\n  day: 1\n</code></pre>"},{"location":"configuration/custom_py_object/#2-timedelta-object","title":"2. Timedelta Object","text":"<p>Python:</p> <pre><code>from datetime import timedelta\nexecution_timeout = timedelta(hours=1)\n</code></pre> <p>YAML:</p> <pre><code>execution_timeout:\n  __type__: datetime.timedelta\n  hours: 1\n</code></pre>"},{"location":"configuration/custom_py_object/#3-airflow-timetable","title":"3. Airflow Timetable","text":"<p>Python:</p> <pre><code>from airflow.timetables.trigger import CronTriggerTimetable\nschedule = CronTriggerTimetable(cron=\"* * * * *\", timezone=\"UTC\")\n</code></pre> <p>YAML:</p> <pre><code>schedule:\n  __type__: airflow.timetables.trigger.CronTriggerTimetable\n  cron: \"* * * * *\"\n  timezone: UTC\n</code></pre>"},{"location":"configuration/custom_py_object/#4-asset-object","title":"4. Asset Object","text":"<p>Python:</p> <pre><code>from airflow.sdk import Asset\nmy_asset = Asset(name=\"sales_report\", uri=\"s3://data/object_storage_ops.csv\")\n</code></pre> <p>YAML:</p> <pre><code>my_asset:\n  __type__: airflow.sdk.Asset\n  name: \"sales_report\"\n  uri: \"s3://data/object_storage_ops.csv\"\n</code></pre>"},{"location":"configuration/custom_py_object/#5-objectstoragepath-with-positional-arguments","title":"5. ObjectStoragePath with Positional Arguments","text":"<p>Python:</p> <pre><code>from airflow.io.path import ObjectStoragePath\nmy_obj_storage = ObjectStoragePath(\"file:///data/object_storage_ops.csv\")\n</code></pre> <p>YAML:</p> <pre><code>my_obj_storage:\n  __type__: airflow.io.path.ObjectStoragePath\n  __args__:\n    - file:///data/object_storage_ops.csv\n</code></pre>"},{"location":"configuration/custom_py_object/#6-kubernetes-object-nested-example","title":"6. Kubernetes Object (Nested Example)","text":"<p>Python:</p> <pre><code>from kubernetes.client.models import V1Pod, V1PodSpec, V1Container, V1ResourceRequirements\npod_override = V1Pod(\n    spec=V1PodSpec(\n        containers=[\n            V1Container(\n                name=\"base\",\n                resources=V1ResourceRequirements(\n                    limits={\"cpu\": \"1\", \"memory\": \"1024Mi\"},\n                    requests={\"cpu\": \"0.5\", \"memory\": \"512Mi\"}\n                )\n            )\n        ]\n    )\n)\n</code></pre> <p>YAML:</p> <pre><code>pod_override:\n  __type__: kubernetes.client.models.V1Pod\n  spec:\n    __type__: kubernetes.client.models.V1PodSpec\n    containers:\n      __type__: builtins.list\n      items:\n        - __type__: kubernetes.client.models.V1Container\n          name: \"base\"\n          resources:\n            __type__: kubernetes.client.models.V1ResourceRequirements\n            limits:\n              cpu: \"1\"\n              memory: \"1024Mi\"\n            requests:\n              cpu: \"0.5\"\n              memory: \"512Mi\"\n</code></pre>"},{"location":"configuration/custom_py_object/#tips-best-practices","title":"Tips &amp; Best Practices","text":"<ul> <li>Always specify the full import path in <code>__type__</code> for clarity and reliability.</li> <li>Use <code>__args__</code> for positional arguments, and other keys for keyword arguments.</li> <li>For lists, use <code>__type__: builtins.list</code> and provide an <code>items</code> key with a list of objects.</li> <li>You can nest generalized objects to any depth, mirroring complex Python object graphs.</li> <li>This feature is especially useful for Airflow 3+ and advanced use cases (e.g., custom timetables, pod overrides).</li> </ul>"},{"location":"configuration/custom_py_object/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Import Errors: Ensure the Python class/module referenced in <code>__type__</code> is installed and importable in your Airflow environment.</li> <li>Type Errors: Double-check the arguments and structure match the Python class signature.</li> <li>YAML Syntax: Indentation and structure matter! Use a YAML linter if you encounter parsing issues.</li> </ul>"},{"location":"configuration/defaults/","title":"Defaults","text":"<p>DAG Factory allows you to define default values for DAG-level arguments and Airflow default_args. There are several ways to accomplish this:</p> <ul> <li>Define the <code>default_args</code> within each DAG definition in the DAGs YAML file;</li> <li>Declare a <code>default</code> block within the toplevel of the DAGs YAML file;</li> <li>Define the <code>default_args_config_dict</code> argument when instantiating the <code>DAGFactory</code> class;</li> <li>Create one or multiple <code>defaults.yml</code> and declare the <code>default_args_config_path</code> argument in the <code>DAGFactory</code> class. This approach includes support for combining multiple <code>defaults.yml</code> files.</li> </ul> <p>Although you cannot use the last two configurations together, you can use a combination of the first two configurations with either the third or the last.</p> <p>Below, we detail how to use each of these approaches and also how to combine them.</p>"},{"location":"configuration/defaults/#specifying-default_args-directly-in-the-dag-yaml-specification","title":"Specifying <code>default_args</code> directly in the DAG YAML specification","text":"<p>This configuration affects only the DAG where the <code>default_args</code> are defined.</p> <p>You can override or define specific <code>default_args</code> at the individual DAG level. This strategy allows you to customize arguments for each DAG without affecting others. Not only can existing <code>default_args</code> be overridden directly in a DAG configuration, but also other DAG-level arguments can be added.</p> <pre><code>etl:\n   default_args:\n      start_date: '2024-12-31'\n      retries: 1  # A new default_arg was added\n...\n</code></pre>"},{"location":"configuration/defaults/#yaml-top-level-default","title":"YAML top-level <code>default</code>","text":"<p>This configuration affects all the DAGs defined in the YAML file where the <code>default</code> block is declared.</p> <p>The <code>default</code> top-level block enables you to share standard settings and configurations across all DAGs in your YAML configuration, with the arguments automatically applied to each DAG defined in the file. It is one of DAG Factory's most powerful features; using defaults allows for the dynamic generation of multiple DAGs.</p>"},{"location":"configuration/defaults/#benefits-of-using-the-default-block","title":"Benefits of using the <code>default</code> block","text":"<ul> <li>Consistency: Ensures uniform configurations across all tasks and DAGs.</li> <li>Maintainability: Reduces duplication by centralizing common properties.</li> <li>Simplicity: Makes configurations easier to read and manage.</li> <li>Dynamic Generation: Use a single default block to generate more than a single DAG easily.</li> </ul>"},{"location":"configuration/defaults/#example-usage-of-the-default-block","title":"Example usage of the <code>default</code> block","text":""},{"location":"configuration/defaults/#specifying-default_args-in-the-default-block","title":"Specifying <code>default_args</code> in the <code>default</code> block","text":"<p>Using a <code>default</code> block in a YAML file allows for those key-value pairs to apply to each DAG defined in that same file. One of the most common examples is using a <code>default</code> block to specify <code>default_args</code> for each DAG defined in that file. Every DAG defined in the file inherits these arguments. Below is an example of this.</p> Usage of default block for default_args in YAML<pre><code>default:\n  default_args:\n    start_date: '2024-01-01'\n  schedule: 0 0 * * *\n  catchup: false\n  tags:\n    - \"data engineering\"\n\netl:\n  tasks:\n    - task_id: \"extract\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo extract\"\n    - task_id: \"transform\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo transform\"\n      dependencies:\n      - extract\n    - task_id: \"load\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo load\"\n      dependencies:\n      - transform\n</code></pre>"},{"location":"configuration/defaults/#example-using-of-default-block-for-dynamic-dag-generation","title":"Example using of default block for dynamic DAG generation","text":"<p>Not only can the <code>default</code> block in a YAML file be used to define <code>default_args</code> for one or more DAGs, you can also use it to create the skeleton of \"templated\" DAGs. In the example below, the <code>default</code> block defines both the <code>default_args</code> of a DAG, and also default Tasks. These Tasks provide a \"template\" for the DAGs defined in this file. Each DAG (<code>machine_learning</code>, <code>data_science</code>, <code>artificial_intelligence</code>) is defined using the values from the <code>default</code> block, and, like with <code>default_args</code>, can override these values. This is a powerful way to use DAG Factory to dynamically create DAGs using a single configuration.</p> Usage of default block in YAML<pre><code>  default:\n    default_args:\n      start_date: '2024-01-01'\n    schedule: 0 0 * * *\n    catchup: false\n    tags:\n      - dynamic\n    tasks:\n      - task_id: \"extract\"\n        operator: airflow.operators.bash.BashOperator\n        bash_command: \"echo extract\"\n      - task_id: \"transform\"\n        operator: airflow.operators.bash.BashOperator\n        bash_command: \"echo transform\"\n        dependencies:\n        - extract\n      - task_id: \"load\"\n        operator: airflow.operators.bash.BashOperator\n        dependencies:\n        - transform\n\n\n  machine_learning:\n    tasks:\n      - task_id: \"load\"\n        bash_command: \"echo machine_larning\"\n\n  data_science:\n    tasks:\n      - task_id: \"load\"\n        bash_command: \"echo data_science\"\n\n  artificial_intelligence:\n    tasks:\n      - task_id: \"load\"\n        bash_command: \"echo artificial_intelligence\"\n</code></pre>"},{"location":"configuration/defaults/#specifying-default-arguments-via-a-python-dictionary","title":"Specifying <code>default</code> arguments via a Python dictionary","text":"<p>This configuration affects DAGs created using the <code>DagFactory</code> class with the <code>default_args_config_dict</code> argument.</p> <p>It allows you to define DAG-level arguments, including the <code>default_args</code>, using Python dictionaries.</p>"},{"location":"configuration/defaults/#example-of-using-a-python-defined-default-configuration","title":"Example of using a Python-defined default configuration","text":"Usage of default block in YAML<pre><code>default:\n  default_args:\n    start_date: '2024-01-01'\n  schedule: 0 0 * * *\n  catchup: false\n  tags:\n    - dynamic\n  tasks:\n    - task_id: \"extract\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo extract\"\n    - task_id: \"transform\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo transform\"\n      dependencies:\n      - extract\n    - task_id: \"load\"\n      operator: airflow.operators.bash.BashOperator\n      dependencies:\n      - transform\n\n\nmachine_learning:\n  tasks:\n    - task_id: \"load\"\n      bash_command: \"echo machine_larning\"\n\ndata_science:\n  tasks:\n    - task_id: \"load\"\n      bash_command: \"echo data_science\"\n\nartificial_intelligence:\n  tasks:\n    - task_id: \"load\"\n      bash_command: \"echo artificial_intelligence\"\n</code></pre> <p>In this example, users create a YAML DAG by using the <code>DagFactory</code> class and declare default arguments in the form of a Python dictionary by setting the <code>default_args_config_dict</code> parameter in the <code>DAGFactory</code> class. This feature mirrors the functionality of  manually specifying a <code>default_args_config_path</code> in the <code>DAGFactory</code> class, described in the next section.</p> Usage of default_args_config_dict in .py file<pre><code>                \"bash_command\": \"echo transform\",\n                \"dependencies\": [\"extract\"],\n            },\n            {\n                \"task_id\": \"load\",\n                \"operator\": \"airflow.operators.bash.BashOperator\",\n                \"bash_command\": \"echo load\",\n</code></pre>"},{"location":"configuration/defaults/#declaring-default-values-using-the-defaultsyml-file","title":"Declaring default values using the <code>defaults.yml</code> file","text":"<p>This configuration affects DAGs created using the <code>DagFactory</code> class without the <code>default_args_config_dict</code> argument.</p> <p>If a <code>defaults.yml</code> file is present in the same directory as the YAML file representing the DAG, DagFactory will use it to build the DAG.</p> <p>If the <code>defaults.yml</code> is added to a separate directory, users can share it using <code>DagFactory</code>'s argument <code>default_args_config_path</code>.</p>"},{"location":"configuration/defaults/#example-of-declaring-the-default_args-using-defaultsyml","title":"Example of declaring the <code>default_args</code> using <code>defaults.yml</code>","text":"<p>Starting DAG Factory 0.22.0, you can also keep the <code>default_args</code> in the <code>defaults.yml</code> file. The configuration  from <code>defaults.yml</code> will be applied to all DAG Factory-generated DAGs. Be careful, DagFactory will apply these to all  generated DAGs.</p> defaults.yml<pre><code>  default_args:\n    start_date: \"2025-01-01\"\n    owner: \"global_owner\"\n</code></pre>"},{"location":"configuration/defaults/#example-usage-of-dag-level-configurations-in-defaultsyml","title":"Example usage of DAG-level configurations in <code>defaults.yml</code>","text":"<p>In Airflow, not all DAG-level arguments are supported under <code>default_args</code>, because they are DAG-specific and not    used by any operator classes, such as <code>schedule</code> and <code>catchup</code>. To set default values for those arguments, they need    to be added at the root level of <code>defaults.yml</code> as follows:</p> <p>```yaml    schedule: 0 1 * * *   # set DAG-specific arguments at the root level    catchup: False</p> <p>default_args:       start_date: '2024-12-31'   ...    ```</p>"},{"location":"configuration/defaults/#combining-multiple-defaultsyml-files","title":"Combining multiple <code>defaults.yml</code> files","text":"<p>It is possible to combine and merge the content of multiple <code>defaults.yml</code> files.</p> <p>To accomplish this, you should declare in the <code>default_args_config_path</code> a folder that is a parent folder of a DAG-defined <code>YAML</code> file. In this case, DAG Factory will merge all the <code>defaults.yml</code> configurations, following the directories' hierarchy, and give precedence to the arguments declared in the <code>defaults.yml</code> file closest to the DAG YAML file.</p> <p>As an example, let's say there are DagFactory DAGs defined inside the <code>a/b/c/some_dags.yml</code> file following this directory tree:</p> <pre><code>sample_project\n\u2514\u2500\u2500 a\n    \u251c\u2500\u2500 b\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 c\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults.yml\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 some_dags.yml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 defaults.yml\n    \u2514\u2500\u2500 defaults.yml\n</code></pre> <p>Assuming you instantiate the DAG by using:</p> <p><code>``python DagFactory(    \"a/b/c/some_dags.yml\",    default_args_config_path=\"a\" )</code></p> <p>The DAG will be using the default configuration defined in all the following files:</p> <ul> <li><code>a/b/c/some_dags.yml</code></li> <li><code>a/b/c/defaults.yml</code></li> <li><code>a/b/defaults.yml</code></li> <li><code>a/defaults.yml</code></li> </ul> <p>Following this precedence order. Illustrating, if the DAG <code>owner</code> is declared both in <code>a/b/c/defaults.yml</code> and in <code>a/defaults.yml</code>, the one that takes precedence is the <code>a/b/c/defaults.yml</code>, since it is closer to the DAG YAML file.</p>"},{"location":"configuration/defaults/#combining-multiple-methods-of-defining-default-values","title":"Combining multiple methods of defining default values","text":"<p>Given the various ways to specify top-level DAG arguments, including <code>default_args</code>, the following precedence order is applied if multiple places define the same argument:</p> <ol> <li>In the DAG configuration</li> <li>In the <code>default</code> block within the workflow's YAML file</li> <li>The arguments defined in <code>default_args_config_dict</code></li> <li>If (3) is not declared, the <code>defaults.yml</code> hierarchy.</li> </ol>"},{"location":"configuration/environment_variables/","title":"Environment variables","text":"<p>Starting release <code>0.20.0</code>, DAG Factory introduces support for referencing environment variables directly within YAML configuration files. This enhancement enables dynamic configuration paths and enhances workflow portability by resolving environment variables during DAG parsing.</p> <p>With this feature, DAG Factory removes the reliance on hard-coded paths, allowing for more flexible and adaptable configurations that work seamlessly across various environments.</p>"},{"location":"configuration/environment_variables/#example-yaml-configuration-with-environment-variables","title":"Example YAML Configuration with Environment Variables","text":"Reference environment variable in YAML<pre><code>example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  dag_display_name: \"Pretty Example DAG\"\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 1\"\n    - task_id: \"task_2\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    - task_id: \"task_3\"\n      operator: airflow.operators.python.PythonOperator\n      python_callable_name: print_hello\n      python_callable_file: $CONFIG_ROOT_DIR/print_hello.py\n      dependencies: [task_1]\n</code></pre> <p>In the above example, <code>$CONFIG_ROOT_DIR</code> is used to reference an environment variable that points to the root directory of your DAG configurations. During DAG parsing, it will be resolved to the value specified for the <code>CONFIG_ROOT_DIR</code> environment variable.</p>"},{"location":"configuration/jinja2_template/","title":"Jinja2 Template","text":"<p>This example shows how to use Apache Airflow\u00ae built-in Jinja templating within a YAML-based DAG definition using DAG-Factory</p>"},{"location":"configuration/jinja2_template/#example-dag","title":"Example DAG","text":"<pre><code>example_jinja2_template_dag:\n  default_args:\n    start_date: \"2025-01-01\"\n  schedule: \"@daily\"\n  description: \"A DAG that uses Airflow's built-in Jinja templates\"\n  catchup: false\n  tasks:\n    - task_id: \"print_run_id\"\n      operator: \"airflow.operators.bash.BashOperator\"\n      bash_command: \"echo 'run-id is {{ run_id }}'\"\n\n    - task_id: \"print_task\"\n      operator: \"airflow.operators.bash.BashOperator\"\n      bash_command: \"echo 'The task is {{ task }}'\"\n      dependencies:\n      - print_run_id\n</code></pre>"},{"location":"configuration/load_yaml_dags/","title":"load_yaml_dags Function","text":"<p>The <code>load_yaml_dags</code> function is responsible for loading DAG configurations from YAML or YML files in a specified folder (or from a specific YAML file or dictionary). It parses the files or dictionary and generates Airflow DAGs by utilizing the provided globals_dict.</p>"},{"location":"configuration/load_yaml_dags/#example-usage","title":"Example Usage","text":""},{"location":"configuration/load_yaml_dags/#1-loading-from-yaml-files-in-a-folder","title":"1. Loading from YAML Files in a Folder","text":"<pre><code>from dagfactory import load_yaml_dags\n\n# Load DAGs from a folder, e.g., /path/to/dags\nload_yaml_dags(globals_dict=globals(), dags_folder='/path/to/your/dags')\n</code></pre>"},{"location":"configuration/load_yaml_dags/#2-loading-from-a-specific-yaml-file","title":"2. Loading from a Specific YAML File","text":"<pre><code>from dagfactory import load_yaml_dags\n\n# Load DAG from a specific YAML file\nload_yaml_dags(globals_dict=globals(), config_filepath='/path/to/your/dag_config.yaml')\n</code></pre>"},{"location":"configuration/load_yaml_dags/#3-loading-from-yaml-file-with-default_args_config_path","title":"3. Loading from YAML File with default_args_config_path","text":"<pre><code>from dagfactory import load_yaml_dags\n\n# Load a single DAG from a YAML file with custom default arguments config path\nload_yaml_dags(\n    globals_dict=globals(),\n    config_filepath='/path/to/your/dag_config.yaml',\n    default_args_config_path='/path/to/your/default_args.yml'\n)\n</code></pre>"},{"location":"configuration/load_yaml_dags/#4-loading-from-a-dictionary","title":"4. Loading from a Dictionary","text":"<pre><code>from dagfactory import load_yaml_dags\n\n# Load DAG from a dictionary configuration\ndag_config_dict = {\n    # Your DAG configuration here\n}\nload_yaml_dags(globals_dict=globals(), config_dict=dag_config_dict)\n</code></pre>"},{"location":"configuration/load_yaml_dags/#5-loading-from-a-dictionary-with-default_args_config_dict","title":"5. Loading from a Dictionary with default_args_config_dict","text":"<pre><code>from dagfactory import load_yaml_dags\n\n# Load DAGs with custom default arguments from a dictionary\ndefault_args_dict = {\n    # Your default arguments\n}\n\ndag_config_dict = {\n     # Your DAG configuration here\n}\nload_yaml_dags(globals_dict=globals(), config_dict=dag_config_dict, default_args_config_dict=default_args_dict)\n</code></pre>"},{"location":"configuration/params/","title":"Params","text":"<p>Params in Airflow are a way to pass dynamic runtime configuration to tasks within a DAG. They allow tasks to be more flexible by enabling templated values to be injected during execution.</p>"},{"location":"configuration/params/#key-features","title":"Key Features","text":"<ul> <li>Available in DAG and task definitions.</li> <li>Can be templated using Jinja.</li> <li>Useful for customizing task behavior without modifying code.</li> </ul>"},{"location":"configuration/params/#example-dag","title":"Example DAG","text":"<pre><code>default:\n  catchup: false\n  default_args:\n    start_date: 2025-01-01\n\nexample_params:\n  params:\n    model_version: 1.0.0\n    input_uri: localhost\n  description: \"This is an DAG-Factory example dag with param\"\n  schedule_interval: \"@daily\"\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 'Running task 1 with model version {{ params.model_version }} and url: {{ params.input_uri }}'\"\n    - task_id: \"task_2\"\n      operator: airflow.operators.python.PythonOperator\n      params:\n        my_param: 10\n      python_callable: sample.read_params\n</code></pre>"},{"location":"configuration/params/#when-to-use","title":"When to Use","text":"<ul> <li>You want to reuse the same DAG for different input values.</li> <li>You want to change a task\u2019s behavior at runtime.</li> </ul>"},{"location":"configuration/schedule/","title":"Scheduling","text":"<ul> <li>Released in version: 0.23.0</li> </ul> <p>DAG-Factory offers flexible scheduling options so your DAGs can run on time or based on data availability. Whether you're triggering DAGs on a cron schedule, a time delta, or when an upstream asset is ready, you can configure it easily using the schedule field in your YAML.</p> <p>Below are the supported scheduling types, each with consistent structure and examples to help you get started.</p>"},{"location":"configuration/schedule/#how-to-use","title":"How to Use","text":"<ul> <li>Every schedule block must specify a type, such as <code>cron</code>, <code>timedelta</code>, <code>relativedelta</code>, <code>timetable</code>, or <code>assets</code>.</li> <li>The actual configuration goes under the value key.</li> <li>Only one schedule type should be defined per DAG.</li> </ul>"},{"location":"configuration/schedule/#example-overview","title":"Example Overview","text":"Type Description Use Case Example <code>cron</code> Run based on a cron string Every day at midnight <code>timedelta</code> Fixed intervals between runs Every 6 hours <code>relativedelta</code> Calendar-aware schedule (e.g. months) Every 1st of the month <code>timetable</code> Advanced Airflow timetables Custom trigger logic <code>assets</code> Trigger based on asset readiness When data <code>X</code> and <code>Y</code> are available <code>datasets</code> Trigger based on datasets readiness When data <code>X</code> and <code>Y</code> are available"},{"location":"configuration/schedule/#schema-options","title":"Schema Options","text":""},{"location":"configuration/schedule/#1-cron-based-schedule","title":"1. Cron-Based Schedule","text":"Corn Schedule<pre><code>schedule: '@daily'\n</code></pre>"},{"location":"configuration/schedule/#2-timedelta-schedule","title":"2. Timedelta Schedule","text":"Timedelta Schedule<pre><code>schedule:\n  __type__: datetime.timedelta\n  seconds: 30\n</code></pre>"},{"location":"configuration/schedule/#3-relativedelta-schedule","title":"3. RelativeDelta Schedule","text":"Relativedelta Schedule<pre><code>schedule:\n  __type__: dateutil.relativedelta.relativedelta\n  hour: 18\n</code></pre>"},{"location":"configuration/schedule/#4-timetable-advanced-scheduling","title":"4. Timetable (Advanced Scheduling)","text":"Timetable Schedule<pre><code>schedule:\n  __type__: airflow.timetables.trigger.CronTriggerTimetable\n  cron: \"* * * * *\"\n  timezone: UTC\n</code></pre>"},{"location":"configuration/schedule/#5-asset-based-triggering","title":"5. Asset-Based Triggering","text":""},{"location":"configuration/schedule/#or-default-when-list-is-provided","title":"OR (default when list is provided)","text":"OR Condition<pre><code>schedule:\n  __type__: builtins.list\n  items:\n    - __type__: airflow.sdk.Asset\n      uri: s3://dag1/output_1.txt\n      extra:\n        hi: bye\n    - __type__: airflow.sdk.Asset\n      uri: s3://dag2/output_1.txt\n      extra:\n        hi: bye\n</code></pre> OR Condition<pre><code>schedule:\n  __or__:\n    - __type__: airflow.sdk.Asset\n      uri: s3://dag1/output_1.txt\n      extra:\n        hi: bye\n    - __type__: airflow.sdk.Asset\n      uri: s3://dag2/output_1.txt\n      extra:\n        hi: bye\n</code></pre>"},{"location":"configuration/schedule/#and-explicit-composition","title":"AND (explicit composition)","text":"AND Condition<pre><code>schedule:\n  __and__:\n    - __type__: airflow.sdk.Asset\n      uri: s3://dag1/output_1.txt\n      extra:\n        hi: bye\n    - __type__: airflow.sdk.Asset\n      uri: s3://dag2/output_1.txt\n      extra:\n        hi: bye\n</code></pre>"},{"location":"configuration/schedule/#nested-and-or-condition","title":"Nested And Or Condition","text":"Nested AND OR Condition<pre><code>schedule:\n  __or__:\n    - __and__:\n        - __type__: airflow.sdk.Asset\n          uri: s3://dag1/output_1.txt\n          extra:\n            hi: bye\n        - __type__: airflow.sdk.Asset\n          uri: s3://dag2/output_1.txt\n          extra:\n            hi: bye\n    - __type__: airflow.sdk.Asset\n      uri: s3://dag3/output_3.txt\n      extra:\n        hi: bye\n</code></pre>"},{"location":"configuration/schedule/#with-watchers","title":"With Watchers","text":"Assert with watcher<pre><code>schedule:\n  __type__: airflow.sdk.Asset\n  uri: s3://dag1/output_1.txt\n  extra:\n    hi: bye\n  watchers:\n    - __type__: airflow.sdk.AssetWatcher\n      name: test_asset_watcher\n      trigger:\n        __type__: airflow.providers.standard.triggers.file.FileDeleteTrigger\n        filepath: \"/temp/file.txt\"\n</code></pre>"},{"location":"configuration/schedule/#6-datasets-based-triggering","title":"6. Datasets-Based Triggering","text":"<pre><code>schedule:\n  __or__:\n    - __and__:\n        - __type__: airflow.datasets.Dataset\n          uri: s3://dag1/output_1.txt\n        - __type__: airflow.datasets.Dataset\n          uri: s3://dag2/output_1.txt\n    - __type__: airflow.datasets.Dataset\n      uri: s3://dag3/output_3.txt\n</code></pre>"},{"location":"contributing/code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"contributing/code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socioeconomic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"contributing/code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"contributing/code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"contributing/code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contributing/code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at humans@astronomer.io.</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"contributing/code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"contributing/code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"contributing/code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"contributing/code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"contributing/code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"contributing/code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ. Translations are available at this page.</p>"},{"location":"contributing/contributors/","title":"Contributors","text":"<p>There are different ways people can contribute to DAG Factory. Learn more about the project contributors roles.</p>"},{"location":"contributing/contributors/#committers","title":"Committers","text":"<ul> <li>Pankaj Koti (@pankajkoti)</li> <li>Pankaj Singh (@pankajastro)</li> <li>Tatiana Al-Chueyr (@tatiana)</li> </ul>"},{"location":"contributing/contributors/#emeritus-committers","title":"Emeritus Committers","text":"<ul> <li>Adam Boscarino (@ajbosco)</li> </ul>"},{"location":"contributing/contributors/#contributors_1","title":"Contributors","text":"<p>Many people are improving DAG Factory each day. Find more contributors in our GitHub page.</p>"},{"location":"contributing/howto/","title":"Contributing Guide","text":"<p>All contributions, bug reports, bug fixes, documentation improvements, and enhancements are welcome.</p> <p>All contributors and maintainers to this project should abide by the Contributor Code of Conduct.</p> <p>Learn more about the contributors' roles in the Roles page.</p> <p>This document describes how to contribute to DAG Factory, covering:</p> <ul> <li>Overview of how to contribute</li> <li>How to set up the local development environment</li> <li>Running tests</li> <li>Pre-commit and linting</li> <li>Authoring the documentation</li> <li>Releasing</li> </ul>"},{"location":"contributing/howto/#overview-of-how-to-contribute","title":"Overview of how to contribute","text":"<p>To contribute to the DAG Factory project:</p> <ol> <li>Please create a GitHub Issue describing a bug, enhancement, or feature request.</li> <li>Fork the repository and clone your fork locally.</li> <li>Open a feature branch off of the main branch in your fork</li> <li>Make your changes, push the branch to your fork, and open a Pull Request from your feature branch into the <code>main</code> branch of the upstream repository.</li> <li>Link your issue to the pull request.</li> <li>After you complete development on your feature branch, request a review. A maintainer will merge your PR after all reviewers approve it.</li> </ol>"},{"location":"contributing/howto/#set-up-a-local-development-environment","title":"Set up a local development environment","text":""},{"location":"contributing/howto/#requirements","title":"Requirements","text":"<ul> <li>Git</li> <li>Python &lt;= 3.12 (due to dependencies, such as <code>google-re2</code> not supporting Python 3.13 yet)</li> <li>uv (for fast package management)</li> <li>Hatch (installed automatically via uv)</li> </ul> <p>Clone the DAG Factory repository and change the current working directory to the repo's root directory:</p> <pre><code>git clone https://github.com/astronomer/dag-factory.git\ncd dag-factory/\n</code></pre> <p>After cloning the project, there are two options for setting up the local development environment:</p> <ul> <li>Use a Python virtual environment, or</li> <li>Use Docker</li> </ul>"},{"location":"contributing/howto/#using-a-python-virtual-environment-for-local-development","title":"Using a Python virtual environment for local development","text":"<p>DAG Factory uses uv for fast and reliable package management. The setup process is significantly faster than traditional pip-based installations.</p>"},{"location":"contributing/howto/#1-install-the-project-dependencies","title":"1. Install the project dependencies","text":"<p>Recommended: uv setup (fast, reproducible builds)</p> <pre><code>uv sync --dev\n</code></pre> <p>Alternative: Traditional setup</p> <pre><code>make setup\n</code></pre> <p>Both commands install all dependencies, including test dependencies. The uv option is significantly faster and uses the lockfile for reproducible builds.</p>"},{"location":"contributing/howto/#2-activate-the-local-python-environment","title":"2. Activate the local python environment","text":"<p>For uv setup:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>For traditional setup:</p> <pre><code>source venv/bin/activate\n</code></pre>"},{"location":"contributing/howto/#additional-uv-commands","title":"Additional uv Commands","text":"<p>Once you're set up with uv, you can use these helpful commands:</p> <ul> <li><code>uv sync</code> - Sync dependencies from the lockfile</li> <li><code>uv lock --upgrade</code> - Update the lockfile with latest dependency versions</li> <li><code>uv add &lt;package&gt;</code> - Add a new dependency</li> <li><code>uv remove &lt;package&gt;</code> - Remove a dependency</li> </ul>"},{"location":"contributing/howto/#3-set-apache-airflow-home-to-the-dev-so-you-can-see-dag-factory-example-dags","title":"3. Set Apache Airflow\u00ae home to the <code>dev/</code>, so you can see DAG Factory example DAGs","text":"<p>Disable loading Airflow standard example DAGs:</p> <pre><code>export AIRFLOW_HOME=$(pwd)/dev/\nexport AIRFLOW__CORE__LOAD_EXAMPLES=false\n</code></pre> <p>Set CONFIG_ROOT_DIR for locating DAG config files:</p> <pre><code>export CONFIG_ROOT_DIR=$AIRFLOW_HOME/dags\n</code></pre> <p>Then, run Airflow in standalone mode; the command below will create a new user (if it does not exist) and run the necessary Airflow component (webserver, scheduler and triggered):</p> <p>Note</p> <p>By default, Airflow will use sqlite as a database; you can override this by setting the variable <code>AIRFLOW__DATABASE__SQL_ALCHEMY_CONN</code> to the SQL connection string.</p> <pre><code>airflow standalone\n</code></pre> <p>After Airflow is running, you can access the Airflow UI at <code>http://localhost:8080</code>.</p> <p>Note</p> <p>whenever you want to start the development server, you need to activate the <code>virtualenv</code> and set the <code>environment variables</code></p>"},{"location":"contributing/howto/#use-docker-for-local-development","title":"Use Docker for local development","text":"<p>It is also possible to build the development environment using Docker:</p> <pre><code>make docker-run\n</code></pre> <p>After the sandbox is running, you can access the Airflow UI at <code>http://localhost:8080</code>.</p> <p>This approach builds a DAG Factory wheel, so if there are code changes, you must stop and restart the containers:</p> <pre><code>make docker-stop\n</code></pre>"},{"location":"contributing/howto/#testing-application-with-hatch","title":"Testing application with hatch","text":"<p>The tests are developed using PyTest and run using hatch.</p> <p>The pyproject. toml file currently defines a matrix of supported versions of Python and Airflow against which a user can run the tests.</p>"},{"location":"contributing/howto/#run-unit-tests","title":"Run unit tests","text":"<p>Note</p> <ul> <li>These tests create local Python virtual environments in a hatch-managed directory.</li> </ul> <p>To run unit tests using Python 3.10 and Airflow 2.5, use the following:</p> <pre><code>hatch run tests.py3.10-2.5:test-cov\n</code></pre> <p>It is also possible to run the tests using all the matrix combinations, by using:</p> <pre><code>hatch run tests:test-cov\n</code></pre>"},{"location":"contributing/howto/#run-integration-tests","title":"Run integration tests","text":"<p>Note</p> <ul> <li>These tests create local Python virtual environments within a <code>hatch</code>-managed directory.</li> <li>They also use the user-defined <code>AIRFLOW_HOME</code>, overriding any pre-existing <code>airflow.cfg</code> and <code>airflow.db</code> files.</li> </ul> <p>First, set the following environment variables:</p> <pre><code>export AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=90\nexport AIRFLOW_HOME=$(pwd)/dev/\nexport CONFIG_ROOT_DIR=$(pwd)/dev/dags\nexport PYTHONPATH=$(pwd)/dev/dags:$PYTHONPATH\n</code></pre> <p>To run the integration tests using Python 3.9 and Airflow 2.9, use</p> <pre><code>hatch run tests.py3.9-2.9:test-integration-setup\nhatch run tests.py3.9-2.9:test-integration\n</code></pre>"},{"location":"contributing/howto/#pre-commit-and-linting","title":"Pre-Commit and linting","text":"<p>We use pre-commit to run several checks on the code before committing. To install pre-commit hooks, run:</p> <pre><code>pre-commit install\n</code></pre> <p>To run the checks manually, run the following:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>Pre-commit runs several static checks, including Black and Ruff. It is also possible to run them using <code>hatch</code>:</p> <pre><code>hatch run tests.py3.9-2.9:static-check\n</code></pre>"},{"location":"contributing/howto/#write-docs","title":"Write docs","text":"<p>We use Markdown to author DAG Factory documentation.</p> <p>Similar to running tests, we also use hatch to manage the documentation.</p> <p>To build and serve the documentation locally:</p> <pre><code>hatch run docs:dev\n</code></pre> <p>To release the documentation with the current project version and set it to the latest:</p> <pre><code>hatch run docs:gh-release\n</code></pre>"},{"location":"contributing/howto/#releasing","title":"Releasing","text":"<p>We currently use hatch for building and distributing <code>dag-factory</code>.</p> <p>We use GitHub actions to create and deploy new releases. To create a new release, update the latest release version.</p> <p>It is possible to update the version either by using hatch:</p> <p>Note</p> <p>You can update the version in several different ways. To learn more, check out the hatch docs.</p> <pre><code>hatch version minor\n</code></pre> <p>Or by manually updating the value of <code>__version__</code> in <code>dagfactory/__init__.py</code>.</p> <p>Make sure the CHANGELOG file is up-to-date.</p> <p>Create a release using the GitHub UI. GitHub will update the package directly to PyPI.</p> <p>If you're a project maintainer in PyPI, it is also possible to create a release manually, by authenticating to PyPI and running the commands:</p> <pre><code>uv build --wheel --sdist\nuv publish --token &lt;your-pypi-token&gt;\n</code></pre>"},{"location":"contributing/roles/","title":"Contributor roles","text":"<p>Contributors are welcome and are greatly appreciated! Every little bit helps, and we give credit to them.</p> <p>This document aims to explain the current roles in the DAG Factory project. For more information, check the contributing docs.</p>"},{"location":"contributing/roles/#contributors","title":"Contributors","text":"<p>A contributor is anyone who wants to contribute code, documentation, tests, ideas, or anything to the DAG Factory project.</p> <p>DAG Factory contributors are listed in the Github insights page.</p> <p>Contributors are responsible for:</p> <ul> <li>Fixing bugs</li> <li>Refactoring code</li> <li>Improving processes and tooling</li> <li>Adding features</li> <li>Improving the documentation</li> </ul>"},{"location":"contributing/roles/#committers","title":"Committers","text":"<p>Committers are community members with write access to the DAG Factory GitHub repository. They can modify the code and the documentation and accept others' contributions to the repo.</p> <p>Check contributors for the official list of DAG Factory committers.</p> <p>Committers have the same responsibilities as standard contributors and also perform the following actions:</p> <ul> <li>Reviewing &amp; merging pull-requests</li> <li>Scanning and responding to GitHub issues, helping triaging them</li> </ul> <p>If you know you are not going to be able to contribute for a long time (for instance, due to a change of job or circumstances), you should inform other maintainers, and we will mark you as \"emeritus\". Emeritus committers will no longer have write access to the repo. As merit earned never expires, once an emeritus committer becomes active again, they can simply email another maintainer from Astronomer and ask to be reinstated.</p>"},{"location":"contributing/roles/#pre-requisites-to-becoming-a-committer","title":"Pre-requisites to becoming a committer","text":"<p>General prerequisites that we look for in all candidates:</p> <ol> <li>Consistent contribution over last few months</li> <li>Visibility on discussions on the Slack channel or GitHub issues/discussions</li> <li>Contributes to community health and project's sustainability for the long-term</li> <li>Understands the project's contributors' guidelines. Astronomer is responsible and accountable for releasing new versions of DAG Factory in PyPI, following the milestones. Astronomer has the right to grant and revoke write access permissions to the project's official repository for any reason it sees fit.</li> </ol>"},{"location":"features/asset/","title":"Asset","text":"<p>DAG Factory supports Airflow's Asset, which allows you to define data assets that DAGs can emit or depend on.</p> <p>To leverage <code>asset</code>, use the <code>__type__</code> annotation to define asset metadata. You can then reference assets using <code>inlets</code> and <code>outlets</code> to emit and track asset events.</p>"},{"location":"features/asset/#example-dag","title":"Example DAG","text":"<p>First, define your DAG configuration in a YAML file</p> asset_triggered_dags.yml<pre><code>default:\n  default_args:\n    start_date: 2025-01-01\n  catchup: false\n\nproducer_dag:\n  schedule: \"@daily\"\n  tasks:\n    produce_data:\n      operator: \"airflow.providers.standard.operators.python.PythonOperator\"\n      python_callable: sample.generate_data\n      outlets:\n        - __type__: airflow.sdk.Asset\n          uri: \"file:///$AIRFLOW_HONE/data.csv\"\n          name: \"data_asset\"\n\nconsumer_dag:\n  schedule:\n    - __type__: airflow.sdk.Asset\n      uri: \"file:///$AIRFLOW_HONE/data.csv\"\n      name: \"data_asset\"\n  tasks:\n    consume_data:\n      operator: \"airflow.providers.standard.operators.bash.BashOperator\"\n      bash_command: \"echo 'Asset was updated, running DAG!'\"\n</code></pre> <p>Then, you can load this YAML configuration dynamically</p> asset_triggered_dags.py<pre><code>import os\nfrom pathlib import Path\n\n# The following import is here so Airflow parses this file\n# from airflow import DAG\nfrom dagfactory import load_yaml_dags\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\n\nconfig_file = str(CONFIG_ROOT_DIR / \"asset_triggered_dags.yml\")\n\nload_yaml_dags(\n    globals_dict=globals(),\n    config_filepath=config_file,\n)\n</code></pre>"},{"location":"features/callbacks/","title":"Callbacks","text":"<p>DAG Factory supports the use of callbacks. These callbacks can be set at the DAG, TaskGroup, or Task level. The way that callbacks that can be configured for DAGs, TaskGroups, and Tasks differ slightly, and details around this can be found in the Apache Airflow documentation.</p> <p>Within DAG Factory itself, there are three approaches to defining callbacks. The goal is to make this process intuitive and provide parity with the traditional DAG authoring experience. These approaches to configure callbacks are outlined below, each with an example of implementation. While proceeding examples are all defined for individual Tasks, callbacks can also be defined using <code>default_args</code>, or at the DAG and TaskGroup level.</p> <ul> <li>Passing a string that points to a callable</li> <li>Specifying a user-defined <code>.py</code> and the function within that file to be executed</li> <li>Configuring callbacks from providers</li> </ul>"},{"location":"features/callbacks/#passing-a-string-that-points-to-a-callable","title":"Passing a string that points to a callable","text":"<p>The most traditional way of configuring callbacks is by defining a custom function within the Airflow project and assigning that callback to the desired Task. Using the syntax below, this can be implemented using DAG Factory. In this case, the <code>output_standard_message</code> function is a user-defined function stored in the <code>include/custom_callbacks.py</code> file. This function requires no parameters, and the YAML would take the form below.</p> <p>For this example to be implemented in DAG Factory, the <code>include/custom_callbacks.py</code> file must be on the Python <code>sys.path</code>. If this is not the case, the full path to a <code>.py</code> function can be specified, as shown below.</p> <pre><code>...\n\n  - task_id: task_1\n    operator: airflow.operators.bash_operator.BashOperator\n    bash_command: \"echo task_1\"\n    on_failure_callback: include.custom_callbacks.output_standard_message\n...\n</code></pre> <p>Sometimes, a function may have parameters that need to be defined within the Task itself. Here, the <code>output_custom_message</code> callback takes two key-word arguments; <code>param1</code>, and <code>param2</code>. These values are defined in the YAML itself, offering DAG Factory authors an additional degree of flexibility and verbosity.</p> <pre><code>...\n\n  - task_id: task_2\n    operator: airflow.operators.bash_operator.BashOperator\n    bash_command: \"echo task_2\"\n    on_success_callback:\n      callback: include.custom_callbacks.output_custom_message\n      param1: \"Task status\"\n      param2: \"Successful!\"\n...\n</code></pre>"},{"location":"features/callbacks/#specifying-a-user-defined-py-file-and-function","title":"Specifying a user-defined <code>.py</code> file and function","text":"<p>In addition to passing a string that points to a callback, the full path to the file and name of the callback can be specified for a DAG, TaskGroup, or Task. This provides a viable option for defining a callback when the director the <code>.py</code> file is stored in is not on the Python path.</p> <pre><code>...\n\n  - task_id: task_3\n    operator: airflow.operators.bash_operator.BashOperator\n    bash_command: \"echo task_3\"\n    on_retry_callback_name: output_standard_message\n    on_retry_callback_file: /usr/local/airflow/include/custom_callbacks.py\n...\n</code></pre> <p>Note that this method for defining callbacks in DAG Factory does not allow for parameters to be passed to the callable within the YAML itself.</p>"},{"location":"features/callbacks/#provider-callbacks","title":"Provider callbacks","text":"<p>In addition to custom-built callbacks, there are a number of provider-built callbacks that can be used when defining a DAG. With DAG Factory, these callbacks can be configured similar to how they would be when authoring a traditional DAG. First, the type of callback is specified (<code>on_success_callback</code>, <code>on_failure_callback</code>, etc.). The <code>callback</code> key-value pair specifies the provider-built function to be executed. Then, the specific key-word arguments the callback takes can be specified, as shown below.</p> <p>Note that the provider package being used must be available on the Python <code>sys.path</code> path, meaning it may need to be <code>pip installed</code>.</p> <pre><code>...\n  - task_id: task_4\n    operator: airflow.operators.bash_operator.BashOperator\n    bash_command: \"echo task_4\"\n    on_failure_callback:\n      callback: airflow.providers.slack.notifications.slack.send_slack_notification\n      slack_conn_id: slack_conn_id\n      text: |\n        :red_circle: Task Failed.\n        This task has failed and needs to be addressed.\n        Please remediate this issue ASAP.\n      channel: \"#channel\"\n...\n</code></pre>"},{"location":"features/cli/","title":"DAG Factory CLI documentation","text":"<p>After installing DAG Factory, the CLI can be invoked using the <code>dagfactory</code> command.</p>"},{"location":"features/cli/#commands-summary","title":"Commands summary","text":"Command Args Flags Description <code>lint</code> <code>path</code> <code>--verbose</code> Check if the given directory or file is a valid YAML <code>convert</code> <code>path</code> <code>--override</code> Convert YAML file(s) from Airflow 2 to 3 in the terminal or in-place <p>For more details about the available commands, run <code>dagfactory --help</code>.</p>"},{"location":"features/cli/#base-command-usage","title":"Base command usage","text":"<pre><code>dagfactory [OPTIONS]\n</code></pre>"},{"location":"features/cli/#flags","title":"Flags","text":"Flag Alias Description <code>--version</code> Show the installed version of DAG Factory and exit <code>--help</code> <code>-h</code> Show this message and exit"},{"location":"features/cli/#identify-the-cli-version","title":"Identify the CLI version","text":"<p>Display the DAG Factory version (both the CLI and the library share the same version number):</p> <pre><code>dagfactory --version\n</code></pre> <p>Output:</p> <pre><code>DAG Factory 1.0.0a1\n</code></pre>"},{"location":"features/cli/#check-all-the-commands-available-in-the-cli","title":"Check all the commands available in the CLI","text":"<p>Find out more about the DAG Factory command line:</p> <pre><code>dagfactory --help\n</code></pre> <p>Output:</p> <pre><code>Usage: dagfactory [OPTIONS]\n\nDAG Factory: Dynamically build Apache Airflow DAGs from YAML files\n\nOptions:\n  -v, --version  Show the version and exit.\n  -h, --help     Show this message and exit.\n</code></pre>"},{"location":"features/cli/#lint-command","title":"<code>lint</code> command","text":"<p>Check if the given directory contains a valid YAML files (recursively) or if the given file is a valid YAML.</p>"},{"location":"features/cli/#example","title":"Example","text":"<pre><code> dagfactory lint some/dir --verbose\n</code></pre> <p>Output:</p> <pre><code>                           DAG Factory: YAML Lint Results\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 File                 \u2503 Status       \u2503 Error Message                               \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 some/dir/v.yml       \u2502 OK           \u2502                                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500------\u253c---------------------------------\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 some/dir/b.yaml      \u2502 Syntax Error \u2502 mapping values are not allowed here         \u2502\n\u2502                      \u2502              \u2502   in \"&lt;unicode string&gt;\", line 2, column 7:  \u2502\n\u2502                      \u2502              \u2502       host: localhost                       \u2502\n\u2502                      \u2502              \u2502           ^                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 some/dir/a.yml       \u2502 Syntax Error \u2502 while parsing a flow sequence               \u2502\n\u2502                      \u2502              \u2502   in \"&lt;unicode string&gt;\", line 2, column 5:  \u2502\n\u2502                      \u2502              \u2502       - [orange, mango                      \u2502\n\u2502                      \u2502              \u2502         ^                                   \u2502\n\u2502                      \u2502              \u2502 expected ',' or ']', but got '&lt;stream end&gt;' \u2502\n\u2502                      \u2502              \u2502   in \"&lt;unicode string&gt;\", line 3, column 1:  \u2502\n\u2502                      \u2502              \u2502                                             \u2502\n\u2502                      \u2502              \u2502     ^                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nAnalysed 3 files, found 2 invalid YAML files.\n</code></pre>"},{"location":"features/cli/#convert-command","title":"<code>convert</code>  command","text":"<p>Given a path to either a directory containing YAML files or to a path to a single YAML file, tries to convert them from Airflow 2 to 3. By default, displays the necessary changes in the terminal (default). If using the flag <code>--override</code>, changes the original files with the necessary changes.</p>"},{"location":"features/cli/#example_1","title":"Example","text":"<pre><code> dagfactory convert dev/dags/airflow3\n</code></pre> <p>Output:</p> <pre><code>No changes needed: dev/dags/airflow3/example_params.yml\nNo changes needed: dev/dags/airflow3/example_dag_factory_multiple_config.yml\nNo changes needed: dev/dags/airflow3/example_task_group.yml\nNo changes needed: dev/dags/airflow3/example_dag_factory_default_args.yml\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Diff for dev/dags/airflow3/example_customize_operator.yml \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n--- dev/dags/airflow3/example_customize_operator.yml\n+++ dev/dags/airflow3/example_customize_operator.yml (converted)\n@@ -11,7 +11,7 @@\n   schedule: 0 3 * * *\n   tasks:\n   - task_id: begin\n-    operator: airflow.operators.empty.EmptyOperator\n+    operator: airflow.providers.standard.operators.empty.EmptyOperator\n   - task_id: make_bread_1\n     operator: customized.operators.breakfast_operators.MakeBreadOperator\n     bread_type: Sourdough\n@@ -30,7 +30,7 @@\n     - make_bread_1\n     - make_bread_2\n   - task_id: end\n-    operator: airflow.operators.empty.EmptyOperator\n+    operator: airflow.providers.standard.operators.empty.EmptyOperator\n     dependencies:\n     - begin\n     - make_bread_1\nNo changes needed: dev/dags/airflow3/example_custom_py_object_dag.yml\nNo changes needed: dev/dags/airflow3/example_taskflow.yml\nNo changes needed: dev/dags/airflow3/example_jinja2_template_dag.yml\nNo changes needed: dev/dags/airflow3/example_dag_factory_default_config.yml\nNo changes needed: dev/dags/airflow3/example_dynamic_task_mapping.yml\nTried to convert 10 files, converted 1 file, no errors found.\n</code></pre>"},{"location":"features/custom_operators/","title":"Custom Operators","text":"<p>DAG-Factory supports custom operators. To leverage, set the path to the custom operator within the <code>operator</code> key in the configuration file. You can add any additional parameters that the custom operator requires.</p> <pre><code>...\n  tasks:\n    - task_id: begin\n      operator: airflow.operators.empty.EmptyOperator\n    - task_id: make_bread_1\n      operator: customized.operators.breakfast_operators.MakeBreadOperator\n      bread_type: 'Sourdough'\n</code></pre> <p></p>"},{"location":"features/custom_python_object/","title":"Custom Python Object","text":"<p>DAG-Factory supports the ability to define custom Python objects directly within your YAML configuration. This is extremely useful for advanced Airflow features, such as specifying Kubernetes pod overrides or other operator-specific configurations using native Python types\u2014without writing Python code.</p> <p>This is achieved via a special <code>key: __type__</code>, which allows recursive construction of Python objects from their fully qualified type names and YAML-defined attributes.</p>"},{"location":"features/custom_python_object/#example","title":"Example","text":"<p>Below is a real-world example of how to define a DAG with Kubernetes-specific executor configuration using <code>__type__</code>.</p> <pre><code>custom_example_dag:\n  description: \"this is an example dag\"\n  schedule: \"@daily\"\n  catchup: false\n  render_template_as_native_obj: True\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 1\"\n      executor_config:\n        pod_override:\n          __type__: kubernetes.client.models.V1Pod\n          spec:\n            __type__: kubernetes.client.models.V1PodSpec\n            containers:\n              __type__: builtins.list\n              items:\n                - __type__: kubernetes.client.models.V1Container\n                  name: \"base\"\n                  resources:\n                    __type__: kubernetes.client.models.V1ResourceRequirements\n                    limits:\n                      cpu: \"1\"\n                      memory: \"1024Mi\"\n                    requests:\n                      cpu: \"0.5\"\n                      memory: \"512Mi\"\n    - task_id: \"task_2\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    - task_id: \"task_3\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n</code></pre>"},{"location":"features/datasets/","title":"Datasets","text":"<p>DAG Factory supports Airflow\u2019s Datasets.</p>"},{"location":"features/datasets/#datasets-outlets-and-inlets","title":"Datasets Outlets and Inlets","text":"<p>To leverage datasets, you need to specify the <code>Dataset</code> in the <code>outlets</code> and <code>inlets</code> keys in the configuration file. The <code>outlets</code> and <code>inlets</code> keys should contain a list of strings representing dataset locations. In the <code>schedule</code> key of the consumer DAG, you can set the <code>Dataset</code> that the DAG should be scheduled against. The key should contain a list of dataset locations. The consumer DAG will run when all the specified datasets become avai</p>"},{"location":"features/datasets/#example-outlet-and-inlet","title":"Example: Outlet and Inlet","text":"example_dag_datasets_outlet_inlet.yml<pre><code>producer_dag:\n  default_args:\n    owner: \"example_owner\"\n    start_date: '2024-01-01'\n  description: \"Example DAG producer simple datasets\"\n  schedule: \"0 5 * * *\"\n  catchup: false\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 1\"\n      inlets: [ 's3://bucket_example/raw/dataset1_source.json' ]\n      outlets: [ 's3://bucket_example/raw/dataset1.json' ]\n    - task_id: \"task_2\"\n      bash_command: \"echo 2\"\n      dependencies: [ task_1 ]\n      inlets: [ 's3://bucket_example/raw/dataset2_source.json' ]\n      outlets: [ 's3://bucket_example/raw/dataset2.json' ]\nconsumer_dag:\n  default_args:\n    owner: \"example_owner\"\n    retries: 1\n    start_date: '2024-01-01'\n  description: \"Example DAG consumer simple datasets\"\n  schedule: [ 's3://bucket_example/raw/dataset1.json', 's3://bucket_example/raw/dataset2.json' ]\n  catchup: false\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 'consumer datasets'\"\n</code></pre>"},{"location":"features/datasets/#conditional-dataset-scheduling","title":"Conditional Dataset Scheduling","text":""},{"location":"features/datasets/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>dag-factory 0.22.0+</li> <li>Apache Airflow\u00ae 2.9+</li> </ul>"},{"location":"features/datasets/#logical-operators-for-datasets","title":"Logical operators for datasets","text":"<p>Airflow supports two logical operators for combining dataset conditions:</p> <ul> <li>AND (<code>&amp;</code>): Specifies that the DAG should be triggered only after all of the specified datasets have been updated.</li> <li>OR (<code>|</code>): Specifies that the DAG should be triggered when any of the specified datasets is updated.</li> </ul> <p>These operators enable you to configure your Airflow workflows to use more complex dataset update conditions, making them more dynamic and flexible.</p>"},{"location":"features/datasets/#examples-of-conditional-dataset-scheduling","title":"Examples of Conditional Dataset Scheduling","text":"<p>Below are examples demonstrating how to configure a consumer DAG using conditional dataset scheduling.</p>"},{"location":"features/datasets/#example-1-string-condition","title":"Example 1: String Condition","text":"example_dataset_condition_string.yml<pre><code>consumer_dag:\n  catchup: false\n  default_args:\n    owner: \"example_owner\"\n    start_date: '2024-01-01'\n  description: \"Example DAG consumer simple datasets\"\n  schedule:\n    datasets: \"((s3://bucket-cjmm/raw/dataset_custom_1 &amp; s3://bucket-cjmm/raw/dataset_custom_2) | s3://bucket-cjmm/raw/dataset_custom_3)\"\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 'consumer datasets'\"\n</code></pre>"},{"location":"features/datasets/#example-2-yaml-syntax","title":"Example 2: YAML Syntax","text":"example_dataset_yaml_syntax.yml<pre><code>consumer_dag:\n  default_args:\n    owner: \"example_owner\"\n    start_date: '2024-01-01'\n  description: \"Example DAG consumer simple datasets\"\n  catchup: false\n  schedule:\n    datasets:\n      __or__:\n        - __and__:\n          - \"s3://bucket-cjmm/raw/dataset_custom_1\"\n          - \"s3://bucket-cjmm/raw/dataset_custom_2\"\n        - \"s3://bucket-cjmm/raw/dataset_custom_3\"\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 'consumer datasets'\"\n</code></pre>"},{"location":"features/datasets/#visualization","title":"Visualization","text":"<p>The following diagrams illustrate the dataset conditions described in the example configurations:</p> <ol> <li><code>s3://bucket-cjmm/raw/dataset_custom_1</code> and <code>s3://bucket-cjmm/raw/dataset_custom_2</code> must both be updated for the first condition to be satisfied.</li> <li>Alternatively, <code>s3://bucket-cjmm/raw/dataset_custom_3</code> alone can satisfy the condition.</li> </ol> <p> </p>"},{"location":"features/dynamic_tasks/","title":"Dynamic tasks","text":"<p>DAG Factory supports Airflow\u2019s Dynamic Task Mapping, enabling workflows to dynamically create tasks at runtime.  This approach allows the number of tasks to be determined during execution, usually based on the outcome of a preceding task, rather than being predefined during DAG authoring.</p>"},{"location":"features/dynamic_tasks/#example-defining-dynamic-tasks","title":"Example: Defining Dynamic Tasks","text":"<p>Below is an example configuration for implementing dynamic tasks using DAG Factory:</p> example_dynamic_task_mapping.yml<pre><code>test_expand:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"test expand\"\n  schedule: \"0 3 * * *\"\n  catchup: false\n  tasks:\n    - task_id: \"process\"\n      operator: airflow.operators.python.PythonOperator\n      python_callable_name: consume_value\n      python_callable_file: $CONFIG_ROOT_DIR/expand_tasks.py\n      partial:\n        op_kwargs:\n          fixed_param: \"test\"\n      expand:\n        op_args:\n            request.output\n      dependencies: [request]\n    # This task is intentionally placed after the \"process\" task to demonstrate that DAG Factory does not require tasks\n    # to be topologically ordered in the YAML file according to their dependencies.\n    - task_id: \"request\"\n      operator: airflow.operators.python.PythonOperator\n      python_callable_name: make_list\n      python_callable_file: $CONFIG_ROOT_DIR/expand_tasks.py\n</code></pre>"},{"location":"features/dynamic_tasks/#explanation-of-the-configuration","title":"Explanation of the Configuration","text":"<ol> <li> <p><code>request</code> Task:</p> <ul> <li>Generates a list of items using the <code>make_list</code> function from the expand_tasks.py module.</li> <li>This task serves as the input provider for the dynamically mapped tasks.</li> </ul> </li> <li> <p><code>process</code> Task:</p> <ul> <li>Dynamically generates one task for each item in the list produced by the <code>request</code> task.</li> <li>The expand argument is used to create these tasks at runtime, with <code>request.output</code> supplying the input list.</li> <li>Additionally, the <code>partial</code> argument is used to specify fixed parameters (<code>op_kwargs</code>) that are applied to all dynamically generated tasks.</li> </ul> </li> </ol>"},{"location":"features/dynamic_tasks/#how-it-works","title":"How It Works","text":"<ul> <li> <p>Dynamic Task Creation:     The <code>expand</code> keyword allows the process task to spawn multiple tasks at runtime, each processing a single item from the list output of the <code>request</code> task.</p> </li> <li> <p>Fixed Parameters:     The partial keyword ensures that common parameters, such as <code>fixed_param</code>, are passed to every dynamically created task instance.</p> </li> </ul>"},{"location":"features/dynamic_tasks/#benefits-of-dynamic-task-mapping-with-dag-factory","title":"Benefits of Dynamic Task Mapping with DAG Factory","text":"<ul> <li>Flexibility: Handle varying input sizes and conditions dynamically without modifying the DAG definition.</li> <li>Scalability: Efficiently process large datasets by leveraging Airflow\u2019s parallel execution capabilities.</li> <li>Simplicity: Define dynamic workflows declaratively using YAML, minimizing boilerplate code.</li> </ul>"},{"location":"features/dynamic_tasks/#airflow-mapped-tasks-view","title":"Airflow mapped tasks view","text":"<p>Below, you can see a list of mapped tasks generated dynamically as part of the <code>process</code> task.</p> <p></p>"},{"location":"features/dynamic_tasks/#advanced-dynamic-task-mapping-with-dag-factory","title":"Advanced Dynamic Task Mapping with DAG Factory","text":"<p>Below, we explain the different methods for defining dynamic task mapping, illustrated by the provided example configuration.</p> Dynamic Task Mapping advanced usage<pre><code>example_taskflow:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"Example of TaskFlow powered DAG that includes dynamic task mapping\"\n  schedule: \"0 3 * * *\"\n  catchup: false\n  tasks:\n    - task_id: \"some_number\"\n      decorator: airflow.decorators.task\n      python_callable: sample.some_number\n    - task_id: \"numbers_list\"\n      decorator: airflow.decorators.task\n      python_callable_name: build_numbers_list\n      python_callable_file: $CONFIG_ROOT_DIR/sample.py\n    - task_id: \"another_numbers_list\"\n      decorator: airflow.decorators.task\n      python_callable: sample.build_numbers_list\n    - task_id: \"double_number_from_arg\"\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      number: 2\n    - task_id: \"double_number_from_task\"\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      number: +some_number  # the prefix + leads to resolving this value as the task `some_number`, previously defined\n    - task_id: \"double_number_with_dynamic_task_mapping_static\"\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      expand:\n          number:\n            - 1\n            - 3\n            - 5\n    - task_id: \"double_number_with_dynamic_task_mapping_taskflow\"\n      decorator: airflow.decorators.task\n      python_callable: sample.double\n      expand:\n          number: +numbers_list  # the prefix + tells DagFactory to resolve this value as the task `numbers_list`, previously defined\n    - task_id: \"multiply_with_multiple_parameters\"\n      decorator: airflow.decorators.task\n      python_callable: sample.multiply\n      expand:\n          a: +numbers_list  # the prefix + tells DagFactory to resolve this value as the task `numbers_list`, previously defined\n          b: +another_numbers_list # the prefix + tells DagFactory to resolve this value as the task `another_numbers_list`, previously defined\n    - task_id: \"double_number_with_dynamic_task_and_partial\"\n      decorator: airflow.decorators.task\n      python_callable: sample.double_with_label\n      expand:\n          number: +numbers_list  # the prefix + tells DagFactory to resolve this value as the task `numbers_list`, previously defined\n      partial:\n          label: True\n</code></pre> <p>The example above illustrates advanced usage of Dynamic Task Mapping using Dag Factory (the callable functions used in the example are kept in sample.py):</p> <ol> <li> <p>Static Input Mapping</p> <p>The task <code>double_number_with_dynamic_task_mapping_static</code> shows how dynamic tasks can be created using static lists as input. Three tasks are created, each processing one number.</p> </li> <li> <p>Task-Generated Input Mapping</p> <p>The task <code>double_number_with_dynamic_task_mapping_taskflow</code> shows how tasks can use outputs from other tasks as input for dynamic task mapping. The prefix <code>+</code> tells DAG Factory to resolve this value as the task <code>numbers_list</code>, previously defined.</p> </li> <li> <p>Mapping with Multiple Inputs</p> <p>The task <code>multiply_with_multiple_parameters</code> shows how dynamic task mapping can combine outputs from multiple tasks as input parameters.</p> </li> </ol>"},{"location":"features/dynamic_tasks/#named-mapping-in-dynamic-tasks-with-dag-factory","title":"Named Mapping in Dynamic Tasks with DAG Factory","text":"<p>Starting with Airflow 2.9, the <code>map_index_template</code> feature allows for custom mapping name for dynamic tasks based on a user-defined key. DAG Factory fully supports this feature, enabling users to name tasks dynamically in a meaningful way during runtime. This can be useful for tracing and debugging tasks.</p> <p>Below is an example of how to configure and use custom names for mapped tasks</p> example_map_index_template.yml<pre><code># Requires Airflow 2.9 or higher\nexample_map_index_template:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"Example of TaskFlow powered DAG that includes dynamic task mapping\"\n  schedule: \"0 3 * * *\"\n  catchup: false\n  tasks:\n    - task_id: \"dynamic_task_with_named_mapping\"\n      decorator: airflow.decorators.task\n      python_callable: sample.extract_last_name\n      map_index_template: \"{{ custom_mapping_key }}\"\n      expand:\n        full_name:\n          - Lucy Black\n          - Vera Santos\n          - Marks Spencer\n</code></pre>"},{"location":"features/dynamic_tasks/#how-it-works_1","title":"How it works","text":"<ol> <li>map_index_template:    Customizes the naming of dynamically mapped tasks using a Jinja2 expression. In this example, it uses <code>custom_mapping_key</code> from the task context to define task names.</li> <li>expand:    Dynamically generates tasks for each entry in the <code>full_name</code> list<ul> <li>Lucy Black</li> <li>Vera Santos</li> <li>Marks Spencer</li> </ul> </li> <li>Dynamic Task Naming:    The <code>custom_mapping_key</code> is set to the first name of each person, e.g., Lucy, Vera, and Marks using the callable function <code>extract_last_name</code>. This callable function is kept in sample.py</li> </ol>"},{"location":"features/dynamic_tasks/#airflow-named-mapped-tasks-view","title":"Airflow named mapped tasks view","text":"<p>The image below shows that the <code>map_index</code> gets the first name of the person in the mapped tasks with the above configuration.</p> <p></p>"},{"location":"features/dynamic_tasks/#scope-and-limitations","title":"Scope and limitations","text":"<p>The Airflow documentation on dynamic task mapping provides various examples of this feature. While the previous sections have discussed the forms supported by DAG Factory, it\u2019s important to note the scenarios that have not been tested or are known to be unsupported.</p> <p>The following cases are tested and expected to work (you can refer to previous sections on how to use them with DAG Factory):</p> <ul> <li>Simple mapping</li> <li>Task-generated mapping</li> <li>Repeated mapping</li> <li>Adding parameters that do not expand (partial)</li> <li>Mapping over multiple parameters</li> <li>Named mapping (map_index_template)</li> </ul> <p>The following cases are untested but are expected to work:</p> <ul> <li>Mapping with non-TaskFlow operators</li> <li>Mapping over the result of classic operators</li> <li>Filtering items from a mapped task</li> </ul> <p>The following cases are untested and may not work:</p> <ul> <li>Assigning multiple parameters to a non-TaskFlow operator</li> <li>Mapping over a task group</li> <li>Transforming expanding data</li> <li>Combining upstream data (aka \u201czipping\u201d)</li> </ul>"},{"location":"features/http_task/","title":"HttpSensor","text":"<p>DAG-Factory supports the HttpSensor from the <code>airflow.providers.http.sensors.http</code> package.</p> <p>The example below demonstrates the response_check logic in a Python file:</p> <pre><code>- task_id: task_2\n  operator: airflow.providers.http.sensors.http.HttpSensor\n  http_conn_id: 'test-http'\n  method: 'GET'\n  response_check_name: check_sensor\n  response_check_file: /path/to/example1/http_conn.py\n  dependencies: [task_1]\n</code></pre> <p>The <code>response_check</code> logic can also be provided as a lambda:</p> <pre><code>- task_id: task_2\n  operator: airflow.providers.http.sensors.http.HttpSensor\n  http_conn_id: 'test-http'\n  method: 'GET'\n  response_check_lambda: 'lambda response: \"ok\" in response.text'\n  dependencies: [task_1]\n</code></pre>"},{"location":"features/kpo/","title":"KubernetesPodOperator","text":"<p>In DAG Factory, you can use Airflow Kubernetes Provider to create and run Pods on a Kubernetes cluster.</p>"},{"location":"features/kpo/#example-dag","title":"Example DAG","text":"<p>First, define your DAG configuration in a YAML file</p> <pre><code>kubernetes_pod_dag:\n  start_date: 2025-01-01\n  schedule_interval: \"@daily\"\n  description: \"A DAG that runs a simple KubernetesPodOperator task\"\n  catchup: false\n  tasks:\n    - task_id: hello-world-pod\n      operator: airflow.providers.cncf.kubernetes.operators.pod.KubernetesPodOperator\n      config_file: \"path/to/kube/config\"\n      image: \"python:3.12-slim\"\n      cmds: [\"python\", \"-c\"]\n      arguments: [\"print('Hello from KubernetesPodOperator!')\"]\n      name: \"example-pod-task\"\n      namespace: \"default\"\n      get_logs: true\n      container_resources:\n        __type__: kubernetes.client.models.V1ResourceRequirements\n        limits:\n          cpu: \"1\"\n          memory: \"1024Mi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"512Mi\"\n</code></pre> <p>Then, you can load this YAML configuration dynamically.</p> <pre><code>import os\nfrom pathlib import Path\n\n# The following import is here so Airflow parses this file\n# from airflow import DAG\nfrom dagfactory import load_yaml_dags\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\n\nconfig_file = str(CONFIG_ROOT_DIR / \"kpo.yml\")\nload_yaml_dags(\n    globals_dict=globals(),\n    config_filepath=config_file,\n)\n</code></pre>"},{"location":"features/multiple_configuration_files/","title":"Multiple Configuration Files","text":"<p>Using DAG-Factory if you want to split your DAG configuration into multiple files, you can do so by leveraging a suffix in the configuration file name.</p> <pre><code>    from dagfactory import load_yaml_dags  # load relevant YAML files as airflow DAGs\n\n    load_yaml_dags(globals_dict=globals(), suffix=['dag.yaml'])\n</code></pre>"},{"location":"features/object_storage/","title":"Object Storage","text":"<p>In DAG-Factory, You can define a custom Python class to represent object storage (e.g., S3, GCS, Azure Blob).</p> <pre><code>object_storage_ops:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"Example of ObjectStorage powered DAG\"\n  schedule: \"0 3 * * *\"\n  tasks:\n    object_storage_ops:\n      operator: airflow.operators.python.PythonOperator\n      python_callable: sample.object_storage_ops\n      op_kwargs:\n        my_obj_storage:\n          __type__: airflow.io.path.ObjectStoragePath\n          __args__:\n            - file:///$CONFIG_ROOT_DIR/data/object_storage_ops.csv\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/","title":"DAG Factory: Quick Start Guide With Airflow","text":"<p>DAG Factory is a Python library Apache Airflow\u00ae that simplifies DAG creation using declarative YAML configuration files instead of Python.</p>"},{"location":"getting-started/quick-start-airflow-standalone/#prerequisites","title":"Prerequisites","text":"<p>The minimum requirements for dag-factory are:</p> <ul> <li>Python 3.9.0+</li> <li>Apache Airflow\u00ae 2.4+</li> </ul>"},{"location":"getting-started/quick-start-airflow-standalone/#step-1-create-a-python-virtual-environment","title":"Step 1: Create a Python Virtual Environment","text":"<p>Create and activate a virtual environment:</p> <pre><code>python3 -m venv dagfactory_env\nsource dagfactory_env/bin/activate\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-2-install-apache-airflow","title":"Step 2: Install Apache Airflow","text":"<p>Install Apache Airflow\u00ae:</p> <ol> <li>Create a directory for your project and navigate to it:</li> </ol> <pre><code>mkdir dag-factory-quick-start &amp;&amp; cd dag-factory-quick-start\n</code></pre> <ol> <li>Set the <code>AIRFLOW_HOME</code> environment variable:</li> </ol> <pre><code>export AIRFLOW_HOME=$(pwd)\nexport AIRFLOW__CORE__LOAD_EXAMPLES=False\n</code></pre> <ol> <li>Install Apache Airflow:</li> </ol> <pre><code>pip install apache-airflow\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-3-install-dag-factory","title":"Step 3: Install DAG Factory","text":"<p>Install the DAG Factory library in your virtual environment:</p> <pre><code>pip install dag-factory\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-4-set-up-the-dags-folder","title":"Step 4: Set Up the DAGS Folder","text":"<p>Create a DAGs folder inside the $AIRFLOW_HOME directory, which is where your DAGs will be stored:</p> <pre><code>mkdir dags\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-5-define-a-dag-in-yaml","title":"Step 5: Define a DAG in YAML","text":"<p>DAG Factory uses YAML files to define DAG configurations. Create a file named <code>example_dag_factory.yml</code> in the <code>$AIRFLOW_HOME/dags</code> folder with the following content:</p> example_dag_factory.yml<pre><code>default:\n  default_args:\n    start_date: 2024-11-11\n\nbasic_example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  catchup: false\n  task_groups:\n    - group_name: \"example_task_group\"\n      tooltip: \"this is an example task group\"\n      dependencies: [task_1]\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 1\"\n    - task_id: \"task_2\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    - task_id: \"task_3\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 3\"\n      dependencies: [task_1]\n      task_group_name: \"example_task_group\"\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-6-generate-the-dag-from-yaml","title":"Step 6: Generate the DAG from YAML","text":"<p>Create a Python script named <code>example_dag_factory.py</code> in the <code>$AIRFLOW_HOME/dags</code> folder. This script will generate the DAG from the YAML configuration</p> example_dag_factory.py<pre><code>import os\nfrom pathlib import Path\n\n# The following import is here so Airflow parses this file\n# from airflow import DAG\nfrom dagfactory import load_yaml_dags\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\n\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\n\nconfig_file = str(CONFIG_ROOT_DIR / \"example_dag_factory.yml\")\n\nload_yaml_dags(\n    globals_dict=globals(),\n    config_filepath=config_file,\n)\n</code></pre>"},{"location":"getting-started/quick-start-airflow-standalone/#step-7-start-airflow","title":"Step 7: Start Airflow","text":"<p>To start the Airflow environment with your DAG Factory setup, run the following command:</p> <pre><code>airflow standalone\n</code></pre> <p>This will take a few minutes to set up. Once completed, you can access the Airflow UI and the generated DAG at <code>http://localhost:8080</code> \ud83d\ude80.</p>"},{"location":"getting-started/quick-start-airflow-standalone/#view-your-generated-dag","title":"View Your Generated DAG","text":"<p>Once Airflow is up and running, you can login with the username <code>admin</code> and the password in <code>$AIRFLOW_HOME/standalone_admin_password.txt</code>. You should be able to see your generated DAG in the Airflow UI.</p>"},{"location":"getting-started/quick-start-airflow-standalone/#generated-dag","title":"Generated DAG","text":""},{"location":"getting-started/quick-start-airflow-standalone/#graph-view","title":"Graph View","text":"<p>Checkout examples for generating more advanced DAGs.</p>"},{"location":"getting-started/quick-start-astro-cli/","title":"DAG Factory: Quick Start Guide With Astro CLI","text":"<p>DAG Factory is a Python library Apache Airflow\u00ae that simplifies DAG creation using declarative YAML configuration files instead of Python.</p>"},{"location":"getting-started/quick-start-astro-cli/#prerequisites","title":"Prerequisites","text":"<p>The minimum requirements for dag-factory are:</p> <ul> <li>Python 3.9.0+</li> <li>Astro CLI</li> </ul>"},{"location":"getting-started/quick-start-astro-cli/#step-1-initialize-airflow-project","title":"Step 1: Initialize Airflow Project","text":"<p>Create a new directory and initialize your Astro CLI project:</p> <pre><code>mkdir dag-factory-quick-start &amp;&amp; cd dag-factory-quick-start\n\nastro dev init\n</code></pre> <p>This will set up the necessary Airflow files and directories.</p>"},{"location":"getting-started/quick-start-astro-cli/#step-2-install-dag-factory","title":"Step 2: Install DAG Factory","text":"<p>Install DAG Factory in your Airflow environment:</p> <ol> <li>Add dag-factory as a dependency to the <code>requirements.txt</code> file created during the project initialization.</li> </ol>"},{"location":"getting-started/quick-start-astro-cli/#step-3-define-a-dag-in-yaml","title":"Step 3: Define a DAG in YAML","text":"<p>DAG Factory uses YAML files to define DAG configurations. Create a file named <code>example_dag_factory.yml</code> in the <code>$AIRFLOW_HOME/dags</code> folder with the following content:</p> example_dag_factory.yml<pre><code>default:\n  default_args:\n    start_date: 2024-11-11\n\nbasic_example_dag:\n  default_args:\n    owner: \"custom_owner\"\n  description: \"this is an example dag\"\n  schedule: \"0 3 * * *\"\n  render_template_as_native_obj: True\n  catchup: false\n  task_groups:\n    - group_name: \"example_task_group\"\n      tooltip: \"this is an example task group\"\n      dependencies: [task_1]\n  tasks:\n    - task_id: \"task_1\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 1\"\n    - task_id: \"task_2\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    - task_id: \"task_3\"\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 3\"\n      dependencies: [task_1]\n      task_group_name: \"example_task_group\"\n</code></pre>"},{"location":"getting-started/quick-start-astro-cli/#step-4-generate-the-dag-from-yaml","title":"Step 4: Generate the DAG from YAML","text":"<p>Create a Python script named <code>example_dag_factory.py</code> in the <code>$AIRFLOW_HOME/dags</code> folder. This script will generate the DAG from the YAML configuration</p> example_dag_factory.py<pre><code>import os\nfrom pathlib import Path\n\n# The following import is here so Airflow parses this file\n# from airflow import DAG\nfrom dagfactory import load_yaml_dags\n\nDEFAULT_CONFIG_ROOT_DIR = \"/usr/local/airflow/dags/\"\n\nCONFIG_ROOT_DIR = Path(os.getenv(\"CONFIG_ROOT_DIR\", DEFAULT_CONFIG_ROOT_DIR))\n\nconfig_file = str(CONFIG_ROOT_DIR / \"example_dag_factory.yml\")\n\nload_yaml_dags(\n    globals_dict=globals(),\n    config_filepath=config_file,\n)\n</code></pre>"},{"location":"getting-started/quick-start-astro-cli/#step-5-start-airflow-project","title":"Step 5: Start Airflow Project","text":"<p>Once you've set up your YAML configuration and Python script, start the Airflow environment with the following command:</p> <pre><code>astro dev start\n</code></pre> <p>This will take a few minutes to set up. Once completed, you can access the Airflow UI and the generated DAG at <code>http://localhost:8080</code> \ud83d\ude80.</p>"},{"location":"getting-started/quick-start-astro-cli/#view-your-generated-dag","title":"View Your Generated DAG","text":"<p>Once Airflow is up and running, you can login with the username <code>admin</code> and the password <code>admin</code>. You should be able to see your generated DAG in the Airflow UI.</p>"},{"location":"getting-started/quick-start-astro-cli/#generated-dag","title":"Generated DAG","text":""},{"location":"getting-started/quick-start-astro-cli/#graph-view","title":"Graph View","text":"<p>Checkout examples for generating more advanced DAGs.</p>"}]}