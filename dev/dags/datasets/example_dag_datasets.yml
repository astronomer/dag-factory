default:
  default_args:
    owner: "default_owner"
    start_date: '2023-07-14'

example_simple_dataset_producer_dag:
  description: "Example DAG producer simple datasets"
  schedule: "0 5 * * *"
  catchup: false
  tasks:
    - task_id: "task_1"
      operator: airflow.operators.bash.BashOperator
      bash_command: "echo 1"
      inlets: [ 's3://bucket_example/raw/dataset1_source.json' ]
      outlets: ['s3://bucket_example/raw/dataset1.json']
    - task_id: "task_2"
      operator: airflow.operators.bash.BashOperator
      bash_command: "echo 2"
      dependencies: [task_1]
      inlets: [ 's3://bucket_example/raw/dataset2_source.json' ]
      outlets: ['s3://bucket_example/raw/dataset2.json']

example_simple_dataset_consumer_dag:
  description: "Example DAG consumer simple datasets"
  schedule: ['s3://bucket_example/raw/dataset1.json', 's3://bucket_example/raw/dataset2.json']
  catchup: false
  tasks:
    - task_id: "task_1"
      operator: airflow.operators.bash.BashOperator
      bash_command: "echo 'consumer datasets'"

example_custom_config_dataset_producer_dag:
  description: "Example DAG producer custom config datasets"
  schedule: "0 5 * * *"
  catchup: false
  tasks:
    - task_id: "task_1"
      operator: airflow.operators.bash.BashOperator
      bash_command: "echo 1"
      outlets:
        file: $CONFIG_ROOT_DIR/datasets/example_config_datasets.yml
        datasets: ['dataset_custom_1', 'dataset_custom_2']

example_custom_config_dataset_consumer_dag:
  description: "Example DAG consumer custom config datasets"
  catchup: false
  schedule:
    file: $CONFIG_ROOT_DIR/datasets/example_config_datasets.yml
    datasets: ['dataset_custom_1', 'dataset_custom_2']
  tasks:
    - task_id: "task_1"
      operator: airflow.operators.bash.BashOperator
      bash_command: "echo 'consumer datasets'"

example_custom_config_condition_dataset_consumer_dag:
  description: "Example DAG consumer custom config condition datasets"
  catchup: false
  schedule:
    file: $CONFIG_ROOT_DIR/datasets/example_config_datasets.yml
    datasets: "((dataset_custom_1 & dataset_custom_2) | dataset_custom_3)"
  tasks:
    - task_id: "task_1"
      operator: airflow.operators.bash.BashOperator
      bash_command: "echo 'consumer datasets'"

example_without_custom_config_condition_dataset_consumer_dag:
  description: "Example DAG consumer custom config condition datasets"
  catchup: false
  schedule:
    datasets:
      __or__:
        - __and__:
          - "s3://bucket-cjmm/raw/dataset_custom_1"
          - "s3://bucket-cjmm/raw/dataset_custom_2"
        - "s3://bucket-cjmm/raw/dataset_custom_3"
  tasks:
    - task_id: "task_1"
      operator: airflow.operators.bash.BashOperator
      bash_command: "echo 'consumer datasets'"
